{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"These are notes I've taken on technologies that I have used or would like to use. I used to keep them in a private mediawiki, but because they would be useful for other people, and because I want to have them immediately available during outages, I've moved this into public view. http://danielhoherd.github.io/tech-notes/ http://github.com/danielhoherd/tech-notes","title":"About these notes"},{"location":"3d_printing/","text":"https://www.makerbot.com http://www.meshlab.net https://www.thingiverse.com https://all3dp.com/1/petg-filament-3d-printing/","title":"3D Printing"},{"location":"airport/","text":"Apple Airport hardware was discontinued in November 2016. Using old Airport Utility apps with new versions of OS X https://zcs.zyniker.org/airport-utility-v5-6-1 https://support.apple.com/kb/DL1536 Or use the 5.6.1 Utility in Windows? Not sure if this works. https://support.apple.com/kb/dl1547","title":"Apple Airport"},{"location":"airport/#using-old-airport-utility-apps-with-new-versions-of-os-x","text":"https://zcs.zyniker.org/airport-utility-v5-6-1 https://support.apple.com/kb/DL1536 Or use the 5.6.1 Utility in Windows? Not sure if this works. https://support.apple.com/kb/dl1547","title":"Using old Airport Utility apps with new versions of OS X"},{"location":"amazon-ec2/","text":"\"Amazon Elastic Compute Cloud (Amazon EC2) is a web service that provides secure, resizable compute capacity in the cloud. It is designed to make web-scale cloud computing easier for developers.\" - https://aws.amazon.com/ec2/ Techniques Determine if you are on an EC2 instance grep -i '^ec2' /sys/hypervisor/uuid See Also Amazon","title":"Amazon EC2"},{"location":"amazon-ec2/#techniques","text":"","title":"Techniques"},{"location":"amazon-ec2/#determine-if-you-are-on-an-ec2-instance","text":"grep -i '^ec2' /sys/hypervisor/uuid","title":"Determine if you are on an EC2 instance"},{"location":"amazon-ec2/#see-also","text":"Amazon","title":"See Also"},{"location":"amazon/","text":"Mostly related to the technological offerings of Amazon, not the shopping experience or anything else they do. Links Amazon service availability by region Amazon EC2 Instance Types EC2Instances.info - Easy Amazon EC2 Instance Comparison AWS Console AWS in plain english Github - Troposphere s3toosl - Command Line S3 Client and Backup awless - awless is a powerful, innovative and small surface command line interface (CLI) to manage Amazon Web Services. Tips Reformat accessKeys.csv into .aws/credentials format awk -F, 'BEGIN { print \"[temp_name]\" ; } !/Access/ {print \"aws_access_key_id = \"$1\"\\naws_secret_access_key = \"$2}' ~/Downloads/accessKeys.csv See Also aws cli CloudFormation Terms and Acronyms Term Definition AMI Amazon Machine Images ASG Auto-Scaling Group. Auto Scaling can automatically increase the number of Amazon EC2 instances during demand spikes to maintain performance and decrease capacity during lulls to reduce costs. AWS Amazon Web Services AZ Data centers. Amazon Availability Zones. EBS Elastic Block Storage. Provides persistent block-level storage volumes for EC2. EC2 Elastic Compute Cloud ELB Elastic Load Balancing EMR Elastic MapReduce. Allows easily and cheap processing of vast amounts of data ETS Elastic Transcoder. Provides video transcoding of S3 hosted videos FPS Flexible Payments Service. Provides an interface for micro payments. HSM Hardware Security Module IAM Identity and Access Management enables you to securely control access to AWS services and resources for your users. Using IAM, you can create and manage AWS users and groups, and use permissions to allow and deny their access to AWS resources. KMS Key Management Service MFA Amazon Web Services Multi-Factor Authentication is a simple best practice that adds an extra layer of protection on top of your user name and password. RDS Relational Database Service RI Reserved Instance. EC2 instances for which there is a 1 or 3 year contract with AWS to achieve lower hourly run costs. Route 53 A highly available and scalable cloud Domain Name System (DNS) web service. S3 Simple Storage Service SES Simple Email Service. Provides bulk and transactional email sending. SG Security Group. Security Groups in AWS are akin to firewalls. SNS Simple Notification Service. Provides a hosted multiprotocol \"push\" messaging for applications. SQS Simple Queue Service. Provides a hosted message queue for web applications. SWF Simple Workflow. Workflow service for building scalable, resilient applications. VPC Virtual Private Cloud","title":"Amazon"},{"location":"amazon/#links","text":"Amazon service availability by region Amazon EC2 Instance Types EC2Instances.info - Easy Amazon EC2 Instance Comparison AWS Console AWS in plain english Github - Troposphere s3toosl - Command Line S3 Client and Backup awless - awless is a powerful, innovative and small surface command line interface (CLI) to manage Amazon Web Services.","title":"Links"},{"location":"amazon/#tips","text":"","title":"Tips"},{"location":"amazon/#reformat-accesskeyscsv-into-awscredentials-format","text":"awk -F, 'BEGIN { print \"[temp_name]\" ; } !/Access/ {print \"aws_access_key_id = \"$1\"\\naws_secret_access_key = \"$2}' ~/Downloads/accessKeys.csv","title":"Reformat accessKeys.csv into .aws/credentials format"},{"location":"amazon/#see-also","text":"aws cli CloudFormation","title":"See Also"},{"location":"amazon/#terms-and-acronyms","text":"Term Definition AMI Amazon Machine Images ASG Auto-Scaling Group. Auto Scaling can automatically increase the number of Amazon EC2 instances during demand spikes to maintain performance and decrease capacity during lulls to reduce costs. AWS Amazon Web Services AZ Data centers. Amazon Availability Zones. EBS Elastic Block Storage. Provides persistent block-level storage volumes for EC2. EC2 Elastic Compute Cloud ELB Elastic Load Balancing EMR Elastic MapReduce. Allows easily and cheap processing of vast amounts of data ETS Elastic Transcoder. Provides video transcoding of S3 hosted videos FPS Flexible Payments Service. Provides an interface for micro payments. HSM Hardware Security Module IAM Identity and Access Management enables you to securely control access to AWS services and resources for your users. Using IAM, you can create and manage AWS users and groups, and use permissions to allow and deny their access to AWS resources. KMS Key Management Service MFA Amazon Web Services Multi-Factor Authentication is a simple best practice that adds an extra layer of protection on top of your user name and password. RDS Relational Database Service RI Reserved Instance. EC2 instances for which there is a 1 or 3 year contract with AWS to achieve lower hourly run costs. Route 53 A highly available and scalable cloud Domain Name System (DNS) web service. S3 Simple Storage Service SES Simple Email Service. Provides bulk and transactional email sending. SG Security Group. Security Groups in AWS are akin to firewalls. SNS Simple Notification Service. Provides a hosted multiprotocol \"push\" messaging for applications. SQS Simple Queue Service. Provides a hosted message queue for web applications. SWF Simple Workflow. Workflow service for building scalable, resilient applications. VPC Virtual Private Cloud","title":"Terms and Acronyms"},{"location":"ansible/","text":"https://docs.ansible.com/intro_inventory.html https://docs.ansible.com/playbooks_best_practices.html https://galaxy.ansible.com http://www.azavea.com/blogs/labs/2014/10/creating-ansible-roles-from-scratch-part-1 http://ryandlane.com/blog/2014/08/04/moving-away-from-puppet-saltstack-or-ansible/ http://docs.ansible.com/ansible/latest/user_guide/intro_patterns.html - Targeting hosts http://docs.ansible.com/ansible/latest/user_guide/playbooks_variables.html#variable-precedence-where-should-i-put-a-variable Modules http://docs.ansible.com/ansible/latest/list_of_all_modules.html http://docs.ansible.com/ansible/latest/modules/apt_module.html http://docs.ansible.com/ansible/latest/modules/apt_repository_module.html http://docs.ansible.com/ansible/latest/modules/sysctl_module.html http://docs.ansible.com/ansible/latest/modules/user_module.html Examples Show a list of installed modules ansible-doc --list Run a playbook and prompt for sudo password ansible-playbook --ask-become-pass -i inventory/hosts.yaml create_users.yaml Run an ad-hoc command You can run one-off ad-hoc commands by passing a module and args to the module. ansible localhost \\ -m get_url \\ -a \"mode=755 url=https://github.com/bcicen/ctop/releases/download/v0.7.1/ctop-0.7.1-linux-amd64 dest=/usr/local/bin/ctop checksum=sha256:38cfd92618ba2d92e0e1262c0c43d7690074b4b8dc77844b654f8e565166b577 owner=root group=root\" Validate and inspect your inventory file This command parses your inventory and group_vars and outputs a json data structure if no syntax faults are found. ansible-inventory -i inventory/hosts.yml --list Use arbitrary groups in static inventory file $ nl -w 2 -s ' ' -ba inventory/example.yml 1 all: 2 hosts: 3 client: 4 ansible_host: 192.168.1.2 5 server: 6 ansible_host: 192.168.2.3 7 8 linux: 9 hosts: 10 server: 11 12 windows: 13 hosts: 14 client: 15 16 california: 17 hosts: 18 client: 19 server: $ ansible-inventory -i inventory/example.yml --graph @all: |--@california: | |--client | |--server |--@linux: | |--server |--@windows: | |--client Merge multiple inventory files The below example gives higher precedence to the later files. $ cat foo.yml all: hosts: client: ansible_host: 192.168.1.2 service_hostname: hostname-from-file-1 server: ansible_host: 192.168.2.3 file_number: one $ cat bar.yml all: hosts: client: ansible_host: 10.1.2.3 server: ansible_host: 10.2.3.4 file_number: two $ ansible-inventory -i foo.yml -i bar.yml --list | json-to-yaml.py _meta: hostvars: client: ansible_host: 10.1.2.3 service_hostname: hostname-from-file-1 server: ansible_host: 10.2.3.4 file_number: two all: children: - ungrouped ungrouped: hosts: - client - server Show all resolved variables for a given inventory host This will show all host vars, including variables resolved from all the different variable locations . ansible -i inventory target_hostname -m debug -a \"var=hostvars[inventory_hostname]\" Generate an idempotent random number This is similar to the Puppet fqdn_rand() function, which is really useful in cron jobs --- # defaults/main.yml demo_cron_minute: \"{{ 59 | random(seed=inventory_hostname) }}\" demo_cron_hour: \"{{ 23 | random(seed=inventory_hostname) }}\" See also: https://docs.ansible.com/ansible/latest/user_guide/playbooks_filters.html#random-number-filter Simple ansible playbook This may be useful for testing syntax and experimenting with ansible modules. --- # playbook.yml - name: A local play hosts: localhost connection: local gather_facts: no tasks: - name: Run cmd shell: /bin/date register: cmd_out - debug: var: cmd_out.stdout ansible-playbook -i localhost playbook.yml Slightly more complicated example: # playbook.yml # run with: ansible-playbook -i localhost playbook.yml - name: A local play hosts: localhost connection: local gather_facts: no vars: region: test_region subnets: - subnet_name: Public_2a subnet_cidr: 192.168.100.0/26 subnet_az: \"{{ region }}_a\" - subnet_name: Public_2b subnet_cidr: 192.168.100.64/26 subnet_az: \"{{ region }}_b\" - subnet_name: Private_2a subnet_cidr: 192.168.100.128/26 subnet_az: \"{{ region }}_a\" - subnet_name: Private_2b subnet_cidr: 192.168.100.192/26 subnet_az: \"{{ region }}_b\" tasks: - name: Run cmd shell: echo \"{{ item.subnet_name }} {{ item.subnet_cidr }} {{ item.subnet_az }}\" register: cmd_out loop: \"{{ subnets }}\" - debug: var: cmd_out Links https://github.com/danielhoherd/ansible-playground - Examples I've made of Ansible techniques Ansible Tests with Molecule - https://molecule.readthedocs.io/en/latest/ / https://www.digitalocean.com/community/tutorials/how-to-test-ansible-roles-with-molecule Molecule sequence of scenario events - https://molecule.readthedocs.io/en/latest/configuration.html#scenario Test-driven infrastructure development with Ansible & Molecule - https://blog.codecentric.de/en/2018/12/test-driven-infrastructure-ansible-molecule/ / https://github.com/jonashackt/molecule-ansible-docker-vagrant","title":"Ansible"},{"location":"ansible/#modules","text":"http://docs.ansible.com/ansible/latest/list_of_all_modules.html http://docs.ansible.com/ansible/latest/modules/apt_module.html http://docs.ansible.com/ansible/latest/modules/apt_repository_module.html http://docs.ansible.com/ansible/latest/modules/sysctl_module.html http://docs.ansible.com/ansible/latest/modules/user_module.html","title":"Modules"},{"location":"ansible/#examples","text":"","title":"Examples"},{"location":"ansible/#show-a-list-of-installed-modules","text":"ansible-doc --list","title":"Show a list of installed modules"},{"location":"ansible/#run-a-playbook-and-prompt-for-sudo-password","text":"ansible-playbook --ask-become-pass -i inventory/hosts.yaml create_users.yaml","title":"Run a playbook and prompt for sudo password"},{"location":"ansible/#run-an-ad-hoc-command","text":"You can run one-off ad-hoc commands by passing a module and args to the module. ansible localhost \\ -m get_url \\ -a \"mode=755 url=https://github.com/bcicen/ctop/releases/download/v0.7.1/ctop-0.7.1-linux-amd64 dest=/usr/local/bin/ctop checksum=sha256:38cfd92618ba2d92e0e1262c0c43d7690074b4b8dc77844b654f8e565166b577 owner=root group=root\"","title":"Run an ad-hoc command"},{"location":"ansible/#validate-and-inspect-your-inventory-file","text":"This command parses your inventory and group_vars and outputs a json data structure if no syntax faults are found. ansible-inventory -i inventory/hosts.yml --list","title":"Validate and inspect your inventory file"},{"location":"ansible/#use-arbitrary-groups-in-static-inventory-file","text":"$ nl -w 2 -s ' ' -ba inventory/example.yml 1 all: 2 hosts: 3 client: 4 ansible_host: 192.168.1.2 5 server: 6 ansible_host: 192.168.2.3 7 8 linux: 9 hosts: 10 server: 11 12 windows: 13 hosts: 14 client: 15 16 california: 17 hosts: 18 client: 19 server: $ ansible-inventory -i inventory/example.yml --graph @all: |--@california: | |--client | |--server |--@linux: | |--server |--@windows: | |--client","title":"Use arbitrary groups in static inventory file"},{"location":"ansible/#merge-multiple-inventory-files","text":"The below example gives higher precedence to the later files. $ cat foo.yml all: hosts: client: ansible_host: 192.168.1.2 service_hostname: hostname-from-file-1 server: ansible_host: 192.168.2.3 file_number: one $ cat bar.yml all: hosts: client: ansible_host: 10.1.2.3 server: ansible_host: 10.2.3.4 file_number: two $ ansible-inventory -i foo.yml -i bar.yml --list | json-to-yaml.py _meta: hostvars: client: ansible_host: 10.1.2.3 service_hostname: hostname-from-file-1 server: ansible_host: 10.2.3.4 file_number: two all: children: - ungrouped ungrouped: hosts: - client - server","title":"Merge multiple inventory files"},{"location":"ansible/#show-all-resolved-variables-for-a-given-inventory-host","text":"This will show all host vars, including variables resolved from all the different variable locations . ansible -i inventory target_hostname -m debug -a \"var=hostvars[inventory_hostname]\"","title":"Show all resolved variables for a given inventory host"},{"location":"ansible/#generate-an-idempotent-random-number","text":"This is similar to the Puppet fqdn_rand() function, which is really useful in cron jobs --- # defaults/main.yml demo_cron_minute: \"{{ 59 | random(seed=inventory_hostname) }}\" demo_cron_hour: \"{{ 23 | random(seed=inventory_hostname) }}\" See also: https://docs.ansible.com/ansible/latest/user_guide/playbooks_filters.html#random-number-filter","title":"Generate an idempotent random number"},{"location":"ansible/#simple-ansible-playbook","text":"This may be useful for testing syntax and experimenting with ansible modules. --- # playbook.yml - name: A local play hosts: localhost connection: local gather_facts: no tasks: - name: Run cmd shell: /bin/date register: cmd_out - debug: var: cmd_out.stdout ansible-playbook -i localhost playbook.yml Slightly more complicated example: # playbook.yml # run with: ansible-playbook -i localhost playbook.yml - name: A local play hosts: localhost connection: local gather_facts: no vars: region: test_region subnets: - subnet_name: Public_2a subnet_cidr: 192.168.100.0/26 subnet_az: \"{{ region }}_a\" - subnet_name: Public_2b subnet_cidr: 192.168.100.64/26 subnet_az: \"{{ region }}_b\" - subnet_name: Private_2a subnet_cidr: 192.168.100.128/26 subnet_az: \"{{ region }}_a\" - subnet_name: Private_2b subnet_cidr: 192.168.100.192/26 subnet_az: \"{{ region }}_b\" tasks: - name: Run cmd shell: echo \"{{ item.subnet_name }} {{ item.subnet_cidr }} {{ item.subnet_az }}\" register: cmd_out loop: \"{{ subnets }}\" - debug: var: cmd_out","title":"Simple ansible playbook"},{"location":"ansible/#links","text":"https://github.com/danielhoherd/ansible-playground - Examples I've made of Ansible techniques Ansible Tests with Molecule - https://molecule.readthedocs.io/en/latest/ / https://www.digitalocean.com/community/tutorials/how-to-test-ansible-roles-with-molecule Molecule sequence of scenario events - https://molecule.readthedocs.io/en/latest/configuration.html#scenario Test-driven infrastructure development with Ansible & Molecule - https://blog.codecentric.de/en/2018/12/test-driven-infrastructure-ansible-molecule/ / https://github.com/jonashackt/molecule-ansible-docker-vagrant","title":"Links"},{"location":"apfs/","text":"Notes here are current as of macOS 10.13, and don't apply specifically to any other devices that run APFS. Usage $ diskutil apfs 2017-11-04 18:23:55-0700 Usage: diskutil [quiet] ap[fs] <verb> <options> where <verb> is as follows: list (Show status of all current APFS Containers) convert (Nondestructively convert from HFS to APFS) create (Create a new APFS Container with one APFS Volume) createContainer (Create a new empty APFS Container) deleteContainer (Delete an APFS Container and reformat disks to HFS) resizeContainer (Resize an APFS Container and its disk space usage) addVolume (Export a new APFS Volume from an APFS Container) deleteVolume (Remove an APFS Volume from its APFS Container) eraseVolume (Erase contents of, but keep, an APFS Volume) changeVolumeRole (Change the Role metadata bits of an APFS Volume) unlockVolume (Unlock an encrypted APFS Volume which is locked) lockVolume (Lock an encrypted APFS Volume (diskutil unmount)) listCryptoUsers (List cryptographic users of encrypted APFS Volume) changePassphrase (Change the passphrase of a cryptographic user) setPassphraseHint (Set or clear passphrase hint of a cryptographic user) encryptVolume (Start async encryption of an unencrypted APFS Volume) decryptVolume (Start async decryption of an encrypted APFS Volume) updatePreboot (Update the APFS Volume's related APFS Preboot Volume) diskutil apfs <verb> with no options will provide help on that verb File clones APFS supports deduplicated file copies, which it calls clonefiles. Copying a file by option-dragging it in Finder creates a clonefile. To create a clonefile on the CLI use cp -c src dst . Creating clonefiless of any size file is instantaneous because no file data is actually being copied. This differs from hard links because if you modify the clone, only the new blocks will be written to disk, and the source of the cloned file will not be modified. Snapshots Snapshots appear to be tied pretty directly to Time Machine, and do not appear to be general purpose. There appear to be many limitations in how they can be used, and what information you can get about them. There was previously a tool called apfs_snapshot but it was removed before macOS 10.13 was released. Create a snapshot You cannot choose a name for your snapshot, it is tied to the date the snapshot was taken in the form of YYYY-MM-DD-HHMMSS. $ sudo tmutil localsnapshot Created local snapshot with date: 2017-11-04-183728 Show snapshots $ sudo tmutil listlocalsnapshots / com.apple.TimeMachine.2017-11-01-161748 com.apple.TimeMachine.2017-11-02-100755 com.apple.TimeMachine.2017-11-03-084837 com.apple.TimeMachine.2017-11-04-182813 Mount a snapshot $ mkdir snap_test $ sudo mount_apfs -s com.apple.TimeMachine.2017-11-04-192829 / \"${PWD}/snap_test\" Delete a snapshot You can only delete snapshots based off of their date. $ sudo tmutil deletelocalsnapshots 2017-11-04-183813 Deleted local snapshot '2017-11-04-183813' Delete all snapshots /usr/bin/tmutil listlocalsnapshots / | while read -r X ; do tmutil deletelocalsnapshots \"${X##*.}\" done Thin out snapshots On the given drive, reclaim the given space by thinning out snapshots. As of tmutil 4.0.0, you cannot use any data unit other than bytes. (EG: 1G or 1GB will not work) $ sudo tmutil thinlocalsnapshots / 250000000 Thinned local snapshots: 2017-11-04-184425 2017-11-04-184433 2017-11-04-184440 See also /System/Library/Filesystems/apfs.fs/Contents/Resources/apfs.util /System/Library/Filesystems/apfs.fs/Contents/Resources/apfs_invert /System/Library/Filesystems/apfs.fs/Contents/Resources/apfs_preflight_converter /System/Library/Filesystems/apfs.fs/Contents/Resources/apfs_stats Links Apple File System Rich Trouton - Storing our digital lives - Mac filesystems from MFS to APFS Apple File System Guide Russ Bishop - Apple File System A ZFS developer\u2019s analysis of the good and bad in Apple\u2019s new APFS file system","title":"Apple APFS"},{"location":"apfs/#usage","text":"$ diskutil apfs 2017-11-04 18:23:55-0700 Usage: diskutil [quiet] ap[fs] <verb> <options> where <verb> is as follows: list (Show status of all current APFS Containers) convert (Nondestructively convert from HFS to APFS) create (Create a new APFS Container with one APFS Volume) createContainer (Create a new empty APFS Container) deleteContainer (Delete an APFS Container and reformat disks to HFS) resizeContainer (Resize an APFS Container and its disk space usage) addVolume (Export a new APFS Volume from an APFS Container) deleteVolume (Remove an APFS Volume from its APFS Container) eraseVolume (Erase contents of, but keep, an APFS Volume) changeVolumeRole (Change the Role metadata bits of an APFS Volume) unlockVolume (Unlock an encrypted APFS Volume which is locked) lockVolume (Lock an encrypted APFS Volume (diskutil unmount)) listCryptoUsers (List cryptographic users of encrypted APFS Volume) changePassphrase (Change the passphrase of a cryptographic user) setPassphraseHint (Set or clear passphrase hint of a cryptographic user) encryptVolume (Start async encryption of an unencrypted APFS Volume) decryptVolume (Start async decryption of an encrypted APFS Volume) updatePreboot (Update the APFS Volume's related APFS Preboot Volume) diskutil apfs <verb> with no options will provide help on that verb","title":"Usage"},{"location":"apfs/#file-clones","text":"APFS supports deduplicated file copies, which it calls clonefiles. Copying a file by option-dragging it in Finder creates a clonefile. To create a clonefile on the CLI use cp -c src dst . Creating clonefiless of any size file is instantaneous because no file data is actually being copied. This differs from hard links because if you modify the clone, only the new blocks will be written to disk, and the source of the cloned file will not be modified.","title":"File clones"},{"location":"apfs/#snapshots","text":"Snapshots appear to be tied pretty directly to Time Machine, and do not appear to be general purpose. There appear to be many limitations in how they can be used, and what information you can get about them. There was previously a tool called apfs_snapshot but it was removed before macOS 10.13 was released.","title":"Snapshots"},{"location":"apfs/#create-a-snapshot","text":"You cannot choose a name for your snapshot, it is tied to the date the snapshot was taken in the form of YYYY-MM-DD-HHMMSS. $ sudo tmutil localsnapshot Created local snapshot with date: 2017-11-04-183728","title":"Create a snapshot"},{"location":"apfs/#show-snapshots","text":"$ sudo tmutil listlocalsnapshots / com.apple.TimeMachine.2017-11-01-161748 com.apple.TimeMachine.2017-11-02-100755 com.apple.TimeMachine.2017-11-03-084837 com.apple.TimeMachine.2017-11-04-182813","title":"Show snapshots"},{"location":"apfs/#mount-a-snapshot","text":"$ mkdir snap_test $ sudo mount_apfs -s com.apple.TimeMachine.2017-11-04-192829 / \"${PWD}/snap_test\"","title":"Mount a snapshot"},{"location":"apfs/#delete-a-snapshot","text":"You can only delete snapshots based off of their date. $ sudo tmutil deletelocalsnapshots 2017-11-04-183813 Deleted local snapshot '2017-11-04-183813'","title":"Delete a snapshot"},{"location":"apfs/#delete-all-snapshots","text":"/usr/bin/tmutil listlocalsnapshots / | while read -r X ; do tmutil deletelocalsnapshots \"${X##*.}\" done","title":"Delete all snapshots"},{"location":"apfs/#thin-out-snapshots","text":"On the given drive, reclaim the given space by thinning out snapshots. As of tmutil 4.0.0, you cannot use any data unit other than bytes. (EG: 1G or 1GB will not work) $ sudo tmutil thinlocalsnapshots / 250000000 Thinned local snapshots: 2017-11-04-184425 2017-11-04-184433 2017-11-04-184440","title":"Thin out snapshots"},{"location":"apfs/#see-also","text":"/System/Library/Filesystems/apfs.fs/Contents/Resources/apfs.util /System/Library/Filesystems/apfs.fs/Contents/Resources/apfs_invert /System/Library/Filesystems/apfs.fs/Contents/Resources/apfs_preflight_converter /System/Library/Filesystems/apfs.fs/Contents/Resources/apfs_stats","title":"See also"},{"location":"apfs/#links","text":"Apple File System Rich Trouton - Storing our digital lives - Mac filesystems from MFS to APFS Apple File System Guide Russ Bishop - Apple File System A ZFS developer\u2019s analysis of the good and bad in Apple\u2019s new APFS file system","title":"Links"},{"location":"aptly/","text":"\"Aptly is a swiss army knife for Debian repository management.\" https://github.com/sepulworld/aptly-vagrant","title":"Aptly"},{"location":"aria2/","text":"\"aria2 is a lightweight multi-protocol & multi-source command-line download utility. It supports HTTP/HTTPS, FTP, SFTP, BitTorrent and Metalink. aria2 can be manipulated via built-in JSON-RPC and XML-RPC interfaces.\" - https://aria2.github.io/ Of particular interest is the ability to download a single file from multiple sources, even using multiple protocols, to have increased download speed. See Also curl httpstat - download and show a some useful connection information wget","title":"Aria2"},{"location":"aria2/#see-also","text":"curl httpstat - download and show a some useful connection information wget","title":"See Also"},{"location":"arpwatch/","text":"\"arpwatch - keep track of ethernet/ip address pairings\" - man arpwatch Examples Fork and log to file, not to e-mail arpwatch -Q tail -F /var/lib/arpwatch/arp.dat","title":"arpwatch"},{"location":"arpwatch/#examples","text":"","title":"Examples"},{"location":"arpwatch/#fork-and-log-to-file-not-to-e-mail","text":"arpwatch -Q tail -F /var/lib/arpwatch/arp.dat","title":"Fork and log to file, not to e-mail"},{"location":"atomicparsley/","text":"AtomicParsley is a lightweight command line program for reading, parsing and setting metadata into MPEG-4 files. This is a functional mp4 equivalent of what i3dv2 is for mp3 files. Examples Set metadata on multiple files Unfortunately the syntax of this tool requires you to edit one file at a time, so you have to iterate each item of an album using shell loops or xargs or whatever you prefer. for file in *.m4a ; do AtomicParsley \"${file}\" --artist \"Various Artists\" ; done ; Remove Personally Identifiable Information (pii) from files Useful if you want to remove your personal info from iTunes Match files. for file in *.m4a ; do AtomicParsley \\ \"$file\" \\ --DeepScan \\ --manualAtomRemove \"moov.trak.mdia.minf.stbl.mp4a.pinf\" \\ --manualAtomRemove \"moov.udta.meta.ilst.----.name:[iTunMOVI]\" \\ --manualAtomRemove \"moov.udta.meta.ilst.apID\" \\ --manualAtomRemove \"moov.udta.meta.ilst.atID\" \\ --manualAtomRemove \"moov.udta.meta.ilst.cnID\" \\ --manualAtomRemove \"moov.udta.meta.ilst.cprt\" \\ --manualAtomRemove \"moov.udta.meta.ilst.flvr\" \\ --manualAtomRemove \"moov.udta.meta.ilst.geID\" \\ --manualAtomRemove \"moov.udta.meta.ilst.plID\" \\ --manualAtomRemove \"moov.udta.meta.ilst.purd\" \\ --manualAtomRemove \"moov.udta.meta.ilst.rtng\" \\ --manualAtomRemove \"moov.udta.meta.ilst.sfID\" \\ --manualAtomRemove \"moov.udta.meta.ilst.soal\" \\ --manualAtomRemove \"moov.udta.meta.ilst.stik\" \\ --manualAtomRemove \"moov.udta.meta.ilst.xid\" \\ done","title":"AtomicParsley"},{"location":"atomicparsley/#examples","text":"","title":"Examples"},{"location":"atomicparsley/#set-metadata-on-multiple-files","text":"Unfortunately the syntax of this tool requires you to edit one file at a time, so you have to iterate each item of an album using shell loops or xargs or whatever you prefer. for file in *.m4a ; do AtomicParsley \"${file}\" --artist \"Various Artists\" ; done ;","title":"Set metadata on multiple files"},{"location":"atomicparsley/#remove-personally-identifiable-information-pii-from-files","text":"Useful if you want to remove your personal info from iTunes Match files. for file in *.m4a ; do AtomicParsley \\ \"$file\" \\ --DeepScan \\ --manualAtomRemove \"moov.trak.mdia.minf.stbl.mp4a.pinf\" \\ --manualAtomRemove \"moov.udta.meta.ilst.----.name:[iTunMOVI]\" \\ --manualAtomRemove \"moov.udta.meta.ilst.apID\" \\ --manualAtomRemove \"moov.udta.meta.ilst.atID\" \\ --manualAtomRemove \"moov.udta.meta.ilst.cnID\" \\ --manualAtomRemove \"moov.udta.meta.ilst.cprt\" \\ --manualAtomRemove \"moov.udta.meta.ilst.flvr\" \\ --manualAtomRemove \"moov.udta.meta.ilst.geID\" \\ --manualAtomRemove \"moov.udta.meta.ilst.plID\" \\ --manualAtomRemove \"moov.udta.meta.ilst.purd\" \\ --manualAtomRemove \"moov.udta.meta.ilst.rtng\" \\ --manualAtomRemove \"moov.udta.meta.ilst.sfID\" \\ --manualAtomRemove \"moov.udta.meta.ilst.soal\" \\ --manualAtomRemove \"moov.udta.meta.ilst.stik\" \\ --manualAtomRemove \"moov.udta.meta.ilst.xid\" \\ done","title":"Remove Personally Identifiable Information (pii) from files"},{"location":"atop/","text":"\"Atop is an ASCII full-screen performance monitor for Linux that is capable of reporting the activity of all processes (even if processes have finished during the interval), daily logging of system and process activity for long-term analysis, highlighting overloaded system resources by using colors, etc.\" - https://www.atoptool.nl/ The feature that sets atop apart from other process top tools is that it catches short-lived processes that would be left out of period sampling that other tools use. See also Top variant list","title":"atop"},{"location":"atop/#see-also","text":"Top variant list","title":"See also"},{"location":"autonomous-vehicles/","text":"Terms ACES: Automated, Connected, Electric, Shared Levels: Refers to the 6 levels of autonomous vehicles SAE: Society of Automotive Engineers Autonomy Levels L0: No automation. Human performs all operations. Base cost. L1: Driver assistance. Human performs most driving operations, system can perform steering OR speed controls. Increased cost. L2: Partial automation. Human performs most driving operations, system can perform steering AND speed controls. Increased cost. L3: Conditional automation. System can perform all driving operations, human must be available to intervene in complex situations. Increased cost. L4: High automation. System can perform all driving operations. Complex situations do not require a human to intervene but intervention is still an option. Decreased cost. L5: Full automation. System performs all driving operations. Humans are all passengers with no requirement to intervene. Decreased cost. Links http://apollo.auto/ https://autonomoustuff.com/ https://renovo.auto http://some-ip.com/ https://www.autosar.org/ http://www.ros.org/ https://en.wikipedia.org/wiki/Units_of_transportation_measurement https://www.automotivelinux.org","title":"Autonomous Vehicle Links"},{"location":"autonomous-vehicles/#terms","text":"ACES: Automated, Connected, Electric, Shared Levels: Refers to the 6 levels of autonomous vehicles SAE: Society of Automotive Engineers","title":"Terms"},{"location":"autonomous-vehicles/#autonomy-levels","text":"L0: No automation. Human performs all operations. Base cost. L1: Driver assistance. Human performs most driving operations, system can perform steering OR speed controls. Increased cost. L2: Partial automation. Human performs most driving operations, system can perform steering AND speed controls. Increased cost. L3: Conditional automation. System can perform all driving operations, human must be available to intervene in complex situations. Increased cost. L4: High automation. System can perform all driving operations. Complex situations do not require a human to intervene but intervention is still an option. Decreased cost. L5: Full automation. System performs all driving operations. Humans are all passengers with no requirement to intervene. Decreased cost.","title":"Autonomy Levels"},{"location":"autonomous-vehicles/#links","text":"http://apollo.auto/ https://autonomoustuff.com/ https://renovo.auto http://some-ip.com/ https://www.autosar.org/ http://www.ros.org/ https://en.wikipedia.org/wiki/Units_of_transportation_measurement https://www.automotivelinux.org","title":"Links"},{"location":"avahi/","text":"The Avahi mDNS/DNS-SD daemon implements Apple's Zeroconf architecture (also known as \"Rendezvous\" or \"Bonjour\"). Tips After installing avahi-daemon it may not start. To fix this you may need to run service messagebus start Service types are defined in /usr/share/avahi/service-types Service configs Correctly formatted and named files in /etc/avahi/services/whatever.service are loaded on the fly, no need to restart avahi-daemon . If your service doesn't immediately show up, check syslog for errors. <?xml version=\"1.0\" standalone='no'?><!--*-nxml-*--> <!DOCTYPE service-group SYSTEM \"avahi-service.dtd\"> <service-group> <name replace-wildcards=\"yes\">%h</name> <service> <type>_ssh._tcp</type> <port>22</port> </service> <service> <type>_http._tcp</type> <port>80</port> </service> </service-group>","title":"Avahi"},{"location":"avahi/#tips","text":"After installing avahi-daemon it may not start. To fix this you may need to run service messagebus start Service types are defined in /usr/share/avahi/service-types","title":"Tips"},{"location":"avahi/#service-configs","text":"Correctly formatted and named files in /etc/avahi/services/whatever.service are loaded on the fly, no need to restart avahi-daemon . If your service doesn't immediately show up, check syslog for errors. <?xml version=\"1.0\" standalone='no'?><!--*-nxml-*--> <!DOCTYPE service-group SYSTEM \"avahi-service.dtd\"> <service-group> <name replace-wildcards=\"yes\">%h</name> <service> <type>_ssh._tcp</type> <port>22</port> </service> <service> <type>_http._tcp</type> <port>80</port> </service> </service-group>","title":"Service configs"},{"location":"awk/","text":"\"pattern-directed scanning and processing language\" - man awk Examples Some of these require GNU awk. Print the first column of a file awk '${print $1}' filename.txt Print column 2 if column 1 matches a string ps aux | awk '$1 == \"root\" {print $2}' Sort a file by line lengths awk '{print length, $0}' testfile.txt | sort -n TDL to CSV awk '{gsub(\"\\t\",\"\\\",\\\"\",$0); print;}' | sed 's#^#\"#;s#$#\"#;' Print the first column of every other line % is the modulus operator, which finds the remainder after an integer divide. awk 'NR % 2 == 0 { print $1 }' Print only even numbered lines ls | awk 'NR % 2 == 0 { print $0 }' Print only odd numbered lines ls | awk 'NR % 2 != 0 { print $0 }' Print even numbered lines on the same line before odd numbered lines awk '{if (NR%2==0) { print $0 \" \" prev } else { prev=$0 }}' Print sum all the first columns of each line in a file awk '{sum += $1} END {print sum}' filename Print sum and average of the first column of a file awk '{sum += $1} END {avg = sum/NR ; printf \"Sum: %s\\nAverage: %s\\n\", sum, avg}' foo.txt Split file by recurring string This will create a new file every time the string \"SERVER\" is found, essentially splitting the file by that string. Concatenating all of the output files would create the original file (potentially adding an extra newline). awk '/SERVER/{n++}{print >\"out\" sprintf(\"%02d\", n) \".txt\" }' example.txt Show count of syslog messages per minute awk -F: {'print $1 `\u201c`:`\u201d` $2'} /var/log/messages |uniq -c Show count of root logins per minute awk -F: '/root/{print $1 \":\" $2}' /var/log/auth.log |uniq -c Print lines in ls where UID is numeric ls -la | awk '$3 ~/[0-9]/{print}' Show only zfs snapshots whose size is zero zfs list -t snapshot | awk '$2 == 0' Print a line if the third field does not match a regex tcpdump -r ops1prod-syn.cap | sort -k2 | awk '$3 !~ /ztmis.prod/ { print }' Show 500 errors in a standard apache access log awk '$9 ~ /5[0-9][0-9]/' www_zoosk_access.log Show total rss and vsz count for all cronolog processes ps aux | grep -i cronolo[g] | awk '{vsz += $5; rss += $6} END {print \"vsz total = \"vsz ; print \"rss total = \"rss}' Get IPv4 address on BSD/OSX ifconfig | awk '$1 == \"inet\" && $2 != \"127.0.0.1\" {print $2}' Get IPv6 address on BSD/OSX ifconfig | awk '$1 == \"inet6\" && $2 !~ \"::1|.*lo\" {print $2}' Print the last element ls -la | awk -F\" \" '{print $NF}' Print 2nd to last element ls -la | awk -F\" \" '{print $(NF - 1)}' Print the previous line on string match This works by storing the previous line. If the current line matches the regex, the previous line is printed from the stored value. $ awk '/32 host/ { print previous_line } {previous_line=$0}' /proc/net/fib_trie | column -t | sort -u |-- 10.134.243.137 |-- 127.0.0.1 |-- 169.50.9.172 See Also http://www.grymoire.com/Unix/Awk.html","title":"awk"},{"location":"awk/#examples","text":"Some of these require GNU awk.","title":"Examples"},{"location":"awk/#print-the-first-column-of-a-file","text":"awk '${print $1}' filename.txt","title":"Print the first column of a file"},{"location":"awk/#print-column-2-if-column-1-matches-a-string","text":"ps aux | awk '$1 == \"root\" {print $2}'","title":"Print column 2 if column 1 matches a string"},{"location":"awk/#sort-a-file-by-line-lengths","text":"awk '{print length, $0}' testfile.txt | sort -n","title":"Sort a file by line lengths"},{"location":"awk/#tdl-to-csv","text":"awk '{gsub(\"\\t\",\"\\\",\\\"\",$0); print;}' | sed 's#^#\"#;s#$#\"#;'","title":"TDL to CSV"},{"location":"awk/#print-the-first-column-of-every-other-line","text":"% is the modulus operator, which finds the remainder after an integer divide. awk 'NR % 2 == 0 { print $1 }'","title":"Print the first column of every other line"},{"location":"awk/#print-only-even-numbered-lines","text":"ls | awk 'NR % 2 == 0 { print $0 }'","title":"Print only even numbered lines"},{"location":"awk/#print-only-odd-numbered-lines","text":"ls | awk 'NR % 2 != 0 { print $0 }'","title":"Print only odd numbered lines"},{"location":"awk/#print-even-numbered-lines-on-the-same-line-before-odd-numbered-lines","text":"awk '{if (NR%2==0) { print $0 \" \" prev } else { prev=$0 }}'","title":"Print even numbered lines on the same line before odd numbered lines"},{"location":"awk/#print-sum-all-the-first-columns-of-each-line-in-a-file","text":"awk '{sum += $1} END {print sum}' filename","title":"Print sum all the first columns of each line in a file"},{"location":"awk/#print-sum-and-average-of-the-first-column-of-a-file","text":"awk '{sum += $1} END {avg = sum/NR ; printf \"Sum: %s\\nAverage: %s\\n\", sum, avg}' foo.txt","title":"Print sum and average of the first column of a file"},{"location":"awk/#split-file-by-recurring-string","text":"This will create a new file every time the string \"SERVER\" is found, essentially splitting the file by that string. Concatenating all of the output files would create the original file (potentially adding an extra newline). awk '/SERVER/{n++}{print >\"out\" sprintf(\"%02d\", n) \".txt\" }' example.txt","title":"Split file by recurring string"},{"location":"awk/#show-count-of-syslog-messages-per-minute","text":"awk -F: {'print $1 `\u201c`:`\u201d` $2'} /var/log/messages |uniq -c","title":"Show count of syslog messages per minute"},{"location":"awk/#show-count-of-root-logins-per-minute","text":"awk -F: '/root/{print $1 \":\" $2}' /var/log/auth.log |uniq -c","title":"Show count of root logins per minute"},{"location":"awk/#print-lines-in-ls-where-uid-is-numeric","text":"ls -la | awk '$3 ~/[0-9]/{print}'","title":"Print lines in ls where UID is numeric"},{"location":"awk/#show-only-zfs-snapshots-whose-size-is-zero","text":"zfs list -t snapshot | awk '$2 == 0'","title":"Show only zfs snapshots whose size is zero"},{"location":"awk/#print-a-line-if-the-third-field-does-not-match-a-regex","text":"tcpdump -r ops1prod-syn.cap | sort -k2 | awk '$3 !~ /ztmis.prod/ { print }'","title":"Print a line if the third field does not match a regex"},{"location":"awk/#show-500-errors-in-a-standard-apache-access-log","text":"awk '$9 ~ /5[0-9][0-9]/' www_zoosk_access.log","title":"Show 500 errors in a standard apache access log"},{"location":"awk/#show-total-rss-and-vsz-count-for-all-cronolog-processes","text":"ps aux | grep -i cronolo[g] | awk '{vsz += $5; rss += $6} END {print \"vsz total = \"vsz ; print \"rss total = \"rss}'","title":"Show total rss and vsz count for all cronolog processes"},{"location":"awk/#get-ipv4-address-on-bsdosx","text":"ifconfig | awk '$1 == \"inet\" && $2 != \"127.0.0.1\" {print $2}'","title":"Get IPv4 address on BSD/OSX"},{"location":"awk/#get-ipv6-address-on-bsdosx","text":"ifconfig | awk '$1 == \"inet6\" && $2 !~ \"::1|.*lo\" {print $2}'","title":"Get IPv6 address on BSD/OSX"},{"location":"awk/#print-the-last-element","text":"ls -la | awk -F\" \" '{print $NF}'","title":"Print the last element"},{"location":"awk/#print-2nd-to-last-element","text":"ls -la | awk -F\" \" '{print $(NF - 1)}'","title":"Print 2nd to last element"},{"location":"awk/#print-the-previous-line-on-string-match","text":"This works by storing the previous line. If the current line matches the regex, the previous line is printed from the stored value. $ awk '/32 host/ { print previous_line } {previous_line=$0}' /proc/net/fib_trie | column -t | sort -u |-- 10.134.243.137 |-- 127.0.0.1 |-- 169.50.9.172","title":"Print the previous line on string match"},{"location":"awk/#see-also","text":"http://www.grymoire.com/Unix/Awk.html","title":"See Also"},{"location":"awless/","text":"\"A Mighty CLI for AWS\" - https://github.com/wallix/awless Examples Example templates - https://github.com/wallix/awless-templates A lot of these syntax examples can be found by issuing the command, verb and entity but no parameters. Such as awless create stack , which will drop you into a prompt series to complete the necessary and optional parameters. List ec2 instances sorted by uptime $ awless list instances --sort=uptime | ID | ZONE | NAME | STATE | TYPE | PUBLIC IP | PRIVATE IP | UPTIME \u25b2 | KEYPAIR | |---------------------|------------|-------------------------|---------|------------|-----------|---------------|----------|---------| | i-050ad501b33c6ad07 | us-west-1a | faruko-nal | running | m4.xlarge | | 172.19.15.172 | 85 mins | foo-ops | | i-5b381e9b | us-west-1b | planted-collector11.foo | running | m4.xlarge | | 172.27.26.159 | 6 days | foo-ops | | i-04ced9880586c009b | us-west-1a | hadoop07.foo | running | m4.4xlarge | | 172.27.37.100 | 8 days | foo-ops | | i-0e583dcd3bc2444d8 | us-west-1a | db-na-historical06.foo | running | m2.4xlarge | | 172.19.48.79 | 12 days | foo-ops | Sum the amount of unattached disks in your environment awless list volumes \\ --filter state=available \\ --format json | jq .[].Size | awk '{sum += $1 ; count += 1 ;} END {print sum \"G in \" count \" volumes\"}' Switch to a different AWS profile This uses the ~/.aws/credentials file for its profiles Short way: awless switch prod Long way: awless config set aws.profile prod Customize output columns awless list instances --columns name,type,launched Add a user to a group awless \\ --aws-profile govcloud \\ --aws-region us-gov-west-1 \\ attach user \\ group=SystemAdministrators \\ name=SpaceGhost Create an access key for a user This creates an access key and saves it in ~/.aws/credentials awless \\ --aws-profile govcloud \\ --aws-region us-gov-west-1 \\ create accesskey \\ user=SpaceGhost \\ save=true Create a tag awless create tag key=test_tag resource=i-9ba90158 value=true Delete a tag awless delete tag key=test_tag_dhoherd resource=i-9ba90158 Create an instance awless create instance \\ count=1 \\ image=ami-5ab82fa8 \\ keypair=ops \\ name=new-hostname \\ securitygroup=[sg-c4321fd1,sg-c4321cb0] \\ subnet=subnet-c4321c33 \\ type=t2.medium See also amazon amazon-ec2 aws-cloudformation awscli","title":"awless"},{"location":"awless/#examples","text":"Example templates - https://github.com/wallix/awless-templates A lot of these syntax examples can be found by issuing the command, verb and entity but no parameters. Such as awless create stack , which will drop you into a prompt series to complete the necessary and optional parameters.","title":"Examples"},{"location":"awless/#list-ec2-instances-sorted-by-uptime","text":"$ awless list instances --sort=uptime | ID | ZONE | NAME | STATE | TYPE | PUBLIC IP | PRIVATE IP | UPTIME \u25b2 | KEYPAIR | |---------------------|------------|-------------------------|---------|------------|-----------|---------------|----------|---------| | i-050ad501b33c6ad07 | us-west-1a | faruko-nal | running | m4.xlarge | | 172.19.15.172 | 85 mins | foo-ops | | i-5b381e9b | us-west-1b | planted-collector11.foo | running | m4.xlarge | | 172.27.26.159 | 6 days | foo-ops | | i-04ced9880586c009b | us-west-1a | hadoop07.foo | running | m4.4xlarge | | 172.27.37.100 | 8 days | foo-ops | | i-0e583dcd3bc2444d8 | us-west-1a | db-na-historical06.foo | running | m2.4xlarge | | 172.19.48.79 | 12 days | foo-ops |","title":"List ec2 instances sorted by uptime"},{"location":"awless/#sum-the-amount-of-unattached-disks-in-your-environment","text":"awless list volumes \\ --filter state=available \\ --format json | jq .[].Size | awk '{sum += $1 ; count += 1 ;} END {print sum \"G in \" count \" volumes\"}'","title":"Sum the amount of unattached disks in your environment"},{"location":"awless/#switch-to-a-different-aws-profile","text":"This uses the ~/.aws/credentials file for its profiles Short way: awless switch prod Long way: awless config set aws.profile prod","title":"Switch to a different AWS profile"},{"location":"awless/#customize-output-columns","text":"awless list instances --columns name,type,launched","title":"Customize output columns"},{"location":"awless/#add-a-user-to-a-group","text":"awless \\ --aws-profile govcloud \\ --aws-region us-gov-west-1 \\ attach user \\ group=SystemAdministrators \\ name=SpaceGhost","title":"Add a user to a group"},{"location":"awless/#create-an-access-key-for-a-user","text":"This creates an access key and saves it in ~/.aws/credentials awless \\ --aws-profile govcloud \\ --aws-region us-gov-west-1 \\ create accesskey \\ user=SpaceGhost \\ save=true","title":"Create an access key for a user"},{"location":"awless/#create-a-tag","text":"awless create tag key=test_tag resource=i-9ba90158 value=true","title":"Create a tag"},{"location":"awless/#delete-a-tag","text":"awless delete tag key=test_tag_dhoherd resource=i-9ba90158","title":"Delete a tag"},{"location":"awless/#create-an-instance","text":"awless create instance \\ count=1 \\ image=ami-5ab82fa8 \\ keypair=ops \\ name=new-hostname \\ securitygroup=[sg-c4321fd1,sg-c4321cb0] \\ subnet=subnet-c4321c33 \\ type=t2.medium","title":"Create an instance"},{"location":"awless/#see-also","text":"amazon amazon-ec2 aws-cloudformation awscli","title":"See also"},{"location":"aws-cloudformation/","text":"\"AWS CloudFormation is a service that helps you model and set up your Amazon Web Services resources so that you can spend less time managing those resources and more time focusing on your applications that run in AWS.\" - http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/Welcome.html Links http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html CloudFormer - CloudFormer is a template creation beta tool that creates an AWS CloudFormation template from existing AWS resources in your account. You select any supported AWS resources that are running in your account, and CloudFormer creates a template in an Amazon S3 bucket. Sceptre - Sceptre is a tool to drive Cloudformation. Sceptre manages the creating, updating and deletion of stacks, and provides meta commands to allow users to get information about their stacks.","title":"Amazon AWS Cloudformation"},{"location":"aws-cloudformation/#links","text":"http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html CloudFormer - CloudFormer is a template creation beta tool that creates an AWS CloudFormation template from existing AWS resources in your account. You select any supported AWS resources that are running in your account, and CloudFormer creates a template in an Amazon S3 bucket. Sceptre - Sceptre is a tool to drive Cloudformation. Sceptre manages the creating, updating and deletion of stacks, and provides meta commands to allow users to get information about their stacks.","title":"Links"},{"location":"awscli/","text":"Official Amazon AWS command-line interface - https://aws.amazon.com/cli Example usage Show subnets for a particular region and account aws --profile=dev --region=us-west-2 ec2 describe-subnets See Also Amazon awless","title":"Amazon awscli"},{"location":"awscli/#example-usage","text":"","title":"Example usage"},{"location":"awscli/#show-subnets-for-a-particular-region-and-account","text":"aws --profile=dev --region=us-west-2 ec2 describe-subnets","title":"Show subnets for a particular region and account"},{"location":"awscli/#see-also","text":"Amazon awless","title":"See Also"},{"location":"backups/","text":"Notes about backing up data. Links http://duplicity.nongnu.org https://www.nixtutor.com/linux/off-site-encrypted-backups-using-rsync-and-aes http://www.nongnu.org/rdiff-backup","title":"Backups"},{"location":"backups/#links","text":"http://duplicity.nongnu.org https://www.nixtutor.com/linux/off-site-encrypted-backups-using-rsync-and-aes http://www.nongnu.org/rdiff-backup","title":"Links"},{"location":"badblocks/","text":"badblocks is a program to test storage devices for bad blocks. - https://wiki.archlinux.org/index.php/badblocks Examples Destroy all data on a disk while logging bad blocks # -v verbose output writes error info to stderr # -s show scan progress, including percent complete, time elapsed, and error count # -w destructive write test, vs -n (nondestructive read/write test) # -b 4096 byte blocks # -t random test pattern # -o output file containing list of bad blocks, which can be passed back to badblocks, fsck or mke2fs badblocks -v -s -w -b 4096 -t random -o ~/sdc.txt /dev/sdc See also dcfldd dd ddrescue pv","title":"badblocks"},{"location":"badblocks/#examples","text":"","title":"Examples"},{"location":"badblocks/#destroy-all-data-on-a-disk-while-logging-bad-blocks","text":"# -v verbose output writes error info to stderr # -s show scan progress, including percent complete, time elapsed, and error count # -w destructive write test, vs -n (nondestructive read/write test) # -b 4096 byte blocks # -t random test pattern # -o output file containing list of bad blocks, which can be passed back to badblocks, fsck or mke2fs badblocks -v -s -w -b 4096 -t random -o ~/sdc.txt /dev/sdc","title":"Destroy all data on a disk while logging bad blocks"},{"location":"badblocks/#see-also","text":"dcfldd dd ddrescue pv","title":"See also"},{"location":"bash/","text":"This doc has been automatically converted and still needs some cleanup. Bash is one of the most common mainstream unix shells. Tricks and Usage Navigating on the command line The following can be seen by running: stty -a ctrl-a - move cursor to the beginning of the line ctrl-e - move cursor to the end of the line ctrl-l - do a \"clear\" on the terminal window ctrl-r - reverse history command search ctrl-t - get status of foreground process ctrl-w - delete previous word View a list of all commands, etc.. compgen -b will list all the built-ins you could run. compgen -a will list all the aliases you could run. compgen -c will list all the commands you could run. compgen -k will list all the keywords you could run. compgen -A function will list all the functions you could run. compgen -back will list all the above in one go. Remove leading zeroes for X in 00{1..20..2} ; do echo \"$X == $((10#${X}))\" ; done ; Or... for X in {1..50..5} ; do Y=00${X} ; echo \"${X} with zeroes is ${Y} and removed with bc is $(echo ${Y} | bc)\" ; done ; Convert base 36 to decimal This converts the base 36 number z to a decimal value echo $((36#z)) Run a command for 5 seconds, then kill it ping -f & sleep 5 ; kill %1 Alternatively, use the timeout command if it's available. In macOS this can be installed through homebrew install coreutils and accessed with gtimeout . timeout 300 cmd Test if a variable is empty if [[ -z \"$var\" ]] Date For date stuff, see date, because it's different depending on platform. Show RANDOM statistics for X in {0..9999} ; do echo $(($RANDOM % 5)) ; done | sort | uniq -c named pipes mkfifo baz ; ps aux > baz then in another terminal cat baz alternate redirection outputs exec 3> /tmp/baz ; ps aux >&3 # sends the output of ps aux to /tmp/baz Redirect all output of a script into a file This is not bash specific, but works in bash. #!/usr/bin/env bash exec >> /tmp/$0.log exec 2>&1 date \"+%F %T%z $0 This is stdout\" date \"+%F %T%z $0 This is stderr\" >&2 Show size of each user's home folder getent passwd | while IFS=: read -r user _ uid _ _ home _ ; do if [[ $uid -ge 500 ]] ; then printf \"$user \" ; sudo du -sh $home ; fi ; done Previous command's args mkdir temp ; cd !!:* Be aware of the location of the tokens. eg: mkdir -p {foo,bar}/{a,b,c} ; stat !!:* creates a problem because you can't stat -p so you must stat -p !!:2* Debug a script This will show everything bash is executing bash -x scriptname.sh Or debug with a function: function debug { if [ ${debug:-0} -gt 0 ] ; then echo $@ 2>&1 ; fi ; } Find where all the inodes are find ~/ -type d -print0 | xargs -I %% -0 bash -c \"echo -n %% ; ls -a '%%' | wc -l\" >> ~/inodes.txt Build and print an array array=(\"one is the first element\"); array+=(\"two is the second element\" \"three is the third\"); echo \"${array[@]}\" This is useful for building command line strings. For example, gpsbabel requires each input file to be prepended with -f . The following script takes a list of files and uses a bash array to create a command line in the form of gpsbabel -i gpx -f input_file_1.gpx -f input_file_2.gpx -o gpx -F output.gpx #!/usr/bin/env bash # Check for at least one argument, print usage if fail if [ $# -lt 2 ] ; then echo \"This script merges gpx files and requires at least two gpx files passed as arguments. Output is output.gpx\"; echo \"Usage: $0 <gpx file> <gpx file> [...<gpx file>]\"; exit 1; fi # Create an array of arguments to pass to gpsbabel args=(); for item in \"$@\" ; do if [ -f \"$item\" ] || [ -h \"$item\" ] ; then args+=( \"-f\" \"$item\" ); else echo \"Skipping $item, it's not a file or symlink.\" fi done; # Verify we have at least two files to work with if [ \"${#args[@]}\" -lt 4 ] ; then echo \"We don't have enough actual files to work with. Exiting.\" exit 1 fi gpsbabel -i gpx \"${args[@]}\" -o gpx -F output.gpx Build and print an associative array (dict, hash) declare -A animals=( [\"cow\"]=\"moo\" [\"dog\"]=\"woof woof\" [\"cat\"]=\"meow\" ) ; for animal in \"${!animals[@]}\" ; do echo \"The $animal says '${animals[$animal]}'\" ; done ; Show permissions in rwx and octal format Linux: stat -c '%A %a %n' filename OSX: stat -f '%A %N' filename Find the length of a variable echo ${#SHELL} Print all variables that start with the substring SH echo ${!SH*} Tertiary type variables ${V:-D} # means \"return the value of the environment variable V or the string D if V isn't set. Do a command, and if it returns false, so some more stuff while ! command_that_will_fail ; do something_else ; done ; Print two digit months echo {1..12} may not work. If not, use echo $(seq -w 1 12) Get filename, extension or path Taken from http://mywiki.wooledge.org/BashFAQ/073 Rename files to a sequence and change their extension at the same time ls | while read -r line ; do stub=${line%.*} ; (( i += 1 )) ; mv \"${line}\" \"${i}-${stub}.txt3\" ; done ; FullPath=/path/to/name4afile-00809.ext # result: # /path/to/name4afile-00809.ext Filename=${FullPath##*/} # name4afile-00809.ext PathPref=${FullPath%\"$Filename\"} # /path/to/ FileStub=${Filename%.*} # name4afile-00809 FileExt=${Filename#\"$FileStub\"} # .ext Sort a line by spaces s=( whiskey tango foxtrot ); sorted=$(printf \"%s\\n\"` `${s[@]}|sort); echo $sorted Calculate the difference between two dates echo $(( $(gdate +%s -d 20120203) - $(gdate +%s -d 20120115) )) substring replace a variable This is not regex, just a simple string replacement. # ${VAR/search/replace} does only the first # ${VAR//search/replace} does all replacements echo \"Paths in your path: ${PATH//:/ }\" Subtract two from a MAC address # printf -v defines a variable instead of printing to stdout printf -v dec \"%d\" 0x$(echo 00:25:9c:52:1c:2a | sed 's/://g') ; let dec=${dec}-2 ; printf \"%012X\" ${dec} \\ | sed -E 's/(..)(..)(..)(..)(..)(..)/\\1:\\2:\\3:\\4:\\5:\\6/g' Print the last for chars of a variable echo ${foo:$((${#foo}-4))} echo ${foo: -4} The space is necessary to prevent it from doing a completely different thing. See the next example... Dereference a variable $ for var in ${!BASH_V*} ; do echo \"${var}: ${!var}\" ; done ; BASH_VERSINFO: 5 BASH_VERSION: 5.0.7(1)-release Print something else if a variable doesn't exist echo ${foo:-foo isn't assigned} echo ${foo:-${bar}} This can even be recursively done... echo ${foo:-${bar:-foo and bar are not assigned}} Print every third number starting with 1 and ending with 30 echo {1..30..3} Print every 5th letter of the alphabet echo {a..z..5} Process all lines, but print out status about what line we are on every Nth line Sometimes during a series of long-running jobs you want to see the status of where you are at, or at least some indicator that things have not paused. when ctrl-t is not available (and even when it is) this pattern can help you monitor that things are still moving a long. N=0 find \"/usr/bin\" -type f | while read -r X ; do N=$((N + 1)) [[ \"$((N % 50))\" -eq 0 ]] && date \"+%F %T file number $N $X\" >&2 shasum -a 512 \"${X}\" >> ~/usr_bin_shasums.txt done Example terminal output from the above command, while all shasum output goes into ~/usr_bin_shasums.txt : $ find \"/usr/bin\" -type f | > while read -r X ; do > N=$((N + 1)) > [[ \"$((N % 50))\" -eq 0 ]] && date \"+%F %T file number $N $X\" >&2 > shasum -a 512 \"${X}\" >> ~/usr_bin_shasums.txt > done 2018-02-24 15:30:29 file number 50 /usr/bin/toe 2018-02-24 15:30:30 file number 100 /usr/bin/db_hotbackup 2018-02-24 15:30:32 file number 150 /usr/bin/host 2018-02-24 15:30:33 file number 200 /usr/bin/groffer 2018-02-24 15:30:35 file number 250 /usr/bin/mail 2018-02-24 15:30:36 file number 300 /usr/bin/dbicadmin 2018-02-24 15:30:38 file number 350 /usr/bin/fwkpfv 2018-02-24 15:30:39 file number 400 /usr/bin/tab2space Make a directory structure of every combination of /adjective/noun mkdir -p {red,green,blue}/{fish,bird,flower} Generate a zero padded random 2 byte hex number printf \"%02X\\n\" $((RANDOM % 256)) grep many log files and sort output by date sudo grep cron /var/log/* \\ | sed 's/:/ /' \\ | while read file month day hour line ; do date -d \"$month $day $hour\" \"+%F %T%z ${file} ${line}\" ; done \\ | sort Get command line switches while getopts p:l:t: opt; do case $opt in p) pages=$OPTARG ;; l) length=$OPTARG ;; t) time=$OPTARG ;; esac done shift $((OPTIND - 1)) echo \"pages is ${pages}\" echo \"length is ${length}\" echo \"time is ${time}\" echo \"\\$1 is $1\" echo \"\\$2 is $2\" Call this script as ./foo.sh -p \"this is p\" -l llll -t this\\ is\\ t foo bar Files These files can change the behavior of bash. .bash_profile ~/.bash_profile is executed every time you log into the system or initiate a login shell. Inclusion of things that write to stdout is allowed here. If you want to write scripts that change your interactive shell environment, such as changing your CWD, define functions here instead of using stand-alone scripts. Example .bash_profile The ~/.bash_profile file can be quite long and complicated. The following example is an incomplete sample: export EDITOR=/usr/bin/vim export GZIP='-9' export HISTSIZE=5000 export HISTTIMEFORMAT='%F %T%z ' export PS1=\"\\u@\\h:\\w$ \" export TERM=xterm-256color export TMOUT=\"1800\" # log out after this many seconds of shell inactivity alias ll='ls -la' alias temp='tempdate=$(date +%F) ; mkdir -p ~/temp/$tempdate 2>/dev/null ; cd ~/temp/$tempdate' sprunge() { curl -F 'sprunge=<-' http://sprunge.us < \"${1:-/dev/stdin}\"; } # usage: sprunge FILE # or some_command | sprunge # Don't record some commands export HISTIGNORE=\"&:[ ]*:exit:ls:bg:fg:history:clear\" # Avoid duplicate entries HISTCONTROL=\"erasedups:ignoreboth\" # Perform file completion in a case insensitive fashion bind \"set completion-ignore-case on\" .bashrc ~/.bashrc is executed every time you open a sub-shell. It should not output any text, otherwise certain things (eg: scp) will fail. ~/.inputrc This file defines some bash behaviors. It also affects some other tools. # Ignore case while completing set completion-ignore-case on Links Command Line Quicksheet: http://www.pixelbeat.org/cmdline.html Tons of BASH examples: http://mywiki.wooledge.org/BashFAQ Bash Manual: Bash Variables Bash pitfalls: http://mywiki.wooledge.org/BashPitfalls Bash prompt howto, including colors: http://www.tldp.org/HOWTO/Bash-Prompt-HOWTO/x329.html Bash Automated Test System http://www.kfirlavi.com/blog/2012/11/14/defensive-bash-programming/ https://google.github.io/styleguide/shell.xml https://www.gnu.org/software/bash/manual/html_node/Bash-Variables.html https://www.shellcheck.net/ - Use this cli tool to help you write awesome shell scripts.","title":"GNU bash"},{"location":"bash/#tricks-and-usage","text":"","title":"Tricks and Usage"},{"location":"bash/#navigating-on-the-command-line","text":"The following can be seen by running: stty -a ctrl-a - move cursor to the beginning of the line ctrl-e - move cursor to the end of the line ctrl-l - do a \"clear\" on the terminal window ctrl-r - reverse history command search ctrl-t - get status of foreground process ctrl-w - delete previous word","title":"Navigating on the command line"},{"location":"bash/#view-a-list-of-all-commands-etc","text":"compgen -b will list all the built-ins you could run. compgen -a will list all the aliases you could run. compgen -c will list all the commands you could run. compgen -k will list all the keywords you could run. compgen -A function will list all the functions you could run. compgen -back will list all the above in one go.","title":"View a list of all commands, etc.."},{"location":"bash/#remove-leading-zeroes","text":"for X in 00{1..20..2} ; do echo \"$X == $((10#${X}))\" ; done ; Or... for X in {1..50..5} ; do Y=00${X} ; echo \"${X} with zeroes is ${Y} and removed with bc is $(echo ${Y} | bc)\" ; done ;","title":"Remove leading zeroes"},{"location":"bash/#convert-base-36-to-decimal","text":"This converts the base 36 number z to a decimal value echo $((36#z))","title":"Convert base 36 to decimal"},{"location":"bash/#run-a-command-for-5-seconds-then-kill-it","text":"ping -f & sleep 5 ; kill %1 Alternatively, use the timeout command if it's available. In macOS this can be installed through homebrew install coreutils and accessed with gtimeout . timeout 300 cmd","title":"Run a command for 5 seconds, then kill it"},{"location":"bash/#test-if-a-variable-is-empty","text":"if [[ -z \"$var\" ]]","title":"Test if a variable is empty"},{"location":"bash/#date","text":"For date stuff, see date, because it's different depending on platform.","title":"Date"},{"location":"bash/#show-random-statistics","text":"for X in {0..9999} ; do echo $(($RANDOM % 5)) ; done | sort | uniq -c","title":"Show RANDOM statistics"},{"location":"bash/#named-pipes","text":"mkfifo baz ; ps aux > baz then in another terminal cat baz","title":"named pipes"},{"location":"bash/#alternate-redirection-outputs","text":"exec 3> /tmp/baz ; ps aux >&3 # sends the output of ps aux to /tmp/baz","title":"alternate redirection outputs"},{"location":"bash/#redirect-all-output-of-a-script-into-a-file","text":"This is not bash specific, but works in bash. #!/usr/bin/env bash exec >> /tmp/$0.log exec 2>&1 date \"+%F %T%z $0 This is stdout\" date \"+%F %T%z $0 This is stderr\" >&2","title":"Redirect all output of a script into a file"},{"location":"bash/#show-size-of-each-users-home-folder","text":"getent passwd | while IFS=: read -r user _ uid _ _ home _ ; do if [[ $uid -ge 500 ]] ; then printf \"$user \" ; sudo du -sh $home ; fi ; done","title":"Show size of each user's home folder"},{"location":"bash/#previous-commands-args","text":"mkdir temp ; cd !!:* Be aware of the location of the tokens. eg: mkdir -p {foo,bar}/{a,b,c} ; stat !!:* creates a problem because you can't stat -p so you must stat -p !!:2*","title":"Previous command's args"},{"location":"bash/#debug-a-script","text":"This will show everything bash is executing bash -x scriptname.sh Or debug with a function: function debug { if [ ${debug:-0} -gt 0 ] ; then echo $@ 2>&1 ; fi ; }","title":"Debug a script"},{"location":"bash/#find-where-all-the-inodes-are","text":"find ~/ -type d -print0 | xargs -I %% -0 bash -c \"echo -n %% ; ls -a '%%' | wc -l\" >> ~/inodes.txt","title":"Find where all the inodes are"},{"location":"bash/#build-and-print-an-array","text":"array=(\"one is the first element\"); array+=(\"two is the second element\" \"three is the third\"); echo \"${array[@]}\" This is useful for building command line strings. For example, gpsbabel requires each input file to be prepended with -f . The following script takes a list of files and uses a bash array to create a command line in the form of gpsbabel -i gpx -f input_file_1.gpx -f input_file_2.gpx -o gpx -F output.gpx #!/usr/bin/env bash # Check for at least one argument, print usage if fail if [ $# -lt 2 ] ; then echo \"This script merges gpx files and requires at least two gpx files passed as arguments. Output is output.gpx\"; echo \"Usage: $0 <gpx file> <gpx file> [...<gpx file>]\"; exit 1; fi # Create an array of arguments to pass to gpsbabel args=(); for item in \"$@\" ; do if [ -f \"$item\" ] || [ -h \"$item\" ] ; then args+=( \"-f\" \"$item\" ); else echo \"Skipping $item, it's not a file or symlink.\" fi done; # Verify we have at least two files to work with if [ \"${#args[@]}\" -lt 4 ] ; then echo \"We don't have enough actual files to work with. Exiting.\" exit 1 fi gpsbabel -i gpx \"${args[@]}\" -o gpx -F output.gpx","title":"Build and print an array"},{"location":"bash/#build-and-print-an-associative-array-dict-hash","text":"declare -A animals=( [\"cow\"]=\"moo\" [\"dog\"]=\"woof woof\" [\"cat\"]=\"meow\" ) ; for animal in \"${!animals[@]}\" ; do echo \"The $animal says '${animals[$animal]}'\" ; done ;","title":"Build and print an associative array (dict, hash)"},{"location":"bash/#show-permissions-in-rwx-and-octal-format","text":"Linux: stat -c '%A %a %n' filename OSX: stat -f '%A %N' filename","title":"Show permissions in rwx and octal format"},{"location":"bash/#find-the-length-of-a-variable","text":"echo ${#SHELL}","title":"Find the length of a variable"},{"location":"bash/#print-all-variables-that-start-with-the-substring-sh","text":"echo ${!SH*}","title":"Print all variables that start with the substring SH"},{"location":"bash/#tertiary-type-variables","text":"${V:-D} # means \"return the value of the environment variable V or the string D if V isn't set.","title":"Tertiary type variables"},{"location":"bash/#do-a-command-and-if-it-returns-false-so-some-more-stuff","text":"while ! command_that_will_fail ; do something_else ; done ;","title":"Do a command, and if it returns false, so some more stuff"},{"location":"bash/#print-two-digit-months","text":"echo {1..12} may not work. If not, use echo $(seq -w 1 12)","title":"Print two digit months"},{"location":"bash/#get-filename-extension-or-path","text":"Taken from http://mywiki.wooledge.org/BashFAQ/073","title":"Get filename, extension or path"},{"location":"bash/#rename-files-to-a-sequence-and-change-their-extension-at-the-same-time","text":"ls | while read -r line ; do stub=${line%.*} ; (( i += 1 )) ; mv \"${line}\" \"${i}-${stub}.txt3\" ; done ; FullPath=/path/to/name4afile-00809.ext # result: # /path/to/name4afile-00809.ext Filename=${FullPath##*/} # name4afile-00809.ext PathPref=${FullPath%\"$Filename\"} # /path/to/ FileStub=${Filename%.*} # name4afile-00809 FileExt=${Filename#\"$FileStub\"} # .ext","title":"Rename files to a sequence and change their extension at the same time"},{"location":"bash/#sort-a-line-by-spaces","text":"s=( whiskey tango foxtrot ); sorted=$(printf \"%s\\n\"` `${s[@]}|sort); echo $sorted","title":"Sort a line by spaces"},{"location":"bash/#calculate-the-difference-between-two-dates","text":"echo $(( $(gdate +%s -d 20120203) - $(gdate +%s -d 20120115) ))","title":"Calculate the difference between two dates"},{"location":"bash/#substring-replace-a-variable","text":"This is not regex, just a simple string replacement. # ${VAR/search/replace} does only the first # ${VAR//search/replace} does all replacements echo \"Paths in your path: ${PATH//:/ }\"","title":"substring replace a variable"},{"location":"bash/#subtract-two-from-a-mac-address","text":"# printf -v defines a variable instead of printing to stdout printf -v dec \"%d\" 0x$(echo 00:25:9c:52:1c:2a | sed 's/://g') ; let dec=${dec}-2 ; printf \"%012X\" ${dec} \\ | sed -E 's/(..)(..)(..)(..)(..)(..)/\\1:\\2:\\3:\\4:\\5:\\6/g'","title":"Subtract two from a MAC address"},{"location":"bash/#print-the-last-for-chars-of-a-variable","text":"echo ${foo:$((${#foo}-4))} echo ${foo: -4} The space is necessary to prevent it from doing a completely different thing. See the next example...","title":"Print the last for chars of a variable"},{"location":"bash/#dereference-a-variable","text":"$ for var in ${!BASH_V*} ; do echo \"${var}: ${!var}\" ; done ; BASH_VERSINFO: 5 BASH_VERSION: 5.0.7(1)-release","title":"Dereference a variable"},{"location":"bash/#print-something-else-if-a-variable-doesnt-exist","text":"echo ${foo:-foo isn't assigned} echo ${foo:-${bar}} This can even be recursively done... echo ${foo:-${bar:-foo and bar are not assigned}}","title":"Print something else if a variable doesn't exist"},{"location":"bash/#print-every-third-number-starting-with-1-and-ending-with-30","text":"echo {1..30..3}","title":"Print every third number starting with 1 and ending with 30"},{"location":"bash/#print-every-5th-letter-of-the-alphabet","text":"echo {a..z..5}","title":"Print every 5th letter of the alphabet"},{"location":"bash/#process-all-lines-but-print-out-status-about-what-line-we-are-on-every-nth-line","text":"Sometimes during a series of long-running jobs you want to see the status of where you are at, or at least some indicator that things have not paused. when ctrl-t is not available (and even when it is) this pattern can help you monitor that things are still moving a long. N=0 find \"/usr/bin\" -type f | while read -r X ; do N=$((N + 1)) [[ \"$((N % 50))\" -eq 0 ]] && date \"+%F %T file number $N $X\" >&2 shasum -a 512 \"${X}\" >> ~/usr_bin_shasums.txt done Example terminal output from the above command, while all shasum output goes into ~/usr_bin_shasums.txt : $ find \"/usr/bin\" -type f | > while read -r X ; do > N=$((N + 1)) > [[ \"$((N % 50))\" -eq 0 ]] && date \"+%F %T file number $N $X\" >&2 > shasum -a 512 \"${X}\" >> ~/usr_bin_shasums.txt > done 2018-02-24 15:30:29 file number 50 /usr/bin/toe 2018-02-24 15:30:30 file number 100 /usr/bin/db_hotbackup 2018-02-24 15:30:32 file number 150 /usr/bin/host 2018-02-24 15:30:33 file number 200 /usr/bin/groffer 2018-02-24 15:30:35 file number 250 /usr/bin/mail 2018-02-24 15:30:36 file number 300 /usr/bin/dbicadmin 2018-02-24 15:30:38 file number 350 /usr/bin/fwkpfv 2018-02-24 15:30:39 file number 400 /usr/bin/tab2space","title":"Process all lines, but print out status about what line we are on every Nth line"},{"location":"bash/#make-a-directory-structure-of-every-combination-of-adjectivenoun","text":"mkdir -p {red,green,blue}/{fish,bird,flower}","title":"Make a directory structure of every combination of /adjective/noun"},{"location":"bash/#generate-a-zero-padded-random-2-byte-hex-number","text":"printf \"%02X\\n\" $((RANDOM % 256))","title":"Generate a zero padded random 2 byte hex number"},{"location":"bash/#grep-many-log-files-and-sort-output-by-date","text":"sudo grep cron /var/log/* \\ | sed 's/:/ /' \\ | while read file month day hour line ; do date -d \"$month $day $hour\" \"+%F %T%z ${file} ${line}\" ; done \\ | sort","title":"grep many log files and sort output by date"},{"location":"bash/#get-command-line-switches","text":"while getopts p:l:t: opt; do case $opt in p) pages=$OPTARG ;; l) length=$OPTARG ;; t) time=$OPTARG ;; esac done shift $((OPTIND - 1)) echo \"pages is ${pages}\" echo \"length is ${length}\" echo \"time is ${time}\" echo \"\\$1 is $1\" echo \"\\$2 is $2\" Call this script as ./foo.sh -p \"this is p\" -l llll -t this\\ is\\ t foo bar","title":"Get command line switches"},{"location":"bash/#files","text":"These files can change the behavior of bash.","title":"Files"},{"location":"bash/#bash_profile","text":"~/.bash_profile is executed every time you log into the system or initiate a login shell. Inclusion of things that write to stdout is allowed here. If you want to write scripts that change your interactive shell environment, such as changing your CWD, define functions here instead of using stand-alone scripts.","title":".bash_profile"},{"location":"bash/#example-bash_profile","text":"The ~/.bash_profile file can be quite long and complicated. The following example is an incomplete sample: export EDITOR=/usr/bin/vim export GZIP='-9' export HISTSIZE=5000 export HISTTIMEFORMAT='%F %T%z ' export PS1=\"\\u@\\h:\\w$ \" export TERM=xterm-256color export TMOUT=\"1800\" # log out after this many seconds of shell inactivity alias ll='ls -la' alias temp='tempdate=$(date +%F) ; mkdir -p ~/temp/$tempdate 2>/dev/null ; cd ~/temp/$tempdate' sprunge() { curl -F 'sprunge=<-' http://sprunge.us < \"${1:-/dev/stdin}\"; } # usage: sprunge FILE # or some_command | sprunge # Don't record some commands export HISTIGNORE=\"&:[ ]*:exit:ls:bg:fg:history:clear\" # Avoid duplicate entries HISTCONTROL=\"erasedups:ignoreboth\" # Perform file completion in a case insensitive fashion bind \"set completion-ignore-case on\"","title":"Example .bash_profile"},{"location":"bash/#bashrc","text":"~/.bashrc is executed every time you open a sub-shell. It should not output any text, otherwise certain things (eg: scp) will fail.","title":".bashrc"},{"location":"bash/#inputrc","text":"This file defines some bash behaviors. It also affects some other tools. # Ignore case while completing set completion-ignore-case on","title":"~/.inputrc"},{"location":"bash/#links","text":"Command Line Quicksheet: http://www.pixelbeat.org/cmdline.html Tons of BASH examples: http://mywiki.wooledge.org/BashFAQ Bash Manual: Bash Variables Bash pitfalls: http://mywiki.wooledge.org/BashPitfalls Bash prompt howto, including colors: http://www.tldp.org/HOWTO/Bash-Prompt-HOWTO/x329.html Bash Automated Test System http://www.kfirlavi.com/blog/2012/11/14/defensive-bash-programming/ https://google.github.io/styleguide/shell.xml https://www.gnu.org/software/bash/manual/html_node/Bash-Variables.html https://www.shellcheck.net/ - Use this cli tool to help you write awesome shell scripts.","title":"Links"},{"location":"bbcp/","text":"\"Securely and quickly copy data from source to target.\" - https://www.slac.stanford.edu/~abh/bbcp/ This is a useful tool for copying files. Notably it gets around some bandwidth limitations of nc that I ran into when trying to copy one large file across an 80gbps network.","title":"bbcp"},{"location":"bc/","text":"bc is a tool that does math on the CLI. Examples Divide one number into another and show two decimal places The scale variable sets the number of significant digits. echo \"scale=2 ; 7 / 3\" | bc Convert decimal to hexadecimal echo \"obase=16 ; 10\" | bc Convert hexadecimal to binary echo \"ibase=16 ; obase=2 ; AF\" | bc Subtract two from the last octet of a MAC address echo 24:b6:fd:ff:ba:31 | while read -r X ; do echo ${X%??}$( echo \"obase=16 ; $(( 0x${X#*:??:??:??:??:} )) - 2\" | bc | sed 's/^\\(.\\)$/0\\1/' | tr A-Z a-z ) ; done ;","title":"GNU bc"},{"location":"bc/#examples","text":"","title":"Examples"},{"location":"bc/#divide-one-number-into-another-and-show-two-decimal-places","text":"The scale variable sets the number of significant digits. echo \"scale=2 ; 7 / 3\" | bc","title":"Divide one number into another and show two decimal places"},{"location":"bc/#convert-decimal-to-hexadecimal","text":"echo \"obase=16 ; 10\" | bc","title":"Convert decimal to hexadecimal"},{"location":"bc/#convert-hexadecimal-to-binary","text":"echo \"ibase=16 ; obase=2 ; AF\" | bc","title":"Convert hexadecimal to binary"},{"location":"bc/#subtract-two-from-the-last-octet-of-a-mac-address","text":"echo 24:b6:fd:ff:ba:31 | while read -r X ; do echo ${X%??}$( echo \"obase=16 ; $(( 0x${X#*:??:??:??:??:} )) - 2\" | bc | sed 's/^\\(.\\)$/0\\1/' | tr A-Z a-z ) ; done ;","title":"Subtract two from the last octet of a MAC address"},{"location":"bind/","text":"BIND, or named, is the most widely used Domain Name System (DNS) software on the Internet. https://www.isc.org/downloads/bind/doc/ https://en.wikipedia.org/wiki/BIND Flush records Flush a single record rndc flushname github.com Flush all records rndc flush","title":"BIND"},{"location":"bind/#flush-records","text":"","title":"Flush records"},{"location":"bind/#flush-a-single-record","text":"rndc flushname github.com","title":"Flush a single record"},{"location":"bind/#flush-all-records","text":"rndc flush","title":"Flush all records"},{"location":"blkid/","text":"\"The blkid program is the command-line interface to working with the libblkid(3) library. It can determine the type of content (e.g. filesystem or swap) that a block device holds, and also attributes (tokens, NAME=value pairs) from the content metadata (e.g. LABEL or UUID fields). blkid has two main forms of operation: either searching for a device with a specific NAME=value pair, or displaying NAME=value pairs for one or more specified devices.\" - man blkid Examples Simple usage Here is the output of blkid on an Ubuntu 16.04 Vagrant box: $ blkid /dev/sda1: LABEL=\"cloudimg-rootfs\" UUID=\"743b1402-d445-494c-af0b-749040bb33e4\" TYPE=\"ext4\" PARTUUID=\"95a4c157-01\" /dev/sdb: UUID=\"2017-12-12-14-38-00-00\" LABEL=\"cidata\" TYPE=\"iso9660\" See Also findmnt lsblk","title":"blkid"},{"location":"blkid/#examples","text":"","title":"Examples"},{"location":"blkid/#simple-usage","text":"Here is the output of blkid on an Ubuntu 16.04 Vagrant box: $ blkid /dev/sda1: LABEL=\"cloudimg-rootfs\" UUID=\"743b1402-d445-494c-af0b-749040bb33e4\" TYPE=\"ext4\" PARTUUID=\"95a4c157-01\" /dev/sdb: UUID=\"2017-12-12-14-38-00-00\" LABEL=\"cidata\" TYPE=\"iso9660\"","title":"Simple usage"},{"location":"blkid/#see-also","text":"findmnt lsblk","title":"See Also"},{"location":"bluetooth/","text":"Examples Linux software bluetoothctl: pair, connect, get info on bluetooth devices gatttool","title":"bluetooth"},{"location":"bluetooth/#examples","text":"","title":"Examples"},{"location":"bluetooth/#linux-software","text":"bluetoothctl: pair, connect, get info on bluetooth devices gatttool","title":"Linux software"},{"location":"bosh/","text":"\"BOSH is an open source tool for release engineering, deployment, lifecycle management, and monitoring of distributed systems.\" - https://bosh.io/","title":"BOSH"},{"location":"bpf/","text":"\"Linux Socket Filtering (LSF) is derived from the Berkeley Packet Filter. Though there are some distinct differences between the BSD and Linux Kernel filtering, but when we speak of BPF or LSF in Linux context, we mean the very same mechanism of filtering in the Linux kernel.\" https://www.kernel.org/doc/Documentation/networking/filter.txt https://lwn.net/Articles/599755/ https://www.facebook.com/atscaleevents/videos/1693888610884236/ http://iovisor.github.io/bcc/ http://www.brendangregg.com/blog/2015-05-15/ebpf-one-small-step.html https://github.com/sharklinux/shark","title":"bpf"},{"location":"calico/","text":"\"Calico provides secure network connectivity for containers and virtual machine workloads.\" - https://docs.projectcalico.org/v3.1/introduction/ Kubernetes Examples Calico works in several environments, but these examples all apply to Kubernetes. Installation https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/ Show a bunch of info about your calico config See also https://docs.projectcalico.org/v2.0/reference/calicoctl/resources/ for X in bgpPeer hostEndpoint ipPool node policy profile workloadEndpoint ; do echo \"=========== $X\" calicoctl get $X 2>/dev/null done Links https://docs.projectcalico.org/latest/ https://docs.projectcalico.org/v2.0/reference/calicoctl/resources/ https://kubernetes.io/docs/concepts/services-networking/network-policies/","title":"calico"},{"location":"calico/#kubernetes-examples","text":"Calico works in several environments, but these examples all apply to Kubernetes.","title":"Kubernetes Examples"},{"location":"calico/#installation","text":"https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/","title":"Installation"},{"location":"calico/#show-a-bunch-of-info-about-your-calico-config","text":"See also https://docs.projectcalico.org/v2.0/reference/calicoctl/resources/ for X in bgpPeer hostEndpoint ipPool node policy profile workloadEndpoint ; do echo \"=========== $X\" calicoctl get $X 2>/dev/null done","title":"Show a bunch of info about your calico config"},{"location":"calico/#links","text":"https://docs.projectcalico.org/latest/ https://docs.projectcalico.org/v2.0/reference/calicoctl/resources/ https://kubernetes.io/docs/concepts/services-networking/network-policies/","title":"Links"},{"location":"calver/","text":"\"CalVer is a software versioning convention that is based on your project's release calendar, instead of arbitrary numbers.\" - https://calver.org/","title":"CalVer"},{"location":"centos/","text":"\"The CentOS Project is a community-driven free software effort focused on delivering a robust open source ecosystem.\" - https://www.centos.org/ CentOS 7 Released 2014-07-07 New things in CentOS 7 firewalld manages the firewall hostnamectl changes the hostname and applies the setting immediately journalctl shows log files of services launched by systemd systemctl manages systemd services Initial setup Set up some base parameters on a fresh instance yum install -y bash-completion bc curl git lsof mlocate mutt net-snmp ntpd smartmontools strace sysstat vim wget ln -sf /usr/share/zoneinfo/America/Los_Angeles /etc/localtime ntpdate {0..3}.pool.ntp.org systemctl start ntpd CentOS 6 Released 2011-07-10 CentOS 6 Initial Setup yum install -y ntp chkconfig --levels 345 ntpd on && ntpdate time.apple.com && service ntpd start yum upgrade -y yum install -y arping avahi avahi-tools bc bind-utils curl elinks fping lsof net-snmp man mlocate mutt openssh openssh-clients openssh-server perl-Crypt-SSLeay perl-libwww-perl rsync strace vim wget yum-cron ln -sf /usr/share/zoneinfo/America/Los_Angeles /etc/localtime chkconfig --levels 345 yum-cron on && service yum-cron start yum install -y dcfldd nfs-utils smartmontools dmidecode lshw dstat htop iotop chkconfig --levels 345 smartd on && service smartd start Tweaks and Tricks Get past protected lib problems yum update --setopt=protected_multilib=false --skip-broken Enable DHCP Hostname for DNS resolution add \" DHCP_HOSTNAME=whatever \" to /etc/sysconfig/network-scripts/ifcfg-eth0 Install OS from USB From Windows: http://iso2usb.sourceforge.net/ From Linux: https://fedoraproject.org/wiki/Livecd-iso-to-disk Show installed repository keys rpm -q gpg-pubkey --qf '%{name}-%{version}-%{release} --> %{summary}\\n' DHCP with DDNS hostname Model your /etc/sysconfig/network-scripts/ifcfg-eth0 like this: TYPE=Ethernet DEVICE=eth0 ONBOOT=yes BOOTPROTO=dhcp # Without the following line, dhclient will not update /etc/resolv.conf and may not get an IP address at all DHCP_HOSTNAME=some_hostname To configure your hostname, edit /etc/sysconfig/network and add HOSTNAME= You also may need to delete these files: rm -f /etc/dhclient-eth?.conf /var/lib/dhclient/dhclient-eth?.leases /etc/udev/rules.d/70-persistent-net.rules /etc/sysconfig/network-scripts/ifcfg-eth1 Configuration to auth against Microsoft AD http://www.uncompiled.com/using-winbind-in-centos-6-for-active-director worked perfectly with samba and winbind Bond Configs $ cat /etc/modprobe.d/bond0.conf alias bond0 bonding options bond0 max_bonds=1 fail_over_mac=2 mode=1 num_grat_arp=2 primary=em1 primary_reselect=1 arp_validate=1 arp_interval=100 arp_ip_target=10.1.5.15,10.1.1.1 See Also chkconfig rpm selinux - http://wiki.centos.org/HowTos/SELinux yum","title":"CentOS Linux"},{"location":"centos/#centos-7","text":"Released 2014-07-07","title":"CentOS 7"},{"location":"centos/#new-things-in-centos-7","text":"firewalld manages the firewall hostnamectl changes the hostname and applies the setting immediately journalctl shows log files of services launched by systemd systemctl manages systemd services","title":"New things in CentOS 7"},{"location":"centos/#initial-setup","text":"Set up some base parameters on a fresh instance yum install -y bash-completion bc curl git lsof mlocate mutt net-snmp ntpd smartmontools strace sysstat vim wget ln -sf /usr/share/zoneinfo/America/Los_Angeles /etc/localtime ntpdate {0..3}.pool.ntp.org systemctl start ntpd","title":"Initial setup"},{"location":"centos/#centos-6","text":"Released 2011-07-10","title":"CentOS 6"},{"location":"centos/#centos-6-initial-setup","text":"yum install -y ntp chkconfig --levels 345 ntpd on && ntpdate time.apple.com && service ntpd start yum upgrade -y yum install -y arping avahi avahi-tools bc bind-utils curl elinks fping lsof net-snmp man mlocate mutt openssh openssh-clients openssh-server perl-Crypt-SSLeay perl-libwww-perl rsync strace vim wget yum-cron ln -sf /usr/share/zoneinfo/America/Los_Angeles /etc/localtime chkconfig --levels 345 yum-cron on && service yum-cron start yum install -y dcfldd nfs-utils smartmontools dmidecode lshw dstat htop iotop chkconfig --levels 345 smartd on && service smartd start","title":"CentOS 6 Initial Setup"},{"location":"centos/#tweaks-and-tricks","text":"","title":"Tweaks and Tricks"},{"location":"centos/#get-past-protected-lib-problems","text":"yum update --setopt=protected_multilib=false --skip-broken","title":"Get past protected lib problems"},{"location":"centos/#enable-dhcp-hostname-for-dns-resolution","text":"add \" DHCP_HOSTNAME=whatever \" to /etc/sysconfig/network-scripts/ifcfg-eth0","title":"Enable DHCP Hostname for DNS resolution"},{"location":"centos/#install-os-from-usb","text":"From Windows: http://iso2usb.sourceforge.net/ From Linux: https://fedoraproject.org/wiki/Livecd-iso-to-disk","title":"Install OS from USB"},{"location":"centos/#show-installed-repository-keys","text":"rpm -q gpg-pubkey --qf '%{name}-%{version}-%{release} --> %{summary}\\n'","title":"Show installed repository keys"},{"location":"centos/#dhcp-with-ddns-hostname","text":"Model your /etc/sysconfig/network-scripts/ifcfg-eth0 like this: TYPE=Ethernet DEVICE=eth0 ONBOOT=yes BOOTPROTO=dhcp # Without the following line, dhclient will not update /etc/resolv.conf and may not get an IP address at all DHCP_HOSTNAME=some_hostname To configure your hostname, edit /etc/sysconfig/network and add HOSTNAME= You also may need to delete these files: rm -f /etc/dhclient-eth?.conf /var/lib/dhclient/dhclient-eth?.leases /etc/udev/rules.d/70-persistent-net.rules /etc/sysconfig/network-scripts/ifcfg-eth1","title":"DHCP with DDNS hostname"},{"location":"centos/#configuration-to-auth-against-microsoft-ad","text":"http://www.uncompiled.com/using-winbind-in-centos-6-for-active-director worked perfectly with samba and winbind","title":"Configuration to auth against Microsoft AD"},{"location":"centos/#bond-configs","text":"$ cat /etc/modprobe.d/bond0.conf alias bond0 bonding options bond0 max_bonds=1 fail_over_mac=2 mode=1 num_grat_arp=2 primary=em1 primary_reselect=1 arp_validate=1 arp_interval=100 arp_ip_target=10.1.5.15,10.1.1.1","title":"Bond Configs"},{"location":"centos/#see-also","text":"chkconfig rpm selinux - http://wiki.centos.org/HowTos/SELinux yum","title":"See Also"},{"location":"ceph/","text":"\"Ceph is a unified, distributed storage system designed for excellent performance, reliability and scalability.\" - https://ceph.com Glossary http://docs.ceph.com/docs/master/glossary/ Ceph OSD: The Ceph OSD software, which interacts with a logical disk (OSD). CephFS: The POSIX filesystem components of Ceph. MDS: (Ceph Metadata Server) The Ceph metadata software. MGR: (Ceph Manager) The Ceph manager software, which collects all the state from the whole cluster in one place. MON: (Ceph Monitor) The Ceph monitor software. OSD: (Object Storage Device) A physical or logical storage unit. RADOS: Reliable Autonomic Distributed Object Store. RBD: The block storage component of Ceph. RGW: The S3/Swift gateway component of Ceph. PG: Placement Group. http://docs.ceph.com/docs/master/rados/operations/placement-groups/ Examples Activate all OSDs sudo ceph-disk activate-all Starting with ceph 13, use: ceph-volume lvm activate --all Start all ceph services sudo systemctl start ceph.target Stop all ceph services Unfortunately there's not a single service or target to stop, so you have to use globs sudo systemctl stop '*ceph*' Show the status of all osds in the cluster ceph osd status Show metadata about all osds in the cluster ceph osd metadata Show all pools ceph osd lspools Monitor existing operations ceph daemon mon.$MON_HOSTNAME ops Links Ceph Intro & Architectural Overview - https://youtu.be/7I9uxoEhUdY http://docs.ceph.com/ceph-ansible/master/ http://docs.ceph.com/docs/master/rados/configuration/network-config-ref/ http://docs.ceph.com/docs/master/rados/operations/operating/ http://docs.ceph.com/docs/master/start/quick-ceph-deploy/","title":"Ceph"},{"location":"ceph/#glossary","text":"http://docs.ceph.com/docs/master/glossary/ Ceph OSD: The Ceph OSD software, which interacts with a logical disk (OSD). CephFS: The POSIX filesystem components of Ceph. MDS: (Ceph Metadata Server) The Ceph metadata software. MGR: (Ceph Manager) The Ceph manager software, which collects all the state from the whole cluster in one place. MON: (Ceph Monitor) The Ceph monitor software. OSD: (Object Storage Device) A physical or logical storage unit. RADOS: Reliable Autonomic Distributed Object Store. RBD: The block storage component of Ceph. RGW: The S3/Swift gateway component of Ceph. PG: Placement Group. http://docs.ceph.com/docs/master/rados/operations/placement-groups/","title":"Glossary"},{"location":"ceph/#examples","text":"","title":"Examples"},{"location":"ceph/#activate-all-osds","text":"sudo ceph-disk activate-all Starting with ceph 13, use: ceph-volume lvm activate --all","title":"Activate all OSDs"},{"location":"ceph/#start-all-ceph-services","text":"sudo systemctl start ceph.target","title":"Start all ceph services"},{"location":"ceph/#stop-all-ceph-services","text":"Unfortunately there's not a single service or target to stop, so you have to use globs sudo systemctl stop '*ceph*'","title":"Stop all ceph services"},{"location":"ceph/#show-the-status-of-all-osds-in-the-cluster","text":"ceph osd status","title":"Show the status of all osds in the cluster"},{"location":"ceph/#show-metadata-about-all-osds-in-the-cluster","text":"ceph osd metadata","title":"Show metadata about all osds in the cluster"},{"location":"ceph/#show-all-pools","text":"ceph osd lspools","title":"Show all pools"},{"location":"ceph/#monitor-existing-operations","text":"ceph daemon mon.$MON_HOSTNAME ops","title":"Monitor existing operations"},{"location":"ceph/#links","text":"Ceph Intro & Architectural Overview - https://youtu.be/7I9uxoEhUdY http://docs.ceph.com/ceph-ansible/master/ http://docs.ceph.com/docs/master/rados/configuration/network-config-ref/ http://docs.ceph.com/docs/master/rados/operations/operating/ http://docs.ceph.com/docs/master/start/quick-ceph-deploy/","title":"Links"},{"location":"chkconfig/","text":"chkconfig is a tool to interact with sys-v init scripts on centos/rhel hosts, and probably others. Examples List services and their runlevels chkconfig --list Turn on mysql at runlevels 3 and 5 chkconfig --level 35 mysql on See Also update-rc.d - similar feature for ubuntu sysvinit","title":"chkconfig"},{"location":"chkconfig/#examples","text":"","title":"Examples"},{"location":"chkconfig/#list-services-and-their-runlevels","text":"chkconfig --list","title":"List services and their runlevels"},{"location":"chkconfig/#turn-on-mysql-at-runlevels-3-and-5","text":"chkconfig --level 35 mysql on","title":"Turn on mysql at runlevels 3 and 5"},{"location":"chkconfig/#see-also","text":"update-rc.d - similar feature for ubuntu sysvinit","title":"See Also"},{"location":"chocolatey/","text":"\"The package manager for Windows\" - https://chocolatey.org Examples choco has to be run from an admin shell. Search for a package choco search xencenter Install software and all its requirements choco install xencenter -y","title":"Choclatey"},{"location":"chocolatey/#examples","text":"choco has to be run from an admin shell.","title":"Examples"},{"location":"chocolatey/#search-for-a-package","text":"choco search xencenter","title":"Search for a package"},{"location":"chocolatey/#install-software-and-all-its-requirements","text":"choco install xencenter -y","title":"Install software and all its requirements"},{"location":"chronos/","text":"\"Chronos is a replacement for cron. It is a distributed and fault-tolerant scheduler that runs on top of Apache Mesos that can be used for job orchestration.\" - https://mesos.github.io/chronos/ Uses ISO 8601 Repeating Interval notation , but the P is required so you can only use the syntax Rn/<datespec>/PT4H - see http://mesos.github.io/chronos/docs/api.html#adding-a-scheduled-job","title":"chronos"},{"location":"circleci/","text":"\"CircleCI's continuous integration and delivery platform makes it easy for teams of all sizes to rapidly build and release quality software at scale. Build for Linux, macOS, and Android, in the cloud or behind your firewall.\" - https://circleci.com/ Links https://circleci.com/docs/2.0/configuration-reference/ https://circleci.com/docs/2.0/sample-config/ https://circleci.com/docs/2.0/circleci-images/ https://circleci.com/docs/2.0/workflows/ https://discuss.circleci.com/","title":"CircleCI"},{"location":"circleci/#links","text":"https://circleci.com/docs/2.0/configuration-reference/ https://circleci.com/docs/2.0/sample-config/ https://circleci.com/docs/2.0/circleci-images/ https://circleci.com/docs/2.0/workflows/ https://discuss.circleci.com/","title":"Links"},{"location":"cncf/","text":"\"CNCF serves as the vendor-neutral home for many of the fastest-growing projects on GitHub, including Kubernetes, Prometheus and Envoy, fostering collaboration between the industry\u2019s top developers, end users, and vendors.\" - https://www.cncf.io","title":"CNCF"},{"location":"cobbler/","text":"\"Cobbler is a Linux installation server that allows for rapid setup of network installation environments.\" -- http://cobbler.github.io Notes Versions prior to 2.6.9 can no longer auto-download loaders Links https://cobbler.github.io/ https://cobbler.github.io/manuals/quickstart/ https://download.opensuse.org/repositories/home:/libertas-ict:/cobbler26/CentOS_CentOS-6/noarch/ - Newer than epel CentOS 6 rpms https://github.com/rhinstaller/pykickstart","title":"Cobbler"},{"location":"cobbler/#notes","text":"Versions prior to 2.6.9 can no longer auto-download loaders","title":"Notes"},{"location":"cobbler/#links","text":"https://cobbler.github.io/ https://cobbler.github.io/manuals/quickstart/ https://download.opensuse.org/repositories/home:/libertas-ict:/cobbler26/CentOS_CentOS-6/noarch/ - Newer than epel CentOS 6 rpms https://github.com/rhinstaller/pykickstart","title":"Links"},{"location":"conjure-up/","text":"\"Get started with big software, fast\" - https://conjure-up.io/ Links Deploying The Canonical Distribution Of Kubernetes onto AWS","title":"conjure-up"},{"location":"conjure-up/#links","text":"Deploying The Canonical Distribution Of Kubernetes onto AWS","title":"Links"},{"location":"consul/","text":"\"Service Discovery and Configuration Made Easy\" - https://www.consul.io/ Links https://www.consul.io/docs/internals/architecture.html","title":"consul"},{"location":"consul/#links","text":"https://www.consul.io/docs/internals/architecture.html","title":"Links"},{"location":"cookiecutter/","text":"Examples Find a list of cookiecutter variables grep -h -o '{{cookiecutter[^}]*}}' \\{\\{cookiecutter.repo_name\\}\\}/* | sort | uniq -c Links Cookiecutter: Better Project Templates A pantry full of cookiecutters","title":"cookiecutter"},{"location":"cookiecutter/#examples","text":"","title":"Examples"},{"location":"cookiecutter/#find-a-list-of-cookiecutter-variables","text":"grep -h -o '{{cookiecutter[^}]*}}' \\{\\{cookiecutter.repo_name\\}\\}/* | sort | uniq -c","title":"Find a list of cookiecutter variables"},{"location":"cookiecutter/#links","text":"Cookiecutter: Better Project Templates A pantry full of cookiecutters","title":"Links"},{"location":"cowsay/","text":"In Linux etc., print a cow that is saying something. Also works as cowthink , and a variety of other animals and artwork are available. Examples Get a list of things that can talk $ cowthink -l Cow files in /usr/share/cowsay/cows: apt beavis.zen bong bud-frogs bunny calvin cheese cock cower daemon default dragon dragon-and-cow duck elephant elephant-in-snake eyes flaming-sheep ghostbusters gnu head-in hellokitty kiss kitty koala kosh luke-koala mech-and-cow meow milk moofasa moose mutilated pony pony-smaller ren sheep skeleton snowman sodomized-sheep stegosaurus stimpy suse three-eyes turkey turtle tux unipony unipony-smaller vader vader-koala www cowsay $ cowsay \"Hello world!\" ______________ < Hello world! > -------------- \\ ^__^ \\ (oo)\\_______ (__)\\ )\\/\\ ||----w | || || cowthink $ cowthink -f dragon \"On the internet, nobody knows you're a dragon!\" ________________________________________ ( On the internet, nobody knows you're a ) ( dragon! ) ---------------------------------------- o / \\ //\\ o |\\___/| / \\// \\\\ /0 0 \\__ / // | \\ \\ / / \\/_/ // | \\ \\ @_^_@'/ \\/_ // | \\ \\ //_^_/ \\/_ // | \\ \\ ( //) | \\/// | \\ \\ ( / /) _|_ / ) // | \\ _\\ ( // /) '/,_ _ _/ ( ; -. | _ _\\.-~ .-~~~^-. (( / / )) ,-{ _ `-.|.-~-. .~ `. (( // / )) '/\\ / ~-. _ .-~ .-~^-. \\ (( /// )) `. { } / \\ \\ (( / )) .----~-.\\ \\-' .~ \\ `. \\^-. ///.----..> \\ _ -~ `. ^-` ^-_ ///-._ _ _ _ _ _ _}^ - - - - ~ ~-- ,.-~ /.-~","title":"cowsay"},{"location":"cowsay/#examples","text":"","title":"Examples"},{"location":"cowsay/#get-a-list-of-things-that-can-talk","text":"$ cowthink -l Cow files in /usr/share/cowsay/cows: apt beavis.zen bong bud-frogs bunny calvin cheese cock cower daemon default dragon dragon-and-cow duck elephant elephant-in-snake eyes flaming-sheep ghostbusters gnu head-in hellokitty kiss kitty koala kosh luke-koala mech-and-cow meow milk moofasa moose mutilated pony pony-smaller ren sheep skeleton snowman sodomized-sheep stegosaurus stimpy suse three-eyes turkey turtle tux unipony unipony-smaller vader vader-koala www","title":"Get a list of things that can talk"},{"location":"cowsay/#cowsay","text":"$ cowsay \"Hello world!\" ______________ < Hello world! > -------------- \\ ^__^ \\ (oo)\\_______ (__)\\ )\\/\\ ||----w | || ||","title":"cowsay"},{"location":"cowsay/#cowthink","text":"$ cowthink -f dragon \"On the internet, nobody knows you're a dragon!\" ________________________________________ ( On the internet, nobody knows you're a ) ( dragon! ) ---------------------------------------- o / \\ //\\ o |\\___/| / \\// \\\\ /0 0 \\__ / // | \\ \\ / / \\/_/ // | \\ \\ @_^_@'/ \\/_ // | \\ \\ //_^_/ \\/_ // | \\ \\ ( //) | \\/// | \\ \\ ( / /) _|_ / ) // | \\ _\\ ( // /) '/,_ _ _/ ( ; -. | _ _\\.-~ .-~~~^-. (( / / )) ,-{ _ `-.|.-~-. .~ `. (( // / )) '/\\ / ~-. _ .-~ .-~^-. \\ (( /// )) `. { } / \\ \\ (( / )) .----~-.\\ \\-' .~ \\ `. \\^-. ///.----..> \\ _ -~ `. ^-` ^-_ ///-._ _ _ _ _ _ _}^ - - - - ~ ~-- ,.-~ /.-~","title":"cowthink"},{"location":"cradlepoint/","text":"\"Founded in 2006, Cradlepoint has grown to become the industry leader in cloud-delivered 4G LTE network solutions for business, service providers, and government organizations, and we are committed to extending our leadership into the emerging 5G space.\" - https://cradlepoint.com/company/about Cradlepoint IBR900 https://cradlepoint.com/products/cor-ibr900-series https://cradlepoint.com/sites/default/files/upload-file/cradlepoint_ibr900_manual_0.pdf The CLI is not a normal shell, but is a minimal appliance type UI. Get config data This will get all config data, which can be over 25k lines of JSON. get To get just a subset of the data, use something like: [admin@IBR900-13e: /]$ get config/vlan [ { \"mode\": \"wan\", \"ports\": [], \"uid\": \"wan\", \"vid\": 1 }, { \"mode\": \"lan\", \"ports\": [ { \"mode\": \"untagged\", \"port\": 0 }, { \"mode\": \"untagged\", \"port\": 1 }, { \"mode\": \"untagged\", \"port\": 2 } ], \"uid\": \"lan\", \"vid\": 2 } ] Set and fetch variables [admin@IBR900-13e: /]$ set foo/bar: \"baz\" [admin@IBR900-13e: /]$ get foo { \"bar:\": \"baz\" } Getting help [admin@IBR900-13e: /]$ help Available Commands: SupportQA adduser append arpdump atterm banner bgp cd clear clients cpconnect date delete deluser devices diff edit exit factory_reset find free get gre grep help inspect ips ipset lan log ls mkdir nemo netcloud netfilter netstat nhrp ospf passwd ping ping6 pwd qos reboot reset resources rip ripng route rtpolicy serial set sleep sms ssh stp switch tcpdump telnet threads traceroute uptime vlan vpn vrrp wan wireless workqueue xfrm zebra Available Aliases: cat => get dir => ls ll => ls -l 1 more => get post => append put => set quit => exit rm => delete To get help for a specific command run: \"help CMD\" [admin@IBR900-13e: /]$ help SupportQA Command to provide debugging data [admin@IBR900-13e: /]$ help adduser Add a new user account and set the password for this account. Usage: adduser USER [admin@IBR900-13e: /]$ help append Append new item to an array in the config append PATH VALUE [admin@IBR900-13e: /]$ help arpdump The arpdump command shows the current ARP table. [admin@IBR900-13e: /]$ help atterm The atterm command can be used to get direct access to a modem's AT command channel. If a modem interface is not given then the system will pick the highest priority modem. To discover a modem's interface to use, use the command 'devices' and use the value found under the 'SubIface' column. USAGE: atterm [interface] [-c ATCMD] [-t SOCKET_TIMEOUT] [admin@IBR900-13e: /]$ help banner banner [set|unset] [admin@IBR900-13e: /]$ help bgp Usage: bgp [cmd] Excecute and show output of \"cmd\". If no \"cmd\" is given then access to the Quagga BGP CLI is given. Use \"bgp list\" to see avaliable commands or See the Quagga documentation for details. *** WARNING: Changes made using the Quagga CLI do not persist! *** The configuration entered via the Cradlepoint user interface will be restored after user interface changes and/or a router reboot. If the routing protocol options in the user interface do not allow a necessary Quagga configuration, please provide all configuration details to Cradlepoint support. [admin@IBR900-13e: /]$ help cd Change to a new config directory Usage: cd DIR/[DIR...] [admin@IBR900-13e: /]$ help clear Clear the screen [admin@IBR900-13e: /]$ help clients The clients command can be used to show the currently connected clients, both wireless and wired as well as hotspot. USAGE: clients [show | revoke | kick ] [SUBOPTIONS...] show [CONNECTION TYPE]: Show clients by connection type. If no connection type option is given, all connection types will be shown. Connection Types: all : All Known Clients, both wired and wireless, that the router knows about. wlan : Wireless LAN Clients hotspot : Authenticated Hotspot Clients revoke [-t [ip | mac | hn]]: Revoke a client's Hotspot authentication. -t : The type of adddress to be used to find the client in the client list ip : IP address of the client (default) mac : MAC address of the client hn : Hostname of the client (cannot be used if client does not have a hostname) kick [-t [ip | mac | hn]]: Remove wireless access until the router is rebooted. -t : The type of adddress to be used to find the client in the client list ip : IP address of the client (default) mac : MAC address of the client hn : Hostname of the client (cannot be used if client does not have a hostname) [admin@IBR900-13e: /]$ help cpconnect Usage: cpconnect {COMMAND} [OPTIONS] where COMMAND := { add | remove | show } OPTIONS := {-n[ame] | -u[ser] | -p[asswd] | -s[server] } [VAR] example: cpconnect add -n test1 -u user1 -p pass1 -s vpn.accessmylan.com cpconnect show [admin@IBR900-13e: /]$ help date Show system clock. [admin@IBR900-13e: /]$ help delete Delete an item from the config Usage: delete PATH [admin@IBR900-13e: /]$ help deluser Delete a user account. Usage: deluser USER [admin@IBR900-13e: /]$ help devices Display network devices connected to the router. USAGE: devices [-v] Show all devices(s) devices [-v] [DEVICE_UID...] Show one or more specific devices. -v Verbose output [admin@IBR900-13e: /]$ help diff Show differences between the current and the default configuration. [admin@IBR900-13e: /]$ help edit Usage: edit CONFIG_PATH Examples: edit /config/system/gps edit . [admin@IBR900-13e: /]$ help exit Exit the shell [admin@IBR900-13e: /]$ help factory_reset Reset config to factory defaults [admin@IBR900-13e: /]$ help find Find or list files and combine with grep to locate specific files Usage: find PATH [| grep file] [admin@IBR900-13e: /]$ help free Show amount (kilobytes) of free and used system memory. \"free\" memory is presently unused, while \"available\" includes used memory, typically for cache, that is readily reclaimed for something else when needed. [admin@IBR900-13e: /]$ help get Get value for config item(s) Usage: get PATH [PATH...] [admin@IBR900-13e: /]$ help gre Start, stop, or show status of all gre tunnels. Start, stop or restart a tunnel. Renew or release the DHCP lease for a tunnel (if DHCP for GRE is enabled). gre [show|start|stop] gre [starttunnel|stoptunnel|restarttunnel|renew|release] -n <TUNNEL NAME> [admin@IBR900-13e: /]$ help grep Usage: grep PATTERN [FILE]... [admin@IBR900-13e: /]$ help help To get help for a specific command run: \"help CMD\" [admin@IBR900-13e: /]$ help inspect Inspect a directory Usage: inspect [PATH] [admin@IBR900-13e: /]$ help ips Interact with the IPS engine. Usage: ips [load|show|mode|status] [SUBOPTIONS..] load [-f PATH]: Load a rule file into the IPS engine -f [PATH]: Load rule file from PATH (Must contain the filename) show [-o [ids | cats | sec | app | anom]]: Show information from the signatures loaded. ids: Print all the signatures (default) cats: Print all the categories sec: Print only the signatures for the Security rules app: Print only the signatures for the Application rules anom: Print only the signatures for the Anomaly rules mode [-o [off | ips | ids]]: Change the IPS Global mode off: Set Default IPS mode to 'Disabled' (If no other Category or Signature is 'Enabled' then the kernel modules will be unloaded) ips: Set Default IPS mode to 'Detect and Prevent' ids: Set Default IPS mode to 'Detect Only' status: Print the status of the IPS engine update: Request a Rule File Update [admin@IBR900-13e: /]$ help ipset ipset [list {name}] specify the name of the set to list, or nothing to see the names of the sets [admin@IBR900-13e: /]$ help lan Show the current LAN configuration and status. [admin@IBR900-13e: /]$ help log Show and manipulate the log system. Usage: log [show|clear|service|level|msg] [SUBOPTONS...] show [FILTER] [FILTERN] [[-bhi] -s SEARCH]: FILTER can be one or more space-separated names or levels. eg. log show wanmgr kernel DEBUG INFO -b bold new entries -h HIGHLIGHT Same usage as searching but does not filter results. -i makes the search case insensitive. -s SEARCH can be any string to search for in the log message contents. eg. log show -s Firmware -f [LINES_OF_HISTORY] Follow mode with optional argument for number of lines of history to show. -r recover crash log if one exists. service [level (DEBUG|INFO|WARNING|ERROR|CRITICAL)]: Display all service log levels. level change service log level. clear: Erase all logs from memory level [NEW_LEVEL]: View current level or set new log level to: critical, error, warning, info, or debug msg [-l LEVEL] MESSAGE: Write a message to the logging system. LEVEL defaults to: info [admin@IBR900-13e: /]$ help ls List files in the current config directory [admin@IBR900-13e: /]$ help mkdir Create an empty container Usage: mkdir DIRNAME [admin@IBR900-13e: /]$ help nemo Show status and configuration of NEMO session Usage: nemo [-v] [admin@IBR900-13e: /]$ help netcloud Manage connection to the Cradlepoint NetCloud. Usage: netcloud [alert|status|register|stop|start|restart|triggers] status: [DEFAULT] Show current status information. register [--username=USER --password=PW]|[--token_id=TID --token_secret=TS]: (Re)register --username: The NetCloud username that should be used to authenticate. --password: The NetCloud password that should be used to authenticate. --token_id: [EXPERT] Token ID for token authentication mode. --token_secret: [EXPERT] Token secret for token authentication mode. unregister: Unregister this router from NetCloud and unregister. stop: Manually stop the NetCloud client. start: Manually start the NetCloud client. restart: Manually restart the NetCloud client. [admin@IBR900-13e: /]$ help netfilter Show info and debug from netfilter2 rule(s) and trigger(s). Usage: netfilter [active|all|upnp|triggers|states] [input|routed|output] [ip/ip6] [-v] [-s] [-r RULE_INDEX] [-t TRIGGER_INDEX] [-s STATE_INDEX] [admin@IBR900-13e: /]$ help netstat Usage: netstat [-al] [-tuwx] [-enWp] Display networking information -a All sockets -l Listening sockets Else: connected sockets -t TCP sockets -u UDP sockets -w Raw sockets -x Unix sockets Else: all socket types -e Other/more information -n Don't resolve names -W Wide display -p Show PID/program name for sockets [admin@IBR900-13e: /]$ help nhrp Usage: nhrp show nhrp flush nhrp flush nbma ip [admin@IBR900-13e: /]$ help ospf Usage: ospf [cmd] Excecute and show output of \"cmd\". If no \"cmd\" is given then access to the Quagga OSPF CLI is given. Use \"ospf list\" to see avaliable commands or See the Quagga documentation for details. *** WARNING: Changes made using the Quagga CLI do not persist! *** The configuration entered via the Cradlepoint user interface will be restored after user interface changes and/or a router reboot. If the routing protocol options in the user interface do not allow a necessary Quagga configuration, please provide all configuration details to Cradlepoint support. [admin@IBR900-13e: /]$ help passwd Set the password for a user. Usage: passwd [USER] [admin@IBR900-13e: /]$ help ping Send ICMP echo_request(s) to a networked host Usage: ping [-w DEADLINE] [-c COUNT] [-i INTERVAL] [-I INTERFACE [-P]] [-M PMTUDISC_OPT] [-s SIZE] HOSTNAME -w DEADLINE Stop after a total of DEADLINE seconds, or (if specified) COUNT packets are sent, whichever comes first. -c COUNT Stop after sending COUNT packets, or (if specified) DEADLINE seconds, whichever comes first -i INTERVAL Wait INTERVAL seconds between packet transmissions. Default is 1. -I INTERFACE Specify the source of the ICMP requests. If INTERFACE is an IP address, then source the ICMP requests from that address. If INTERFACE is not an IP address, treat it as an interface name and source from that interface. When treated as a name, exact matches of interface \"Ifaces\" are chosen first, followed by exact matches of \"Device UIDs\", and finally case-insensitive matches to Network names are returned. See the \"devices\" command for a list of valid \"Ifaces\" and \"Device UIDs\". -P Requires the -I INTERFACE option. When -P is present, ping will source from an IP address on the specified interface, instead of the interface itself. -s SIZE Specifiy the ICMP data length, in bytes. The default is 56 bytes, which will result in an ICMP packet length of 64 bytes (56 data bytes plus 8 ICMP header bytes) -M PMTU_OPT Select Path MTU discovery. PMTU_OPT must be one of: \"do\", \"want\" or \"dont\". If the PMTU_OPT is \"do\" (default), then ping will set the Don't Fragment (DF) flag in the ICMP requests, which will prohibit packet fragmentation. If PMTU_OPT is \"want\", then ping will fragment if the ICMP request exceeds the local outbound interfaces' MTU. Finally if PMTU_OPT is \"dont\" (do not set the DF flag), then fragmentation is allowed and ICMP Requests will be fragmented as necessary in response to ICMP Fragmentation Responses. [admin@IBR900-13e: /]$ help ping6 Send ICMPv6 echo_request(s) to a networked host Usage: ping6 [-w DEADLINE] [-c COUNT] [-i INTERVAL] [-I INTERFACE [-P]] [-M PMTUDISC_OPT] [-s SIZE] HOSTNAME -w DEADLINE Stop after a total of DEADLINE seconds, or (if specified) COUNT packets are sent, whichever comes first. -c COUNT Stop after sending COUNT packets, or (if specified) DEADLINE seconds, whichever comes first -i INTERVAL Wait INTERVAL seconds between packet transmissions. Default is 1. -I INTERFACE Specify the source of the ICMP requests. If INTERFACE is an IP address, then source the ICMP requests from that address. If INTERFACE is not an IP address, treat it as an interface name and source from that interface. When treated as a name, exact matches of interface \"Ifaces\" are chosen first, followed by exact matches of \"Device UIDs\", and finally case-insensitive matches to Network names are returned. See the \"devices\" command for a list of valid \"Ifaces\" and \"Device UIDs\". -P Requires the -I INTERFACE option. When -P is present, ping will source from an IP address on the specified interface, instead of the interface itself. -s SIZE Specifiy the ICMP data length, in bytes. The default is 56 bytes, which will result in an ICMP packet length of 64 bytes (56 data bytes plus 8 ICMP header bytes) [admin@IBR900-13e: /]$ help pwd Print the current working directory [admin@IBR900-13e: /]$ help qos Show QoS statistics. Usage: qos [admin@IBR900-13e: /]$ help reboot Reboot the router [admin@IBR900-13e: /]$ help reset Reset the tty to default settings [admin@IBR900-13e: /]$ help resources Report the system resource usage. [admin@IBR900-13e: /]$ help rip Usage: rip [cmd] Excecute and show output of \"cmd\". If no \"cmd\" is given then access to the Quagga RIP CLI is given. Use \"rip list\" to see avaliable commands or See the Quagga documentation for details. *** WARNING: Changes made using the Quagga CLI do not persist! *** The configuration entered via the Cradlepoint user interface will be restored after user interface changes and/or a router reboot. If the routing protocol options in the user interface do not allow a necessary Quagga configuration, please provide all configuration details to Cradlepoint support. [admin@IBR900-13e: /]$ help ripng Usage: ripng [cmd] Excecute and show output of \"cmd\". If no \"cmd\" is given then access to the Quagga RIPNG CLI is given. Use \"ripng list\" to see avaliable commands or See the Quagga documentation for details. *** WARNING: Changes made using the Quagga CLI do not persist! *** The configuration entered via the Cradlepoint user interface will be restored after user interface changes and/or a router reboot. If the routing protocol options in the user interface do not allow a necessary Quagga configuration, please provide all configuration details to Cradlepoint support. [admin@IBR900-13e: /]$ help route The route command can be used to show the current routing tables as well as make changes to the user defined static routing table. USAGE: route [show [{TABLE}]] Show route(s) route config {bgp|ospf|rip|ripng|static} Show routing protocol configuration route add IP/NM [gw IP] [dev UID] [auto] Add new user defined static route route del IP/NM [gw IP] [dev UID] Remove user defined static route(s) EXAMPLES: route add 172.0.0.100 dev primarylan route add 10.0.0.0/8 gw 172.0.0.100 route add 2000::/3 dev guestlan metric 100 NOTE: Only the user defined static routing table may be modified directly. [admin@IBR900-13e: /]$ help rtpolicy Show the routing policies used by the router. These policies control which routing table will be used based on certain packet matching criteria and can also control certain routing decisions. USAGE: rtpolicy Show policies(s) [admin@IBR900-13e: /]$ help serial The serial command can be used to access a serial adapter connected to the router. USAGE: serial [--force] Connects to the first serial device using the serial configuration in config/system/serial USAGE: serial [--force] [number] Connects to the [number] port of a multi-port serial device using the serial configuration in config/system/serial Only one remote user can be connected to a serial port at a time. The --force option will force quit another user to allow this command to take over. [admin@IBR900-13e: /]$ help set Set a value to a config item Usage: set PATH VALUE [admin@IBR900-13e: /]$ help sleep sleep SECONDS [admin@IBR900-13e: /]$ help sms The sms command is used to send a msg to the given address (optionally from the modem on port) Usage: sms addr msg [port] Example: sms 2081234567 'hello world' usb1 Note: Add a '+' before the addr for international numbers [admin@IBR900-13e: /]$ help ssh Create an SSH connection to an SSH server. usage: ssh [-v] [-C] [-1] [-2] [-l username] [-p port] [-c cipher_spec] [user@]hostname required arguments: hostname Either the hostname or a user@hostname pair optional arguments: -v Debug level. May be specified up to 3 times (-v, -vv, -vvv). -C Requests compression of all data. -1 Force ssh to try protocol version 1 only. -2 Force ssh to try protocol version 2 only. -l Specifies login name. -p Specifies port. -c Comma separated list of ciphers (e.g. aes256-ctr,aes192-ctr,aes128-ctr,). Supported ciphers: aes256-ctr,aes192-ctr,aes128-ctr,aes256-cbc,aes192-cbc,aes128-cbc,3des-cbc,blowfish-cbc Note: Cradlepoint routers support only PCI-compliant ciphers by default. To relax this behavior, set /config/firewall/ssh_admin/weak_ciphers to true. Doing so will set the ciphers to the list of supported ciphers (above). [admin@IBR900-13e: /]$ help stp Show the current STP configuration and status. [admin@IBR900-13e: /]$ help switch Show ethernet switch status. Usage: switch [-v] [show|set|clearstats] show [port PORT_NUM]: Only show the port specified set PORT_NUM [--link_speed=LINK_SPD] [--enabled=true/false] [--port_name=PORT_NAME] clearstats [PORT_NUM] [admin@IBR900-13e: /]$ help tcpdump Usage: tcpdump [-aAdDeflLnNOpqRStuvxX] [-c count] [ -E algo:secret ] [ -i interface ] [ -M secret ] [ -s snaplen ] [ -T type ] [ -y datalinktype ] [ expression ] [admin@IBR900-13e: /]$ help telnet The telnet command can be used to access telnet services located on the routers LAN. USAGE: telnet [host] [port] [timeout] [admin@IBR900-13e: /]$ help threads Show currently active threads. USAGE: threads [-v] [admin@IBR900-13e: /]$ help traceroute Usage: traceroute [options] host [packet_len] Available options: -4 Use IPv4 -6 Use IPv6 -F Do not fragment packets -f Start from the first_ttl hop (instead from 1) -g Route packets through the specified gateway -I Use ICMP ECHO for tracerouting -T Use TCP SYN for tracerouting (default port is 80) -i Specify a network interface to operate with -m Set the max number of hops (max TTL to be reached). Default is 30 -n Do not resolve IP addresses to their domain names -p Set the destination port to use -t Set the TOS (IPv4 type of service) or TC (IPv6 traffic class) value for outgoing packets -l Use specified flow_label for IPv6 packets -w Set the number of seconds to wait for response to a probe (default is 5.0) -q Set the number of probes per each hop. Default is 3 -r Bypass the normal routing and send directly to a host on an attached network -s Use source src_addr for outgoing packets -z Minimal time interval between probes (default 0) -e Show ICMP extensions (if present), including MPLS --sport=num Use source port num for outgoing packets. Implies \"-N 1\" --fwmark=num Set firewall mark for outgoing packets -U Use UDP to particular port for tracerouting -UL Use UDPLITE for tracerouting (default dest port is 53) -P Use raw packet of protocol prot for tracerouting --mtu Discover MTU along the path being traced. Implies \"-F -N 1\" --back Guess the number of hops in the backward path and print if it differs -V Print version info and exit [admin@IBR900-13e: /]$ help uptime Show system uptime and load avg. [admin@IBR900-13e: /]$ help vlan Show VLAN configuration. [admin@IBR900-13e: /]$ help vpn Start, stop, or show status of all vpn tunnels. Start, stop or restart a tunnel. Adjust logging for various ipsec subsystems. Reset sets the default and all subsystems to log level 1. vpn [show|start|stop] vpn [starttunnel|stoptunnel|restarttunnel] -n <TUNNEL NAME> vpn config vpn loglevel reset vpn loglevel default [-1|0|1|2|3|4] vpn loglevel [app|asn|cfg|chd|dmn|enc|esp|ike|imc|imv|job|knl|lib|mgr|net|pts|tls|tnc] [-1|0|1|2|3|4] [admin@IBR900-13e: /]$ help vrrp Show the current VRRP configuration and status. [admin@IBR900-13e: /]$ help wan Show all the attached wan devices and their current state. Usage: wan [monitor] [UID] [CONFIG...] CONFIG: Can be any number of --key=[value] pairs as defined in the /config/wan/rules2 config section. If the optional [value] argument is ommited then the current value (if any) will be printed. Get example: wan cp1 --ip_mode --static.ip_address Set example: wan cp1 --ip_mode=\"static\" --static.ip_address=\"10.0.0.1\" --static.netmask=\"255.0.0.0\" [admin@IBR900-13e: /]$ help wireless Show the current wireless configuration and status. Usage: wireless [OPTIONS...] -w Print information about enabled Access Point profiles -d Print information about enabled WiFi as WAN / WiFi Bridge profiles -c Print information about currently associated WiFi clients -v Print additional driver specific debug when available -t [CHANNEL] Set wireless channel -k [MAC ADDRESS] Kick wireless client -s [OPTIONAL DWELL TIME] Perform a wireless survey -a [OPTIONAL DWELL TIME] Do a survey and autoselect a channel -r [RADIO] Radio to operate on (0: first, 1: second) [admin@IBR900-13e: /]$ help workqueue Print current workqueue activities and schedules. USAGE: workqueue [-v] [-t TASKID] [-a] -v: VERBOSE -t TASKID: Only show information for a single task -a: Show information for active tasks (ie. currently executing) [admin@IBR900-13e: /]$ help xfrm Usage: xfrm policy list [ SELECTOR ] [ dir DIR ] [ index INDEX ] [ action ACTION ] Usage: xfrm policy flush SELECTOR := [ src ADDR[/PLEN] ] [ dst ADDR[/PLEN] ] [ dev DEV ] [ UPSPEC ] UPSPEC := proto { { tcp | udp | sctp | dccp } [ sport PORT ] [ dport PORT ] | { icmp | ipv6-icmp | 135 } [ type NUMBER ] [ code NUMBER ] | gre [ key { DOTTED-QUAD | NUMBER } ] | PROTO } DIR := in | out | fwd ACTION := allow | block Usage: xfrm state list [ ID ] [ mode MODE ] Usage: xfrm state flush ID := [ src ADDR ] [ dst ADDR ] [ spi SPI ] MODE := transport | tunnel [admin@IBR900-13e: /]$ help zebra Usage: zebra [cmd] Excecute and show output of \"cmd\". If no \"cmd\" is given then access to the Quagga CLI is given. Use \"zebra list\" to see avaliable commands or See the Quagga documentation for details. *** WARNING: Changes made using the Quagga CLI do not persist! *** The configuration entered via the Cradlepoint user interface will be restored after user interface changes and/or a router reboot. If the routing protocol options in the user interface do not allow a necessary Quagga configuration, please provide all configuration details to Cradlepoint support. [admin@IBR900-13e: /]$","title":"Cradlepoint"},{"location":"cradlepoint/#cradlepoint-ibr900","text":"https://cradlepoint.com/products/cor-ibr900-series https://cradlepoint.com/sites/default/files/upload-file/cradlepoint_ibr900_manual_0.pdf The CLI is not a normal shell, but is a minimal appliance type UI.","title":"Cradlepoint IBR900"},{"location":"cradlepoint/#get-config-data","text":"This will get all config data, which can be over 25k lines of JSON. get To get just a subset of the data, use something like: [admin@IBR900-13e: /]$ get config/vlan [ { \"mode\": \"wan\", \"ports\": [], \"uid\": \"wan\", \"vid\": 1 }, { \"mode\": \"lan\", \"ports\": [ { \"mode\": \"untagged\", \"port\": 0 }, { \"mode\": \"untagged\", \"port\": 1 }, { \"mode\": \"untagged\", \"port\": 2 } ], \"uid\": \"lan\", \"vid\": 2 } ]","title":"Get config data"},{"location":"cradlepoint/#set-and-fetch-variables","text":"[admin@IBR900-13e: /]$ set foo/bar: \"baz\" [admin@IBR900-13e: /]$ get foo { \"bar:\": \"baz\" }","title":"Set and fetch variables"},{"location":"cradlepoint/#getting-help","text":"[admin@IBR900-13e: /]$ help Available Commands: SupportQA adduser append arpdump atterm banner bgp cd clear clients cpconnect date delete deluser devices diff edit exit factory_reset find free get gre grep help inspect ips ipset lan log ls mkdir nemo netcloud netfilter netstat nhrp ospf passwd ping ping6 pwd qos reboot reset resources rip ripng route rtpolicy serial set sleep sms ssh stp switch tcpdump telnet threads traceroute uptime vlan vpn vrrp wan wireless workqueue xfrm zebra Available Aliases: cat => get dir => ls ll => ls -l 1 more => get post => append put => set quit => exit rm => delete To get help for a specific command run: \"help CMD\" [admin@IBR900-13e: /]$ help SupportQA Command to provide debugging data [admin@IBR900-13e: /]$ help adduser Add a new user account and set the password for this account. Usage: adduser USER [admin@IBR900-13e: /]$ help append Append new item to an array in the config append PATH VALUE [admin@IBR900-13e: /]$ help arpdump The arpdump command shows the current ARP table. [admin@IBR900-13e: /]$ help atterm The atterm command can be used to get direct access to a modem's AT command channel. If a modem interface is not given then the system will pick the highest priority modem. To discover a modem's interface to use, use the command 'devices' and use the value found under the 'SubIface' column. USAGE: atterm [interface] [-c ATCMD] [-t SOCKET_TIMEOUT] [admin@IBR900-13e: /]$ help banner banner [set|unset] [admin@IBR900-13e: /]$ help bgp Usage: bgp [cmd] Excecute and show output of \"cmd\". If no \"cmd\" is given then access to the Quagga BGP CLI is given. Use \"bgp list\" to see avaliable commands or See the Quagga documentation for details. *** WARNING: Changes made using the Quagga CLI do not persist! *** The configuration entered via the Cradlepoint user interface will be restored after user interface changes and/or a router reboot. If the routing protocol options in the user interface do not allow a necessary Quagga configuration, please provide all configuration details to Cradlepoint support. [admin@IBR900-13e: /]$ help cd Change to a new config directory Usage: cd DIR/[DIR...] [admin@IBR900-13e: /]$ help clear Clear the screen [admin@IBR900-13e: /]$ help clients The clients command can be used to show the currently connected clients, both wireless and wired as well as hotspot. USAGE: clients [show | revoke | kick ] [SUBOPTIONS...] show [CONNECTION TYPE]: Show clients by connection type. If no connection type option is given, all connection types will be shown. Connection Types: all : All Known Clients, both wired and wireless, that the router knows about. wlan : Wireless LAN Clients hotspot : Authenticated Hotspot Clients revoke [-t [ip | mac | hn]]: Revoke a client's Hotspot authentication. -t : The type of adddress to be used to find the client in the client list ip : IP address of the client (default) mac : MAC address of the client hn : Hostname of the client (cannot be used if client does not have a hostname) kick [-t [ip | mac | hn]]: Remove wireless access until the router is rebooted. -t : The type of adddress to be used to find the client in the client list ip : IP address of the client (default) mac : MAC address of the client hn : Hostname of the client (cannot be used if client does not have a hostname) [admin@IBR900-13e: /]$ help cpconnect Usage: cpconnect {COMMAND} [OPTIONS] where COMMAND := { add | remove | show } OPTIONS := {-n[ame] | -u[ser] | -p[asswd] | -s[server] } [VAR] example: cpconnect add -n test1 -u user1 -p pass1 -s vpn.accessmylan.com cpconnect show [admin@IBR900-13e: /]$ help date Show system clock. [admin@IBR900-13e: /]$ help delete Delete an item from the config Usage: delete PATH [admin@IBR900-13e: /]$ help deluser Delete a user account. Usage: deluser USER [admin@IBR900-13e: /]$ help devices Display network devices connected to the router. USAGE: devices [-v] Show all devices(s) devices [-v] [DEVICE_UID...] Show one or more specific devices. -v Verbose output [admin@IBR900-13e: /]$ help diff Show differences between the current and the default configuration. [admin@IBR900-13e: /]$ help edit Usage: edit CONFIG_PATH Examples: edit /config/system/gps edit . [admin@IBR900-13e: /]$ help exit Exit the shell [admin@IBR900-13e: /]$ help factory_reset Reset config to factory defaults [admin@IBR900-13e: /]$ help find Find or list files and combine with grep to locate specific files Usage: find PATH [| grep file] [admin@IBR900-13e: /]$ help free Show amount (kilobytes) of free and used system memory. \"free\" memory is presently unused, while \"available\" includes used memory, typically for cache, that is readily reclaimed for something else when needed. [admin@IBR900-13e: /]$ help get Get value for config item(s) Usage: get PATH [PATH...] [admin@IBR900-13e: /]$ help gre Start, stop, or show status of all gre tunnels. Start, stop or restart a tunnel. Renew or release the DHCP lease for a tunnel (if DHCP for GRE is enabled). gre [show|start|stop] gre [starttunnel|stoptunnel|restarttunnel|renew|release] -n <TUNNEL NAME> [admin@IBR900-13e: /]$ help grep Usage: grep PATTERN [FILE]... [admin@IBR900-13e: /]$ help help To get help for a specific command run: \"help CMD\" [admin@IBR900-13e: /]$ help inspect Inspect a directory Usage: inspect [PATH] [admin@IBR900-13e: /]$ help ips Interact with the IPS engine. Usage: ips [load|show|mode|status] [SUBOPTIONS..] load [-f PATH]: Load a rule file into the IPS engine -f [PATH]: Load rule file from PATH (Must contain the filename) show [-o [ids | cats | sec | app | anom]]: Show information from the signatures loaded. ids: Print all the signatures (default) cats: Print all the categories sec: Print only the signatures for the Security rules app: Print only the signatures for the Application rules anom: Print only the signatures for the Anomaly rules mode [-o [off | ips | ids]]: Change the IPS Global mode off: Set Default IPS mode to 'Disabled' (If no other Category or Signature is 'Enabled' then the kernel modules will be unloaded) ips: Set Default IPS mode to 'Detect and Prevent' ids: Set Default IPS mode to 'Detect Only' status: Print the status of the IPS engine update: Request a Rule File Update [admin@IBR900-13e: /]$ help ipset ipset [list {name}] specify the name of the set to list, or nothing to see the names of the sets [admin@IBR900-13e: /]$ help lan Show the current LAN configuration and status. [admin@IBR900-13e: /]$ help log Show and manipulate the log system. Usage: log [show|clear|service|level|msg] [SUBOPTONS...] show [FILTER] [FILTERN] [[-bhi] -s SEARCH]: FILTER can be one or more space-separated names or levels. eg. log show wanmgr kernel DEBUG INFO -b bold new entries -h HIGHLIGHT Same usage as searching but does not filter results. -i makes the search case insensitive. -s SEARCH can be any string to search for in the log message contents. eg. log show -s Firmware -f [LINES_OF_HISTORY] Follow mode with optional argument for number of lines of history to show. -r recover crash log if one exists. service [level (DEBUG|INFO|WARNING|ERROR|CRITICAL)]: Display all service log levels. level change service log level. clear: Erase all logs from memory level [NEW_LEVEL]: View current level or set new log level to: critical, error, warning, info, or debug msg [-l LEVEL] MESSAGE: Write a message to the logging system. LEVEL defaults to: info [admin@IBR900-13e: /]$ help ls List files in the current config directory [admin@IBR900-13e: /]$ help mkdir Create an empty container Usage: mkdir DIRNAME [admin@IBR900-13e: /]$ help nemo Show status and configuration of NEMO session Usage: nemo [-v] [admin@IBR900-13e: /]$ help netcloud Manage connection to the Cradlepoint NetCloud. Usage: netcloud [alert|status|register|stop|start|restart|triggers] status: [DEFAULT] Show current status information. register [--username=USER --password=PW]|[--token_id=TID --token_secret=TS]: (Re)register --username: The NetCloud username that should be used to authenticate. --password: The NetCloud password that should be used to authenticate. --token_id: [EXPERT] Token ID for token authentication mode. --token_secret: [EXPERT] Token secret for token authentication mode. unregister: Unregister this router from NetCloud and unregister. stop: Manually stop the NetCloud client. start: Manually start the NetCloud client. restart: Manually restart the NetCloud client. [admin@IBR900-13e: /]$ help netfilter Show info and debug from netfilter2 rule(s) and trigger(s). Usage: netfilter [active|all|upnp|triggers|states] [input|routed|output] [ip/ip6] [-v] [-s] [-r RULE_INDEX] [-t TRIGGER_INDEX] [-s STATE_INDEX] [admin@IBR900-13e: /]$ help netstat Usage: netstat [-al] [-tuwx] [-enWp] Display networking information -a All sockets -l Listening sockets Else: connected sockets -t TCP sockets -u UDP sockets -w Raw sockets -x Unix sockets Else: all socket types -e Other/more information -n Don't resolve names -W Wide display -p Show PID/program name for sockets [admin@IBR900-13e: /]$ help nhrp Usage: nhrp show nhrp flush nhrp flush nbma ip [admin@IBR900-13e: /]$ help ospf Usage: ospf [cmd] Excecute and show output of \"cmd\". If no \"cmd\" is given then access to the Quagga OSPF CLI is given. Use \"ospf list\" to see avaliable commands or See the Quagga documentation for details. *** WARNING: Changes made using the Quagga CLI do not persist! *** The configuration entered via the Cradlepoint user interface will be restored after user interface changes and/or a router reboot. If the routing protocol options in the user interface do not allow a necessary Quagga configuration, please provide all configuration details to Cradlepoint support. [admin@IBR900-13e: /]$ help passwd Set the password for a user. Usage: passwd [USER] [admin@IBR900-13e: /]$ help ping Send ICMP echo_request(s) to a networked host Usage: ping [-w DEADLINE] [-c COUNT] [-i INTERVAL] [-I INTERFACE [-P]] [-M PMTUDISC_OPT] [-s SIZE] HOSTNAME -w DEADLINE Stop after a total of DEADLINE seconds, or (if specified) COUNT packets are sent, whichever comes first. -c COUNT Stop after sending COUNT packets, or (if specified) DEADLINE seconds, whichever comes first -i INTERVAL Wait INTERVAL seconds between packet transmissions. Default is 1. -I INTERFACE Specify the source of the ICMP requests. If INTERFACE is an IP address, then source the ICMP requests from that address. If INTERFACE is not an IP address, treat it as an interface name and source from that interface. When treated as a name, exact matches of interface \"Ifaces\" are chosen first, followed by exact matches of \"Device UIDs\", and finally case-insensitive matches to Network names are returned. See the \"devices\" command for a list of valid \"Ifaces\" and \"Device UIDs\". -P Requires the -I INTERFACE option. When -P is present, ping will source from an IP address on the specified interface, instead of the interface itself. -s SIZE Specifiy the ICMP data length, in bytes. The default is 56 bytes, which will result in an ICMP packet length of 64 bytes (56 data bytes plus 8 ICMP header bytes) -M PMTU_OPT Select Path MTU discovery. PMTU_OPT must be one of: \"do\", \"want\" or \"dont\". If the PMTU_OPT is \"do\" (default), then ping will set the Don't Fragment (DF) flag in the ICMP requests, which will prohibit packet fragmentation. If PMTU_OPT is \"want\", then ping will fragment if the ICMP request exceeds the local outbound interfaces' MTU. Finally if PMTU_OPT is \"dont\" (do not set the DF flag), then fragmentation is allowed and ICMP Requests will be fragmented as necessary in response to ICMP Fragmentation Responses. [admin@IBR900-13e: /]$ help ping6 Send ICMPv6 echo_request(s) to a networked host Usage: ping6 [-w DEADLINE] [-c COUNT] [-i INTERVAL] [-I INTERFACE [-P]] [-M PMTUDISC_OPT] [-s SIZE] HOSTNAME -w DEADLINE Stop after a total of DEADLINE seconds, or (if specified) COUNT packets are sent, whichever comes first. -c COUNT Stop after sending COUNT packets, or (if specified) DEADLINE seconds, whichever comes first -i INTERVAL Wait INTERVAL seconds between packet transmissions. Default is 1. -I INTERFACE Specify the source of the ICMP requests. If INTERFACE is an IP address, then source the ICMP requests from that address. If INTERFACE is not an IP address, treat it as an interface name and source from that interface. When treated as a name, exact matches of interface \"Ifaces\" are chosen first, followed by exact matches of \"Device UIDs\", and finally case-insensitive matches to Network names are returned. See the \"devices\" command for a list of valid \"Ifaces\" and \"Device UIDs\". -P Requires the -I INTERFACE option. When -P is present, ping will source from an IP address on the specified interface, instead of the interface itself. -s SIZE Specifiy the ICMP data length, in bytes. The default is 56 bytes, which will result in an ICMP packet length of 64 bytes (56 data bytes plus 8 ICMP header bytes) [admin@IBR900-13e: /]$ help pwd Print the current working directory [admin@IBR900-13e: /]$ help qos Show QoS statistics. Usage: qos [admin@IBR900-13e: /]$ help reboot Reboot the router [admin@IBR900-13e: /]$ help reset Reset the tty to default settings [admin@IBR900-13e: /]$ help resources Report the system resource usage. [admin@IBR900-13e: /]$ help rip Usage: rip [cmd] Excecute and show output of \"cmd\". If no \"cmd\" is given then access to the Quagga RIP CLI is given. Use \"rip list\" to see avaliable commands or See the Quagga documentation for details. *** WARNING: Changes made using the Quagga CLI do not persist! *** The configuration entered via the Cradlepoint user interface will be restored after user interface changes and/or a router reboot. If the routing protocol options in the user interface do not allow a necessary Quagga configuration, please provide all configuration details to Cradlepoint support. [admin@IBR900-13e: /]$ help ripng Usage: ripng [cmd] Excecute and show output of \"cmd\". If no \"cmd\" is given then access to the Quagga RIPNG CLI is given. Use \"ripng list\" to see avaliable commands or See the Quagga documentation for details. *** WARNING: Changes made using the Quagga CLI do not persist! *** The configuration entered via the Cradlepoint user interface will be restored after user interface changes and/or a router reboot. If the routing protocol options in the user interface do not allow a necessary Quagga configuration, please provide all configuration details to Cradlepoint support. [admin@IBR900-13e: /]$ help route The route command can be used to show the current routing tables as well as make changes to the user defined static routing table. USAGE: route [show [{TABLE}]] Show route(s) route config {bgp|ospf|rip|ripng|static} Show routing protocol configuration route add IP/NM [gw IP] [dev UID] [auto] Add new user defined static route route del IP/NM [gw IP] [dev UID] Remove user defined static route(s) EXAMPLES: route add 172.0.0.100 dev primarylan route add 10.0.0.0/8 gw 172.0.0.100 route add 2000::/3 dev guestlan metric 100 NOTE: Only the user defined static routing table may be modified directly. [admin@IBR900-13e: /]$ help rtpolicy Show the routing policies used by the router. These policies control which routing table will be used based on certain packet matching criteria and can also control certain routing decisions. USAGE: rtpolicy Show policies(s) [admin@IBR900-13e: /]$ help serial The serial command can be used to access a serial adapter connected to the router. USAGE: serial [--force] Connects to the first serial device using the serial configuration in config/system/serial USAGE: serial [--force] [number] Connects to the [number] port of a multi-port serial device using the serial configuration in config/system/serial Only one remote user can be connected to a serial port at a time. The --force option will force quit another user to allow this command to take over. [admin@IBR900-13e: /]$ help set Set a value to a config item Usage: set PATH VALUE [admin@IBR900-13e: /]$ help sleep sleep SECONDS [admin@IBR900-13e: /]$ help sms The sms command is used to send a msg to the given address (optionally from the modem on port) Usage: sms addr msg [port] Example: sms 2081234567 'hello world' usb1 Note: Add a '+' before the addr for international numbers [admin@IBR900-13e: /]$ help ssh Create an SSH connection to an SSH server. usage: ssh [-v] [-C] [-1] [-2] [-l username] [-p port] [-c cipher_spec] [user@]hostname required arguments: hostname Either the hostname or a user@hostname pair optional arguments: -v Debug level. May be specified up to 3 times (-v, -vv, -vvv). -C Requests compression of all data. -1 Force ssh to try protocol version 1 only. -2 Force ssh to try protocol version 2 only. -l Specifies login name. -p Specifies port. -c Comma separated list of ciphers (e.g. aes256-ctr,aes192-ctr,aes128-ctr,). Supported ciphers: aes256-ctr,aes192-ctr,aes128-ctr,aes256-cbc,aes192-cbc,aes128-cbc,3des-cbc,blowfish-cbc Note: Cradlepoint routers support only PCI-compliant ciphers by default. To relax this behavior, set /config/firewall/ssh_admin/weak_ciphers to true. Doing so will set the ciphers to the list of supported ciphers (above). [admin@IBR900-13e: /]$ help stp Show the current STP configuration and status. [admin@IBR900-13e: /]$ help switch Show ethernet switch status. Usage: switch [-v] [show|set|clearstats] show [port PORT_NUM]: Only show the port specified set PORT_NUM [--link_speed=LINK_SPD] [--enabled=true/false] [--port_name=PORT_NAME] clearstats [PORT_NUM] [admin@IBR900-13e: /]$ help tcpdump Usage: tcpdump [-aAdDeflLnNOpqRStuvxX] [-c count] [ -E algo:secret ] [ -i interface ] [ -M secret ] [ -s snaplen ] [ -T type ] [ -y datalinktype ] [ expression ] [admin@IBR900-13e: /]$ help telnet The telnet command can be used to access telnet services located on the routers LAN. USAGE: telnet [host] [port] [timeout] [admin@IBR900-13e: /]$ help threads Show currently active threads. USAGE: threads [-v] [admin@IBR900-13e: /]$ help traceroute Usage: traceroute [options] host [packet_len] Available options: -4 Use IPv4 -6 Use IPv6 -F Do not fragment packets -f Start from the first_ttl hop (instead from 1) -g Route packets through the specified gateway -I Use ICMP ECHO for tracerouting -T Use TCP SYN for tracerouting (default port is 80) -i Specify a network interface to operate with -m Set the max number of hops (max TTL to be reached). Default is 30 -n Do not resolve IP addresses to their domain names -p Set the destination port to use -t Set the TOS (IPv4 type of service) or TC (IPv6 traffic class) value for outgoing packets -l Use specified flow_label for IPv6 packets -w Set the number of seconds to wait for response to a probe (default is 5.0) -q Set the number of probes per each hop. Default is 3 -r Bypass the normal routing and send directly to a host on an attached network -s Use source src_addr for outgoing packets -z Minimal time interval between probes (default 0) -e Show ICMP extensions (if present), including MPLS --sport=num Use source port num for outgoing packets. Implies \"-N 1\" --fwmark=num Set firewall mark for outgoing packets -U Use UDP to particular port for tracerouting -UL Use UDPLITE for tracerouting (default dest port is 53) -P Use raw packet of protocol prot for tracerouting --mtu Discover MTU along the path being traced. Implies \"-F -N 1\" --back Guess the number of hops in the backward path and print if it differs -V Print version info and exit [admin@IBR900-13e: /]$ help uptime Show system uptime and load avg. [admin@IBR900-13e: /]$ help vlan Show VLAN configuration. [admin@IBR900-13e: /]$ help vpn Start, stop, or show status of all vpn tunnels. Start, stop or restart a tunnel. Adjust logging for various ipsec subsystems. Reset sets the default and all subsystems to log level 1. vpn [show|start|stop] vpn [starttunnel|stoptunnel|restarttunnel] -n <TUNNEL NAME> vpn config vpn loglevel reset vpn loglevel default [-1|0|1|2|3|4] vpn loglevel [app|asn|cfg|chd|dmn|enc|esp|ike|imc|imv|job|knl|lib|mgr|net|pts|tls|tnc] [-1|0|1|2|3|4] [admin@IBR900-13e: /]$ help vrrp Show the current VRRP configuration and status. [admin@IBR900-13e: /]$ help wan Show all the attached wan devices and their current state. Usage: wan [monitor] [UID] [CONFIG...] CONFIG: Can be any number of --key=[value] pairs as defined in the /config/wan/rules2 config section. If the optional [value] argument is ommited then the current value (if any) will be printed. Get example: wan cp1 --ip_mode --static.ip_address Set example: wan cp1 --ip_mode=\"static\" --static.ip_address=\"10.0.0.1\" --static.netmask=\"255.0.0.0\" [admin@IBR900-13e: /]$ help wireless Show the current wireless configuration and status. Usage: wireless [OPTIONS...] -w Print information about enabled Access Point profiles -d Print information about enabled WiFi as WAN / WiFi Bridge profiles -c Print information about currently associated WiFi clients -v Print additional driver specific debug when available -t [CHANNEL] Set wireless channel -k [MAC ADDRESS] Kick wireless client -s [OPTIONAL DWELL TIME] Perform a wireless survey -a [OPTIONAL DWELL TIME] Do a survey and autoselect a channel -r [RADIO] Radio to operate on (0: first, 1: second) [admin@IBR900-13e: /]$ help workqueue Print current workqueue activities and schedules. USAGE: workqueue [-v] [-t TASKID] [-a] -v: VERBOSE -t TASKID: Only show information for a single task -a: Show information for active tasks (ie. currently executing) [admin@IBR900-13e: /]$ help xfrm Usage: xfrm policy list [ SELECTOR ] [ dir DIR ] [ index INDEX ] [ action ACTION ] Usage: xfrm policy flush SELECTOR := [ src ADDR[/PLEN] ] [ dst ADDR[/PLEN] ] [ dev DEV ] [ UPSPEC ] UPSPEC := proto { { tcp | udp | sctp | dccp } [ sport PORT ] [ dport PORT ] | { icmp | ipv6-icmp | 135 } [ type NUMBER ] [ code NUMBER ] | gre [ key { DOTTED-QUAD | NUMBER } ] | PROTO } DIR := in | out | fwd ACTION := allow | block Usage: xfrm state list [ ID ] [ mode MODE ] Usage: xfrm state flush ID := [ src ADDR ] [ dst ADDR ] [ spi SPI ] MODE := transport | tunnel [admin@IBR900-13e: /]$ help zebra Usage: zebra [cmd] Excecute and show output of \"cmd\". If no \"cmd\" is given then access to the Quagga CLI is given. Use \"zebra list\" to see avaliable commands or See the Quagga documentation for details. *** WARNING: Changes made using the Quagga CLI do not persist! *** The configuration entered via the Cradlepoint user interface will be restored after user interface changes and/or a router reboot. If the routing protocol options in the user interface do not allow a necessary Quagga configuration, please provide all configuration details to Cradlepoint support. [admin@IBR900-13e: /]$","title":"Getting help"},{"location":"cron/","text":"Per-user and system-wide scheduled tasks, handled by the cron daemon. Locations Cron scripts and entries can run from several locations. By using /etc/crontab.d/scriptname you can set different MAILTO and ENV variables and isolate your scheduled jobs. User jobs can be edited via crontab -e . DST Some cron daemons don't handle DST correctly. Because of this, do not schedule jobs within the our of 1am. During DST changes this hour happens twice or is skipped altogether. Cronie says it handled DST gracefully, running jobs that should have run but haven't yet due to time changes, or no running jobs twice when time goes back. Syntax quirks Some systems have problems with #/# syntax. (eg: 15 2/6 * * * /usr/bin/whatever ) Default Editor In some systems, the default editor is found by the symlink located at /etc/defaults/editor. To override this, export your EDITOR shell variable. (eg: export EDITOR=/usr/bin/vim ) Examples Quick and dirty realignment This will definitely fail in some circumstances. alias crontab-align='crontab -l | while read -r a b c d e f ; do if [[ \"$a\" =~ ^## ]] ; then echo \"$a $b $c $d $e $f\" ; else printf \"% -20s %s\\n\" \"$a $b $c $d $e\" \"$f\" ; fi ; done' crontab-align | crontab - Add a random delay This example sleeps for a random number of seconds lower than 1800, including 0. The % symbol has to be escaped in crontabs. 0 * * * * sleep $((RANDOM \\% 1800)) ; /usr/local/bin/do-a-thing.sh ; Programmatic editing of the crontab This is potentially dangerous because you can wipe out a user's crontab. crontab -l | sed -e '/downtime/s/^\\#//' | crontab - echo \"* * * * * /usr/local/bin/every_minute.sh\" | crontab - See if and when parts are running Put this in /etc/cron.*/01-cron-log : logger -t cron Running `basename $PWD` OS X Alarm Clock ## Alarm Clock 55 5 * * 1-5 /usr/bin/osascript -e 'tell application \"iTunes\"' -e 'set the sound volume to 100' -e 'end tell' 00 6 * * 1-5 /usr/bin/osascript -e 'tell application \"iTunes\"' -e 'play playlist \"Old Podcasts\"' -e 'end tell' 15 8 * * 1-5 /usr/bin/osascript -e 'tell application \"iTunes\" to stop' Curl every other hour 0 */2 * * * curl -fs \"http://bighugelabs.com/flickr/profilewidget/randomint/000000/ffffff/94246031@N00.jpg\" > /dev/null Troubleshooting Having junk files like temp vim files in /var/cron/tabs can make cron go to 100% cpu usage. Remove all non crontab files and kill cron to fix it. Links https://crontab.guru/ - Online crontab linter and explainer","title":"cron"},{"location":"cron/#locations","text":"Cron scripts and entries can run from several locations. By using /etc/crontab.d/scriptname you can set different MAILTO and ENV variables and isolate your scheduled jobs. User jobs can be edited via crontab -e .","title":"Locations"},{"location":"cron/#dst","text":"Some cron daemons don't handle DST correctly. Because of this, do not schedule jobs within the our of 1am. During DST changes this hour happens twice or is skipped altogether. Cronie says it handled DST gracefully, running jobs that should have run but haven't yet due to time changes, or no running jobs twice when time goes back.","title":"DST"},{"location":"cron/#syntax-quirks","text":"Some systems have problems with #/# syntax. (eg: 15 2/6 * * * /usr/bin/whatever )","title":"Syntax quirks"},{"location":"cron/#default-editor","text":"In some systems, the default editor is found by the symlink located at /etc/defaults/editor. To override this, export your EDITOR shell variable. (eg: export EDITOR=/usr/bin/vim )","title":"Default Editor"},{"location":"cron/#examples","text":"","title":"Examples"},{"location":"cron/#quick-and-dirty-realignment","text":"This will definitely fail in some circumstances. alias crontab-align='crontab -l | while read -r a b c d e f ; do if [[ \"$a\" =~ ^## ]] ; then echo \"$a $b $c $d $e $f\" ; else printf \"% -20s %s\\n\" \"$a $b $c $d $e\" \"$f\" ; fi ; done' crontab-align | crontab -","title":"Quick and dirty realignment"},{"location":"cron/#add-a-random-delay","text":"This example sleeps for a random number of seconds lower than 1800, including 0. The % symbol has to be escaped in crontabs. 0 * * * * sleep $((RANDOM \\% 1800)) ; /usr/local/bin/do-a-thing.sh ;","title":"Add a random delay"},{"location":"cron/#programmatic-editing-of-the-crontab","text":"This is potentially dangerous because you can wipe out a user's crontab. crontab -l | sed -e '/downtime/s/^\\#//' | crontab - echo \"* * * * * /usr/local/bin/every_minute.sh\" | crontab -","title":"Programmatic editing of the crontab"},{"location":"cron/#see-if-and-when-parts-are-running","text":"Put this in /etc/cron.*/01-cron-log : logger -t cron Running `basename $PWD`","title":"See if and when parts are running"},{"location":"cron/#os-x-alarm-clock","text":"## Alarm Clock 55 5 * * 1-5 /usr/bin/osascript -e 'tell application \"iTunes\"' -e 'set the sound volume to 100' -e 'end tell' 00 6 * * 1-5 /usr/bin/osascript -e 'tell application \"iTunes\"' -e 'play playlist \"Old Podcasts\"' -e 'end tell' 15 8 * * 1-5 /usr/bin/osascript -e 'tell application \"iTunes\" to stop'","title":"OS X Alarm Clock"},{"location":"cron/#curl-every-other-hour","text":"0 */2 * * * curl -fs \"http://bighugelabs.com/flickr/profilewidget/randomint/000000/ffffff/94246031@N00.jpg\" > /dev/null","title":"Curl every other hour"},{"location":"cron/#troubleshooting","text":"Having junk files like temp vim files in /var/cron/tabs can make cron go to 100% cpu usage. Remove all non crontab files and kill cron to fix it.","title":"Troubleshooting"},{"location":"cron/#links","text":"https://crontab.guru/ - Online crontab linter and explainer","title":"Links"},{"location":"cryptsetup/","text":"\"Cryptsetup is utility used to conveniently setup disk encryption based on DMCrypt kernel module.\" - https://gitlab.com/cryptsetup/cryptsetup/blob/master/README.md \"LUKS is the standard for Linux hard disk encryption. By providing a standard on-disk-format, it does not only facilitate compatibility among distributions, but also provides secure management of multiple user passwords.\" - https://gitlab.com/cryptsetup/cryptsetup/blob/master/README.md","title":"cryptsetup"},{"location":"curl/","text":"\"command line tool and library for transferring data with URLs\" - https://curl.haxx.se curl is a tool to transfer data from or to a server, using one of the supported protocols (DICT, FILE, FTP, FTPS, GOPHER, HTTP, HTTPS, IMAP, IMAPS, LDAP, LDAPS, POP3, POP3S, RTMP, RTSP, SCP, SFTP, SMTP, SMTPS, TELNET and TFTP). The command is designed to work without user interaction. Examples Follow location redirects curl -L http://whatever Show the HTTP code for a given request This downloads 1 byte from the remote URI and shows the HTTP code, so it's a quick way to discover HTTP codes. curl --max-filesize 1 -s -w \"%{http_code}\\n\" -o /dev/null http://www.example.com/ Request a specific vhost from a server This is useful for testing production code on non-production multi-tennant name based virtual hosts. curl -H 'Host: www.domain.com' http://example.com Get the length of the file to be downloaded curl -qI https://www.google.com/index.php 2>/dev/null | awk '/Length/ {print $2}' Fetch only HTTP headers, not content curl -I http://www.example.com/some_huge_file.iso Send POST variables curl --data \"user=foo&pass=bar\" http://example.com/login.php Scrape URLs from a page This appears to have problems with some strings. For instance, this doesn't catch the full https://acounts.google.com string. The regex is correct according to http://regexpal.com , but egrep is apparently not handling it correctly. curl -s http://www.google.com | egrep -o '(((https?|ftp|gopher)://|(mailto|file|news):)[^\u2019 <>\\n\"]+|(www|web|w3)\\.[-a-z0-9.]+)[^\u2019 .,;<>\":]' Links Release Notes See Also aria2 httpstat - download and show a some useful connection information wget","title":"cURL"},{"location":"curl/#examples","text":"","title":"Examples"},{"location":"curl/#follow-location-redirects","text":"curl -L http://whatever","title":"Follow location redirects"},{"location":"curl/#show-the-http-code-for-a-given-request","text":"This downloads 1 byte from the remote URI and shows the HTTP code, so it's a quick way to discover HTTP codes. curl --max-filesize 1 -s -w \"%{http_code}\\n\" -o /dev/null http://www.example.com/","title":"Show the HTTP code for a given request"},{"location":"curl/#request-a-specific-vhost-from-a-server","text":"This is useful for testing production code on non-production multi-tennant name based virtual hosts. curl -H 'Host: www.domain.com' http://example.com","title":"Request a specific vhost from a server"},{"location":"curl/#get-the-length-of-the-file-to-be-downloaded","text":"curl -qI https://www.google.com/index.php 2>/dev/null | awk '/Length/ {print $2}'","title":"Get the length of the file to be downloaded"},{"location":"curl/#fetch-only-http-headers-not-content","text":"curl -I http://www.example.com/some_huge_file.iso","title":"Fetch only HTTP headers, not content"},{"location":"curl/#send-post-variables","text":"curl --data \"user=foo&pass=bar\" http://example.com/login.php","title":"Send POST variables"},{"location":"curl/#scrape-urls-from-a-page","text":"This appears to have problems with some strings. For instance, this doesn't catch the full https://acounts.google.com string. The regex is correct according to http://regexpal.com , but egrep is apparently not handling it correctly. curl -s http://www.google.com | egrep -o '(((https?|ftp|gopher)://|(mailto|file|news):)[^\u2019 <>\\n\"]+|(www|web|w3)\\.[-a-z0-9.]+)[^\u2019 .,;<>\":]'","title":"Scrape URLs from a page"},{"location":"curl/#links","text":"Release Notes","title":"Links"},{"location":"curl/#see-also","text":"aria2 httpstat - download and show a some useful connection information wget","title":"See Also"},{"location":"dat/","text":"\"Dat is a nonprofit-backed data sharing protocol for applications of the future\" - https://datproject.org/ https://datproject.org/ https://docs.datproject.org/","title":"dat"},{"location":"date/","text":"The date shell command date behaves differently between gnu and bsd. In OS X you can install gnu date by doing brew install coreutils GNU date Show adjusted date/time date -d -2month # two months ago date -d +1hour # one hour in the future date -d +15minute date -d \"last week + 1 hour\" date -d \"january 10 1978 + 5 years\" +%a Convert a string date to epoch seconds date -d \"Fri Sep 7 2:00 2012\" +%s Convert epoch seconds to string date date -d @1375899534 Output various RFC 3339 time formats date --rfc-3339=date date --rfc-3339=seconds date --rfc-3339=ns Show and number all previous weeks from one year ago for X in {1..53} ; do printf \"%02s \" ${X} ; date -d -49weeks-2days+${X}week \"+%b %d %Y\" ; done ; Show and number all weeks from the point I started working at Zoosk for X in {1..90} ; do printf \"%02s \" ${X} ; date -d \"June 10 2013 - 1 week + ${X} week\" \"+%a %b %d %Y\" ; done ; Show how many seconds old I am echo \"$(date +%s) - $(date -d \"January 10 1978 7:46pm\" +%s)\" | bc Show subsecond date, without going full nano for X in {1..100} ; do date +%s.%N | cut -c1-15 ; done ; Sleep until the next 5 minute 0 seconds mark while sleep $(date \"+60 - %S.%N\" | bc) 240 ; do date \"+%F %T.%N\" ; done ; BSD date Show adjusted date/time date -v-2m # two months ago date -v+1H # one hour in the future Convert epoch seconds to string date date -r 1514308711 See also ntp ptp time","title":"date"},{"location":"date/#gnu-date","text":"","title":"GNU date"},{"location":"date/#show-adjusted-datetime","text":"date -d -2month # two months ago date -d +1hour # one hour in the future date -d +15minute date -d \"last week + 1 hour\" date -d \"january 10 1978 + 5 years\" +%a","title":"Show adjusted date/time"},{"location":"date/#convert-a-string-date-to-epoch-seconds","text":"date -d \"Fri Sep 7 2:00 2012\" +%s","title":"Convert a string date to epoch seconds"},{"location":"date/#convert-epoch-seconds-to-string-date","text":"date -d @1375899534","title":"Convert epoch seconds to string date"},{"location":"date/#output-various-rfc-3339-time-formats","text":"date --rfc-3339=date date --rfc-3339=seconds date --rfc-3339=ns","title":"Output various RFC 3339 time formats"},{"location":"date/#show-and-number-all-previous-weeks-from-one-year-ago","text":"for X in {1..53} ; do printf \"%02s \" ${X} ; date -d -49weeks-2days+${X}week \"+%b %d %Y\" ; done ;","title":"Show and number all previous weeks from one year ago"},{"location":"date/#show-and-number-all-weeks-from-the-point-i-started-working-at-zoosk","text":"for X in {1..90} ; do printf \"%02s \" ${X} ; date -d \"June 10 2013 - 1 week + ${X} week\" \"+%a %b %d %Y\" ; done ;","title":"Show and number all weeks from the point I started working at Zoosk"},{"location":"date/#show-how-many-seconds-old-i-am","text":"echo \"$(date +%s) - $(date -d \"January 10 1978 7:46pm\" +%s)\" | bc","title":"Show how many seconds old I am"},{"location":"date/#show-subsecond-date-without-going-full-nano","text":"for X in {1..100} ; do date +%s.%N | cut -c1-15 ; done ;","title":"Show subsecond date, without going full nano"},{"location":"date/#sleep-until-the-next-5-minute-0-seconds-mark","text":"while sleep $(date \"+60 - %S.%N\" | bc) 240 ; do date \"+%F %T.%N\" ; done ;","title":"Sleep until the next 5 minute 0 seconds mark"},{"location":"date/#bsd-date","text":"","title":"BSD date"},{"location":"date/#show-adjusted-datetime_1","text":"date -v-2m # two months ago date -v+1H # one hour in the future","title":"Show adjusted date/time"},{"location":"date/#convert-epoch-seconds-to-string-date_1","text":"date -r 1514308711","title":"Convert epoch seconds to string date"},{"location":"date/#see-also","text":"ntp ptp time","title":"See also"},{"location":"dcfldd/","text":"dcfldd is an advanced version of dd which is more useful than pv in some situations. One simple advantage dcfldd has over dd is a progress counter displayed by default, although even with dd you can see progress by pressing ctrl-t . However, if all you need is a progress display, pv is really your best bet. Another useful advantage dcfldd has is the ability to specify hex and ascii patterns, as well as the output of a command as the source. You may also specify multiple outputs. Examples Wipe a hard disk This wipes hard disk /dev/rdisk9 with binary 01010101 pattern. dcfldd pattern=AAAA of=/dev/rdisk9 Resume wiping a hard disk You can use seek to skip past the first N blocks on the destination disk. If you have to resume multiple times, perhaps the best option is to use bash's arithmetic expansion to add up the number of blocks written. $ dcfldd pattern=AAAA of=/dev/rdisk3 3328 blocks (104Mb) written.^C 3466+0 records in 3465+0 records out $ dcfldd pattern=AAAA of=/dev/rdisk3 seek=3328 2936064 blocks (91752Mb) written.^C 2936132+0 records in 2936131+0 records out $ dcfldd pattern=AAAA of=/dev/rdisk3 seek=$((3328+2936064)) View progress with pv pv is useful for seeing the transfer rate of the pipe, which can help diagnose continued success or lack thereof with failing hard disks. root# dcfldd pattern=AAAA | pv | dcfldd of=/dev/rdisk3 seek=$((4192000+504000+10240000+2936064)) 512 blocks (16Mb) written.22.1MiB 0:00:07 [21.7MiB/s] [ <=> 1280 blocks (40Mb) written.43.5MiB 0:00:08 [21.5MiB/s] [ <=> 2304 blocks (72Mb) written.79.4MiB 0:00:09 [35.9MiB/s] [ <=> 3584 blocks (112Mb) written. 114MiB 0:00:10 [35.2MiB/s] [ <=>","title":"dcfldd"},{"location":"dcfldd/#examples","text":"","title":"Examples"},{"location":"dcfldd/#wipe-a-hard-disk","text":"This wipes hard disk /dev/rdisk9 with binary 01010101 pattern. dcfldd pattern=AAAA of=/dev/rdisk9","title":"Wipe a hard disk"},{"location":"dcfldd/#resume-wiping-a-hard-disk","text":"You can use seek to skip past the first N blocks on the destination disk. If you have to resume multiple times, perhaps the best option is to use bash's arithmetic expansion to add up the number of blocks written. $ dcfldd pattern=AAAA of=/dev/rdisk3 3328 blocks (104Mb) written.^C 3466+0 records in 3465+0 records out $ dcfldd pattern=AAAA of=/dev/rdisk3 seek=3328 2936064 blocks (91752Mb) written.^C 2936132+0 records in 2936131+0 records out $ dcfldd pattern=AAAA of=/dev/rdisk3 seek=$((3328+2936064))","title":"Resume wiping a hard disk"},{"location":"dcfldd/#view-progress-with-pv","text":"pv is useful for seeing the transfer rate of the pipe, which can help diagnose continued success or lack thereof with failing hard disks. root# dcfldd pattern=AAAA | pv | dcfldd of=/dev/rdisk3 seek=$((4192000+504000+10240000+2936064)) 512 blocks (16Mb) written.22.1MiB 0:00:07 [21.7MiB/s] [ <=> 1280 blocks (40Mb) written.43.5MiB 0:00:08 [21.5MiB/s] [ <=> 2304 blocks (72Mb) written.79.4MiB 0:00:09 [35.9MiB/s] [ <=> 3584 blocks (112Mb) written. 114MiB 0:00:10 [35.2MiB/s] [ <=>","title":"View progress with pv"},{"location":"dcgc/","text":"\"Keep docker hosts tidy\" - https://github.com/Yelp/docker-custodian Examples Sync script This script will pull the latest version of some containers and delete old containers and images #!/usr/bin/env bash containers=( debian:latest homeassistant/home-assistant:latest linuxserver/unifi:latest ubuntu:xenial yelp/docker-custodian:latest ) for c in \"${containers[@]}\" ; do docker pull \"$c\" keepers+=( \"--exclude-image\" \"$c\" ) done docker run -ti -v /var/run/docker.sock:/var/run/docker.sock yelp/docker-custodian \\ dcgc \"${keepers[@]}\" --dangling-volumes --max-image-age 10w --max-container-age 5w","title":"Docker Custodian"},{"location":"dcgc/#examples","text":"","title":"Examples"},{"location":"dcgc/#sync-script","text":"This script will pull the latest version of some containers and delete old containers and images #!/usr/bin/env bash containers=( debian:latest homeassistant/home-assistant:latest linuxserver/unifi:latest ubuntu:xenial yelp/docker-custodian:latest ) for c in \"${containers[@]}\" ; do docker pull \"$c\" keepers+=( \"--exclude-image\" \"$c\" ) done docker run -ti -v /var/run/docker.sock:/var/run/docker.sock yelp/docker-custodian \\ dcgc \"${keepers[@]}\" --dangling-volumes --max-image-age 10w --max-container-age 5w","title":"Sync script"},{"location":"dcraw/","text":"\"dcraw decodes raw photos, displays metadata, and extracts thumbnails.\" - man dcraw Examples Identify corrupt DNG files find /photos -iname '*.dng' | xargs dcraw -i > /dev/null 2> corrupt-files.txt","title":"dcraw"},{"location":"dcraw/#examples","text":"","title":"Examples"},{"location":"dcraw/#identify-corrupt-dng-files","text":"find /photos -iname '*.dng' | xargs dcraw -i > /dev/null 2> corrupt-files.txt","title":"Identify corrupt DNG files"},{"location":"dd-wrt/","text":"\"DD-WRT is a Linux based alternative OpenSource firmware suitable for a great variety of WLAN routers and embedded systems.\" - https://www.dd-wrt.com Netgear R7000 https://www.myopenrouter.com/downloads/dd-wrt-r7000 http://www.desipro.de/ddwrt/K3-AC-Arm/ Restart script This device with dd-wrt has caused me so much trouble I have to monitor it and reboot it when it fails. Here is a short script to do that. I have this set up in cron to run every 5 minutes. The router will not reboot unless it's been up for 10 minutes. fping -q google.com || { date '+%F %T%z Router is locked up. Restarting it.' | tee -a \"${HOME}/router_reboot.log\" ssh root@192.168.1.1 'set -x ; uptime ; awk \"int(\\$1) < 600 { exit 1 }\" /proc/uptime && reboot ;' | tee -a \"${HOME}/router_reboot.log\" } Version notes 2015-12-24 v3.0-r28598 kongac Cannot edit DHCP reservations. Only can push and pop from the list, but cannot edit the added entries. 2015-12-03 v3.0-r28600M kongac No observed differences from v3.0-r28598 2017-01-18 v3.0-r31160M kongac General instability. Periodic lockups requiring power cycle to fix. Potential weirdness playing with other wifi access points, unable to roam from this to Airport AC as I used to. 2017-03-10 v3.0-r31520M kongac http://www.dd-wrt.com/phpBB2/viewtopic.php?p=1071890 Installed 2017-03-20 Experienced hard lock within 24 hours, had to power cycle to fix. Found posts in dd-wrt forum about other folks experiencing the same issue. 2017-03-26 v3.0-r31575M kongac Installed on 2017-03-21 Appears to have fixed the hard lock-ups 2017-03-26 v3.0-r31780M kongac Installed on 2017-03-31 via ddup --flash-latest . First attempt failed. Rebooted, and second attempt worked. Never had any problems with this 2017-03-31 v3.0-r31800M kongac Installed on 2017-04-01 1 router lockup 3 days after installation 2 router lockups on day 4 2017-04-08 v3.0-r31830M kongac Installed on 2017-04-07 Locked up after 10 days 2017-04-16 v3.0-r31870M kongac Installed on 2017-04-17 Router locked up after 4 days 2017-04-30 v3.0-r31920M kongac Installed on 2017-05-02 Had periodic lockups 2017-05-11 v3.0-r31980M kongac Installed on 2017-05-14 Experienced wifi problem, dhcp problem, and routing problems within 24 hours Experienced the same problems within 12 hours after reboot Uptime peak is 9 days 2017-06-03 v3.0-r32170M kongac Installed on 2017-06-08 Sometimes wireless clients are unable to connect to the network. Sometimes the router becomes inaccessable to clients even though it is still up. 2017-08-02 v3.0-r33000M kongac Installed on 2017-08-05 Quite stable 2017-10-22 v3.0-r33575M kongac Installed on 2017-10-22 Seeing lock-ups and wifi unavailability after 24 hours Seeing more lock-ups and wifi unavailability hours after restart 2017-11-03 v3.0-r33655M kongac Installed on 2017-11-04 WiFi instability observed within hours 2017-11-03 v3.0-r33675M kongac 2017-12-25: Discovered this firmware had been installed 2018-01-03 v3.0-r34320M kongac Installed on 2018-01-16 DD-WRT v3.0-r34320M kongac (c) 2017 NewMedia-NET GmbH Release: 01/03/18 http://www.desipro.de/ddwrt/K3-AC-Arm/TEST/dd-wrt.v24-K3_AC_ARM_STD.bin Frequent lock-ups 2018-02-11 v3.0-r34900M kongac Installed on 2018-02-16 DD-WRT v3.0-r34900M kongac (c) 2018 NewMedia-NET GmbH Release: 02/11/18 Very unstable wifi 2018-02-19 v3.0-r35030M kongac Installed on 2018-02-24 (?) DD-WRT v3.0-r35030M kongac (c) 2018 NewMedia-NET GmbH Release: 02/19/18 This version cannot seem to stay up for more than 12 hours without losing connectivity. 2018-04-04 v3.0-r35550M kongac Installed on 2018-04-04 DD-WRT v3.0-r35550M kongac (03/28/18) Still having stability problems, didn't make it 2h before needing a reboot.","title":"dd-wrt"},{"location":"dd-wrt/#netgear-r7000","text":"https://www.myopenrouter.com/downloads/dd-wrt-r7000 http://www.desipro.de/ddwrt/K3-AC-Arm/","title":"Netgear R7000"},{"location":"dd-wrt/#restart-script","text":"This device with dd-wrt has caused me so much trouble I have to monitor it and reboot it when it fails. Here is a short script to do that. I have this set up in cron to run every 5 minutes. The router will not reboot unless it's been up for 10 minutes. fping -q google.com || { date '+%F %T%z Router is locked up. Restarting it.' | tee -a \"${HOME}/router_reboot.log\" ssh root@192.168.1.1 'set -x ; uptime ; awk \"int(\\$1) < 600 { exit 1 }\" /proc/uptime && reboot ;' | tee -a \"${HOME}/router_reboot.log\" }","title":"Restart script"},{"location":"dd-wrt/#version-notes","text":"","title":"Version notes"},{"location":"dd-wrt/#2015-12-24-v30-r28598-kongac","text":"Cannot edit DHCP reservations. Only can push and pop from the list, but cannot edit the added entries.","title":"2015-12-24 v3.0-r28598 kongac"},{"location":"dd-wrt/#2015-12-03-v30-r28600m-kongac","text":"No observed differences from v3.0-r28598","title":"2015-12-03 v3.0-r28600M kongac"},{"location":"dd-wrt/#2017-01-18-v30-r31160m-kongac","text":"General instability. Periodic lockups requiring power cycle to fix. Potential weirdness playing with other wifi access points, unable to roam from this to Airport AC as I used to.","title":"2017-01-18 v3.0-r31160M kongac"},{"location":"dd-wrt/#2017-03-10-v30-r31520m-kongac","text":"http://www.dd-wrt.com/phpBB2/viewtopic.php?p=1071890 Installed 2017-03-20 Experienced hard lock within 24 hours, had to power cycle to fix. Found posts in dd-wrt forum about other folks experiencing the same issue.","title":"2017-03-10 v3.0-r31520M kongac"},{"location":"dd-wrt/#2017-03-26-v30-r31575m-kongac","text":"Installed on 2017-03-21 Appears to have fixed the hard lock-ups","title":"2017-03-26 v3.0-r31575M kongac"},{"location":"dd-wrt/#2017-03-26-v30-r31780m-kongac","text":"Installed on 2017-03-31 via ddup --flash-latest . First attempt failed. Rebooted, and second attempt worked. Never had any problems with this","title":"2017-03-26 v3.0-r31780M kongac"},{"location":"dd-wrt/#2017-03-31-v30-r31800m-kongac","text":"Installed on 2017-04-01 1 router lockup 3 days after installation 2 router lockups on day 4","title":"2017-03-31 v3.0-r31800M kongac"},{"location":"dd-wrt/#2017-04-08-v30-r31830m-kongac","text":"Installed on 2017-04-07 Locked up after 10 days","title":"2017-04-08 v3.0-r31830M kongac"},{"location":"dd-wrt/#2017-04-16-v30-r31870m-kongac","text":"Installed on 2017-04-17 Router locked up after 4 days","title":"2017-04-16 v3.0-r31870M kongac"},{"location":"dd-wrt/#2017-04-30-v30-r31920m-kongac","text":"Installed on 2017-05-02 Had periodic lockups","title":"2017-04-30 v3.0-r31920M kongac"},{"location":"dd-wrt/#2017-05-11-v30-r31980m-kongac","text":"Installed on 2017-05-14 Experienced wifi problem, dhcp problem, and routing problems within 24 hours Experienced the same problems within 12 hours after reboot Uptime peak is 9 days","title":"2017-05-11 v3.0-r31980M kongac"},{"location":"dd-wrt/#2017-06-03-v30-r32170m-kongac","text":"Installed on 2017-06-08 Sometimes wireless clients are unable to connect to the network. Sometimes the router becomes inaccessable to clients even though it is still up.","title":"2017-06-03 v3.0-r32170M kongac"},{"location":"dd-wrt/#2017-08-02-v30-r33000m-kongac","text":"Installed on 2017-08-05 Quite stable","title":"2017-08-02 v3.0-r33000M kongac"},{"location":"dd-wrt/#2017-10-22-v30-r33575m-kongac","text":"Installed on 2017-10-22 Seeing lock-ups and wifi unavailability after 24 hours Seeing more lock-ups and wifi unavailability hours after restart","title":"2017-10-22 v3.0-r33575M kongac"},{"location":"dd-wrt/#2017-11-03-v30-r33655m-kongac","text":"Installed on 2017-11-04 WiFi instability observed within hours","title":"2017-11-03 v3.0-r33655M kongac"},{"location":"dd-wrt/#2017-11-03-v30-r33675m-kongac","text":"2017-12-25: Discovered this firmware had been installed","title":"2017-11-03 v3.0-r33675M kongac"},{"location":"dd-wrt/#2018-01-03-v30-r34320m-kongac","text":"Installed on 2018-01-16 DD-WRT v3.0-r34320M kongac (c) 2017 NewMedia-NET GmbH Release: 01/03/18 http://www.desipro.de/ddwrt/K3-AC-Arm/TEST/dd-wrt.v24-K3_AC_ARM_STD.bin Frequent lock-ups","title":"2018-01-03 v3.0-r34320M kongac"},{"location":"dd-wrt/#2018-02-11-v30-r34900m-kongac","text":"Installed on 2018-02-16 DD-WRT v3.0-r34900M kongac (c) 2018 NewMedia-NET GmbH Release: 02/11/18 Very unstable wifi","title":"2018-02-11 v3.0-r34900M kongac"},{"location":"dd-wrt/#2018-02-19-v30-r35030m-kongac","text":"Installed on 2018-02-24 (?) DD-WRT v3.0-r35030M kongac (c) 2018 NewMedia-NET GmbH Release: 02/19/18 This version cannot seem to stay up for more than 12 hours without losing connectivity.","title":"2018-02-19 v3.0-r35030M kongac"},{"location":"dd-wrt/#2018-04-04-v30-r35550m-kongac","text":"Installed on 2018-04-04 DD-WRT v3.0-r35550M kongac (03/28/18) Still having stability problems, didn't make it 2h before needing a reboot.","title":"2018-04-04 v3.0-r35550M kongac"},{"location":"dd/","text":"Disk Dump, used for cloning disks, wiping data, copying blocks of data. Tips and Tricks Write random data dd if=/dev/urandom of=/dev/hda Write zeros dd if=/dev/zero of=/dev/hda Wipe a failed disk If you can't use shred or ddrescue , this is a very slow but portable alternative i=0 while true ; do echo \"Writing block $i\" dd if=/dev/zero of=/dev/sda count=1 bs=1 seek=\"$i\" let i=i+1 done Wipe first and last 1G of a hard disk dd bs=4096 if=/dev/zero of=/dev/sdx count=256 seek=$(( $(blockdev --getsz /dev/sdx) - 256))","title":"dd"},{"location":"dd/#tips-and-tricks","text":"","title":"Tips and Tricks"},{"location":"dd/#write-random-data","text":"dd if=/dev/urandom of=/dev/hda","title":"Write random data"},{"location":"dd/#write-zeros","text":"dd if=/dev/zero of=/dev/hda","title":"Write zeros"},{"location":"dd/#wipe-a-failed-disk","text":"If you can't use shred or ddrescue , this is a very slow but portable alternative i=0 while true ; do echo \"Writing block $i\" dd if=/dev/zero of=/dev/sda count=1 bs=1 seek=\"$i\" let i=i+1 done","title":"Wipe a failed disk"},{"location":"dd/#wipe-first-and-last-1g-of-a-hard-disk","text":"dd bs=4096 if=/dev/zero of=/dev/sdx count=256 seek=$(( $(blockdev --getsz /dev/sdx) - 256))","title":"Wipe first and last 1G of a hard disk"},{"location":"ddrescue/","text":"\"GNU ddrescue - Data recovery tool. Copies data from one file or block device to another, trying to rescue the good parts first in case of read errors.\" - man ddrescue There are actually two tools called ddrescue: dd_rescue and gddrescue. gddrescue is the best. Software home page - http://www.gnu.org/software/ddrescue/ Instruction Manual - http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html Examples Attempt to mirror an entire hard disk ddrescue -f -n --min-read-rate=500000 /dev/source_disk /dev/target_disk ~/ddrescue.log This uses the minimum read rate to skip sctors that may be bad on the input device. Wipe a hard disk and log bad sectors sudo ddrescue --force /dev/zero /dev/disk/by-id/ata-foo ~/ddrescue-ata-foo.log You can re-run this exact same command to resume the wipe of a hard disk. Attempt to continue the mirror of a hard disk ddrescue -f -n -A /dev/source_disk /dev/target_disk ~/ddrescue.log Wipe the good sectors of a failing disk This requires a valid rescue log file mapping out the good sectors that were recovered. ddrescue --fill=+ --force /dev/zero /dev/bad_drive ~/bad_drive_wipe.log See Also dcfldd dd pv","title":"ddrescue"},{"location":"ddrescue/#examples","text":"","title":"Examples"},{"location":"ddrescue/#attempt-to-mirror-an-entire-hard-disk","text":"ddrescue -f -n --min-read-rate=500000 /dev/source_disk /dev/target_disk ~/ddrescue.log This uses the minimum read rate to skip sctors that may be bad on the input device.","title":"Attempt to mirror an entire hard disk"},{"location":"ddrescue/#wipe-a-hard-disk-and-log-bad-sectors","text":"sudo ddrescue --force /dev/zero /dev/disk/by-id/ata-foo ~/ddrescue-ata-foo.log You can re-run this exact same command to resume the wipe of a hard disk.","title":"Wipe a hard disk and log bad sectors"},{"location":"ddrescue/#attempt-to-continue-the-mirror-of-a-hard-disk","text":"ddrescue -f -n -A /dev/source_disk /dev/target_disk ~/ddrescue.log","title":"Attempt to continue the mirror of a hard disk"},{"location":"ddrescue/#wipe-the-good-sectors-of-a-failing-disk","text":"This requires a valid rescue log file mapping out the good sectors that were recovered. ddrescue --fill=+ --force /dev/zero /dev/bad_drive ~/bad_drive_wipe.log","title":"Wipe the good sectors of a failing disk"},{"location":"ddrescue/#see-also","text":"dcfldd dd pv","title":"See Also"},{"location":"deb/","text":"Notes and tips about working with the .deb package format Examples Show installed package versions # -V = sort by version (GNU sort only) /usr/bin/dpkg-query -W --showformat '${Package} ${Version} ${Status}\\n' | sort -k2 -V | column -t List files in packages that are available in configured repositories apt-file list package_name Find a file available inside packages that are available in configured repositories apt-file find libmysqlclient.so Show a list of packages that are installed or have left things on the filesystem dpkg --list Show which package a file came from dpkg -S /bin/bash List files in package that is installed dpkg-query -L klibc-utils List files in package that is not installed dpkg -c package.deb List packages available in the repository apt-cache dumpavail Show information about a package apt-cache show coreutils Show reverse dependencies of a package apt-cache rdepends ec2-api-tools Show reverse dependencies of installed package aptitude why openjdk-7-jre-headless Re-install many packages and validate that they were re-installed When apt-get install --reinstall isn't good enough, this is the next option. This should not be done unless you're willing to reload the system if it fails. # Generate a list of packages dpkg -l | grep 'python-' > dpkg-l-python ; # Remove and re-install each individual package one at a time awk '{print $2,$3}' dpkg-l-python | while read -r p v ; do echo \"Working on $p version $v\" ; sudo dpkg --purge --force-depends \"$p\" ; sudo apt-get install \"${p}=${v}\" ; done ; # Validate that all packages are re-installed with the right version awk '{print $2,$3}' dpkg-l-python | while read -r p v ; do dpkg -l \"$p\" | grep \"$v\" || echo \"ERROR: Problem with $p $v\" ; done ; Links https://wiki.debian.org/RPM","title":"deb"},{"location":"deb/#examples","text":"","title":"Examples"},{"location":"deb/#show-installed-package-versions","text":"# -V = sort by version (GNU sort only) /usr/bin/dpkg-query -W --showformat '${Package} ${Version} ${Status}\\n' | sort -k2 -V | column -t","title":"Show installed package versions"},{"location":"deb/#list-files-in-packages-that-are-available-in-configured-repositories","text":"apt-file list package_name","title":"List files in packages that are available in configured repositories"},{"location":"deb/#find-a-file-available-inside-packages-that-are-available-in-configured-repositories","text":"apt-file find libmysqlclient.so","title":"Find a file available inside packages that are available in configured repositories"},{"location":"deb/#show-a-list-of-packages-that-are-installed-or-have-left-things-on-the-filesystem","text":"dpkg --list","title":"Show a list of packages that are installed or have left things on the filesystem"},{"location":"deb/#show-which-package-a-file-came-from","text":"dpkg -S /bin/bash","title":"Show which package a file came from"},{"location":"deb/#list-files-in-package-that-is-installed","text":"dpkg-query -L klibc-utils","title":"List files in package that is installed"},{"location":"deb/#list-files-in-package-that-is-not-installed","text":"dpkg -c package.deb","title":"List files in package that is not installed"},{"location":"deb/#list-packages-available-in-the-repository","text":"apt-cache dumpavail","title":"List packages available in the repository"},{"location":"deb/#show-information-about-a-package","text":"apt-cache show coreutils","title":"Show information about a package"},{"location":"deb/#show-reverse-dependencies-of-a-package","text":"apt-cache rdepends ec2-api-tools","title":"Show reverse dependencies of a package"},{"location":"deb/#show-reverse-dependencies-of-installed-package","text":"aptitude why openjdk-7-jre-headless","title":"Show reverse dependencies of installed package"},{"location":"deb/#re-install-many-packages-and-validate-that-they-were-re-installed","text":"When apt-get install --reinstall isn't good enough, this is the next option. This should not be done unless you're willing to reload the system if it fails. # Generate a list of packages dpkg -l | grep 'python-' > dpkg-l-python ; # Remove and re-install each individual package one at a time awk '{print $2,$3}' dpkg-l-python | while read -r p v ; do echo \"Working on $p version $v\" ; sudo dpkg --purge --force-depends \"$p\" ; sudo apt-get install \"${p}=${v}\" ; done ; # Validate that all packages are re-installed with the right version awk '{print $2,$3}' dpkg-l-python | while read -r p v ; do dpkg -l \"$p\" | grep \"$v\" || echo \"ERROR: Problem with $p $v\" ; done ;","title":"Re-install many packages and validate that they were re-installed"},{"location":"deb/#links","text":"https://wiki.debian.org/RPM","title":"Links"},{"location":"defaults/","text":"defaults allows users to read, write, and delete Mac OS X user defaults from a command-line shell. Examples Set some boolean values defaults write NSGlobalDomain NSAutomaticQuoteSubstitutionEnabled -bool false defaults write NSGlobalDomain NSAutomaticDashSubstitutionEnabled -bool false defaults write com.apple.TextEdit SmartQuotes -bool false defaults write com.apple.TextEdit SmartDashes -bool false Add a value (an array) to a dict FILENAME=\"${HOME}/Library/Preferences/com.googlecode.iterm2.plist\" defaults write \"${FILENAME}\" GlobalKeyMap -dict-add 0xf703-0x280000 '{ Action = 10; Text = f; }' defaults write \"${FILENAME}\" GlobalKeyMap -dict-add 0xf702-0x280000 '{ Action = 10; Text = b; }'","title":"defaults"},{"location":"defaults/#examples","text":"","title":"Examples"},{"location":"defaults/#set-some-boolean-values","text":"defaults write NSGlobalDomain NSAutomaticQuoteSubstitutionEnabled -bool false defaults write NSGlobalDomain NSAutomaticDashSubstitutionEnabled -bool false defaults write com.apple.TextEdit SmartQuotes -bool false defaults write com.apple.TextEdit SmartDashes -bool false","title":"Set some boolean values"},{"location":"defaults/#add-a-value-an-array-to-a-dict","text":"FILENAME=\"${HOME}/Library/Preferences/com.googlecode.iterm2.plist\" defaults write \"${FILENAME}\" GlobalKeyMap -dict-add 0xf703-0x280000 '{ Action = 10; Text = f; }' defaults write \"${FILENAME}\" GlobalKeyMap -dict-add 0xf702-0x280000 '{ Action = 10; Text = b; }'","title":"Add a value (an array) to a dict"},{"location":"devops/","text":"DevOps lifecycle Plan - Jira, Github tickets, Gitlab Milestones Code - Git, Eclipse, pycharm Build - Ant, Maven, Gradle Test - Selenium, JUnit Release - Gitlab CI, Jenkins Deploy - Puppet, Chef, Ansible, Saltstack Operate - Linux, Chrome, iOS Monitor - Sensu, Splunk, Nagios, Kibana Links The 9 Circles of Deployment Hell Raft: Understandable distributed consensus - Good visualization of the Raft algorithm. https://aws.amazon.com/devops/what-is-devops/ https://www.reddit.com/r/devops/comments/aqc8cj/interested_in_getting_into_devops_start_here/ https://landing.google.com/sre/books/ https://web.devopstopologies.com/ Team topologies, not technology stack topologies.","title":"devops"},{"location":"devops/#devops-lifecycle","text":"Plan - Jira, Github tickets, Gitlab Milestones Code - Git, Eclipse, pycharm Build - Ant, Maven, Gradle Test - Selenium, JUnit Release - Gitlab CI, Jenkins Deploy - Puppet, Chef, Ansible, Saltstack Operate - Linux, Chrome, iOS Monitor - Sensu, Splunk, Nagios, Kibana","title":"DevOps lifecycle"},{"location":"devops/#links","text":"The 9 Circles of Deployment Hell Raft: Understandable distributed consensus - Good visualization of the Raft algorithm. https://aws.amazon.com/devops/what-is-devops/ https://www.reddit.com/r/devops/comments/aqc8cj/interested_in_getting_into_devops_start_here/ https://landing.google.com/sre/books/ https://web.devopstopologies.com/ Team topologies, not technology stack topologies.","title":"Links"},{"location":"dhcp/","text":"Dynamic Host Configuration Protocol isc dhcpd https://www.isc.org/downloads/dhcp/ Test configuration file dhcpd3 -t Test lease file dhcpd3 -T Handshake Process Apr 21 15:33:00 ops1prod dhcpd: DHCPDISCOVER from 08:9e:01:8b:18:94 via eth0 Apr 21 15:33:01 ops1prod dhcpd: DHCPOFFER on 10.1.14.127 to 08:9e:01:8b:18:94 via eth0 Apr 21 15:33:01 ops1prod dhcpd: DHCPREQUEST for 10.1.225.43 from 00:1e:0b:bc:8a:c4 via eth1 Apr 21 15:33:01 ops1prod dhcpd: DHCPACK on 10.1.225.43 to 00:1e:0b:bc:8a:c4 via eth1","title":"DHCP"},{"location":"dhcp/#isc-dhcpd","text":"https://www.isc.org/downloads/dhcp/","title":"isc dhcpd"},{"location":"dhcp/#test-configuration-file","text":"dhcpd3 -t","title":"Test configuration file"},{"location":"dhcp/#test-lease-file","text":"dhcpd3 -T","title":"Test lease file"},{"location":"dhcp/#handshake-process","text":"Apr 21 15:33:00 ops1prod dhcpd: DHCPDISCOVER from 08:9e:01:8b:18:94 via eth0 Apr 21 15:33:01 ops1prod dhcpd: DHCPOFFER on 10.1.14.127 to 08:9e:01:8b:18:94 via eth0 Apr 21 15:33:01 ops1prod dhcpd: DHCPREQUEST for 10.1.225.43 from 00:1e:0b:bc:8a:c4 via eth1 Apr 21 15:33:01 ops1prod dhcpd: DHCPACK on 10.1.225.43 to 00:1e:0b:bc:8a:c4 via eth1","title":"Handshake Process"},{"location":"dig/","text":"dig (domain information groper) is a flexible tool for interrogating DNS name servers. The syntax for this tool is a bit cryptic and is not standard. Usage Simple usage dig yelp.com Show only the Answer section dig +noall +answer \"yelp.com\" Trace a query from the root servers This is the most accurate way to get a DNS record as it will appear to anybody else on the internet who has not queried it before, and will show you all the DNS steps involved in the resolution. dig +trace yelp.com If this doesn't give you a trace you must specify an alternate DNS server dig @8.8.8.8 +trace renovo.auto","title":"dig"},{"location":"dig/#usage","text":"","title":"Usage"},{"location":"dig/#simple-usage","text":"dig yelp.com","title":"Simple usage"},{"location":"dig/#show-only-the-answer-section","text":"dig +noall +answer \"yelp.com\"","title":"Show only the Answer section"},{"location":"dig/#trace-a-query-from-the-root-servers","text":"This is the most accurate way to get a DNS record as it will appear to anybody else on the internet who has not queried it before, and will show you all the DNS steps involved in the resolution. dig +trace yelp.com If this doesn't give you a trace you must specify an alternate DNS server dig @8.8.8.8 +trace renovo.auto","title":"Trace a query from the root servers"},{"location":"dmidecode/","text":"tool for listing hardware information and decoding it into human readable form. This tool decodes the DMI information. Examples Show system serial number sudo dmidecode -t system | grep Serial sudo dmidecode -s system-serial-number Show memory info including max installable sudo dmidecode -t memory Show bios version You may need to grep for a different string, but even then it doesn't always show the info because not all machines support this. sudo dmidecode -t bios | grep -i revision Shower power supply information This doesn't always work. Some power supplies are not supported. dmidecode -t 39 See Also Show all keywords and their values: # -s without a keyword lists all keywords # -s with a keyword shows only the value of that keyword dmidecode -s |& grep '^ ' | while read -r X ; do echo $X: $(sudo dmidecode -s $X) ; done ; lshw - list hardware","title":"dmidecode"},{"location":"dmidecode/#examples","text":"","title":"Examples"},{"location":"dmidecode/#show-system-serial-number","text":"sudo dmidecode -t system | grep Serial sudo dmidecode -s system-serial-number","title":"Show system serial number"},{"location":"dmidecode/#show-memory-info-including-max-installable","text":"sudo dmidecode -t memory","title":"Show memory info including max installable"},{"location":"dmidecode/#show-bios-version","text":"You may need to grep for a different string, but even then it doesn't always show the info because not all machines support this. sudo dmidecode -t bios | grep -i revision","title":"Show bios version"},{"location":"dmidecode/#shower-power-supply-information","text":"This doesn't always work. Some power supplies are not supported. dmidecode -t 39","title":"Shower power supply information"},{"location":"dmidecode/#see-also","text":"","title":"See Also"},{"location":"dmidecode/#show-all-keywords-and-their-values","text":"# -s without a keyword lists all keywords # -s with a keyword shows only the value of that keyword dmidecode -s |& grep '^ ' | while read -r X ; do echo $X: $(sudo dmidecode -s $X) ; done ; lshw - list hardware","title":"Show all keywords and their values:"},{"location":"dns/","text":"Domain Name System \"The Domain Name System (DNS) is a hierarchical decentralized naming system for computers, services, or any resource connected to the Internet or a private network.\" https://en.wikipedia.org/wiki/List_of_DNS_record_types https://en.wikipedia.org/wiki/Category:Application_layer_protocols https://miek.nl/2009/july/31/dns-classes/","title":"dns"},{"location":"docker/","text":"\"An open source project to pack, ship and run any application as a lightweight container.\" - https://www.docker.com/ Examples Show help on the run command docker help run Show the history of an image, and count its layers docker history ubuntu:bionic | nl -ba -v0 Run a docker image in an interactive shell docker run -i -t node bash -i, --interactive -t, --tty https://docs.docker.com/reference/commandline/cli/#run Get a bash terminal on a running docker container docker exec -i -t container_name bash Run a docker image and assign it a hostname, and a docker name docker run --hostname=somehost1 --name=\"host1\" -ti centos:centos6 bash The hostname shows up to the OS. The docker name can be used to interact with the container: docker ps host1 Show a complete vertically oriented list of docker processes docker ps has no --json flag, but you can work around that with golang style formatting. docker ps --no-trunc --format='{{ . | json }}' | jq -S . This trick also works with docker images , which also lacks a --json arg. Show a table of docker containers sorted by space used in the container (not by the image) $ docker ps --format=\"{{.Size}}\\t{{.ID}}\\t{{.Image}}\\t{{.Names}}\" | sort -h -k1 | column -t 0B (virtual 101MB) 2f7ba92f1e66 wan-connection-logger wan-connection-logger 0B (virtual 413MB) 21d474032755 gitlab/gitlab-runner gitlab-runner 2B (virtual 392MB) c15b2ad88901 mariadb:10.4-bionic mariadb 312kB (virtual 710MB) ccee541f32c2 jacobalberty/unifi unifi 1.45MB (virtual 2.3GB) a9a60f4c6efc homeassistant/home-assistant home-assistant 239MB (virtual 412MB) 5d9f9cc3b46a plexinc/pms-docker:plexpass plex Run a container with a tcp port map This maps port 18022 of the host to 22 of the guest. docker run -ti -p 18022:22 centos bash Run a container with a shared directory We are specifying :ro to make this a read-only mount. Default is rw. docker run -d -v \"$HOME/www/:/var/www/html/:ro\" php:5.4.35-apache Show configuration parameters for a container This shows more things that you can configure, like DNS, DNS search, etc.. docker inspect host1 Show what has changed since a container was started docker diff <container-id> https://docs.docker.com/reference/commandline/cli/#diff View the terminal scrollback of a a container docker logs <container-id> List all containers, including ones that have been stopped This allows you to restart previous instances of a container. docker ps -a https://docs.docker.com/reference/commandline/cli/#ps Start a named container By default containers don't restart when your system restarts, so you have to start them manually. docker start ttrss Stop a named container docker stop ttrss Update the restart policy on a running container docker update --restart=unless-stopped 3a898cb672ad Get a shell on a running container docker exec -ti ttrss bash Delete old containers https://docs.docker.com/reference/commandline/cli/#rm Remove -r from xargs on non-GNU systems. docker ps -a --format=\"{{.ID}} {{.Status}}\" | awk '$2 == \"Exited\" && $5 ~ /(days|weeks|months)/ {print $1}' | xargs -r docker rm A more systematic approach is to use Docker Custodian . Delete old images This is safe to run as long as valuable containers are running, as it won't delete any images that are attached to running containers. docker rmi $(docker images | grep '^<none>' | awk '{print $3}') https://docs.docker.com/reference/commandline/cli/#rmi A more systematic approach is to use Docker Custodian . Show processes running inside all docker containers On hosts without cgroup integration, run: pgrep docker | xargs -n1 pstree See Also https://www.docker.io - Main page http://dockerfile.github.io - Trusted builds of FOSS software https://registry.hub.docker.com - Public docker images https://docs.docker.com/reference/builder/ - How to build Dockerfiles https://cloud.google.com/solutions/best-practices-for-building-containers","title":"docker"},{"location":"docker/#examples","text":"","title":"Examples"},{"location":"docker/#show-help-on-the-run-command","text":"docker help run","title":"Show help on the run command"},{"location":"docker/#show-the-history-of-an-image-and-count-its-layers","text":"docker history ubuntu:bionic | nl -ba -v0","title":"Show the history of an image, and count its layers"},{"location":"docker/#run-a-docker-image-in-an-interactive-shell","text":"docker run -i -t node bash -i, --interactive -t, --tty https://docs.docker.com/reference/commandline/cli/#run","title":"Run a docker image in an interactive shell"},{"location":"docker/#get-a-bash-terminal-on-a-running-docker-container","text":"docker exec -i -t container_name bash","title":"Get a bash terminal on a running docker container"},{"location":"docker/#run-a-docker-image-and-assign-it-a-hostname-and-a-docker-name","text":"docker run --hostname=somehost1 --name=\"host1\" -ti centos:centos6 bash The hostname shows up to the OS. The docker name can be used to interact with the container: docker ps host1","title":"Run a docker image and assign it a hostname, and a docker name"},{"location":"docker/#show-a-complete-vertically-oriented-list-of-docker-processes","text":"docker ps has no --json flag, but you can work around that with golang style formatting. docker ps --no-trunc --format='{{ . | json }}' | jq -S . This trick also works with docker images , which also lacks a --json arg.","title":"Show a complete vertically oriented list of docker processes"},{"location":"docker/#show-a-table-of-docker-containers-sorted-by-space-used-in-the-container-not-by-the-image","text":"$ docker ps --format=\"{{.Size}}\\t{{.ID}}\\t{{.Image}}\\t{{.Names}}\" | sort -h -k1 | column -t 0B (virtual 101MB) 2f7ba92f1e66 wan-connection-logger wan-connection-logger 0B (virtual 413MB) 21d474032755 gitlab/gitlab-runner gitlab-runner 2B (virtual 392MB) c15b2ad88901 mariadb:10.4-bionic mariadb 312kB (virtual 710MB) ccee541f32c2 jacobalberty/unifi unifi 1.45MB (virtual 2.3GB) a9a60f4c6efc homeassistant/home-assistant home-assistant 239MB (virtual 412MB) 5d9f9cc3b46a plexinc/pms-docker:plexpass plex","title":"Show a table of docker containers sorted by space used in the container (not by the image)"},{"location":"docker/#run-a-container-with-a-tcp-port-map","text":"This maps port 18022 of the host to 22 of the guest. docker run -ti -p 18022:22 centos bash","title":"Run a container with a tcp port map"},{"location":"docker/#run-a-container-with-a-shared-directory","text":"We are specifying :ro to make this a read-only mount. Default is rw. docker run -d -v \"$HOME/www/:/var/www/html/:ro\" php:5.4.35-apache","title":"Run a container with a shared directory"},{"location":"docker/#show-configuration-parameters-for-a-container","text":"This shows more things that you can configure, like DNS, DNS search, etc.. docker inspect host1","title":"Show configuration parameters for a container"},{"location":"docker/#show-what-has-changed-since-a-container-was-started","text":"docker diff <container-id> https://docs.docker.com/reference/commandline/cli/#diff","title":"Show what has changed since a container was started"},{"location":"docker/#view-the-terminal-scrollback-of-a-a-container","text":"docker logs <container-id>","title":"View the terminal scrollback of a a container"},{"location":"docker/#list-all-containers-including-ones-that-have-been-stopped","text":"This allows you to restart previous instances of a container. docker ps -a https://docs.docker.com/reference/commandline/cli/#ps","title":"List all containers, including ones that have been stopped"},{"location":"docker/#start-a-named-container","text":"By default containers don't restart when your system restarts, so you have to start them manually. docker start ttrss","title":"Start a named container"},{"location":"docker/#stop-a-named-container","text":"docker stop ttrss","title":"Stop a named container"},{"location":"docker/#update-the-restart-policy-on-a-running-container","text":"docker update --restart=unless-stopped 3a898cb672ad","title":"Update the restart policy on a running container"},{"location":"docker/#get-a-shell-on-a-running-container","text":"docker exec -ti ttrss bash","title":"Get a shell on a running container"},{"location":"docker/#delete-old-containers","text":"https://docs.docker.com/reference/commandline/cli/#rm Remove -r from xargs on non-GNU systems. docker ps -a --format=\"{{.ID}} {{.Status}}\" | awk '$2 == \"Exited\" && $5 ~ /(days|weeks|months)/ {print $1}' | xargs -r docker rm A more systematic approach is to use Docker Custodian .","title":"Delete old containers"},{"location":"docker/#delete-old-images","text":"This is safe to run as long as valuable containers are running, as it won't delete any images that are attached to running containers. docker rmi $(docker images | grep '^<none>' | awk '{print $3}') https://docs.docker.com/reference/commandline/cli/#rmi A more systematic approach is to use Docker Custodian .","title":"Delete old images"},{"location":"docker/#show-processes-running-inside-all-docker-containers","text":"On hosts without cgroup integration, run: pgrep docker | xargs -n1 pstree","title":"Show processes running inside all docker containers"},{"location":"docker/#see-also","text":"https://www.docker.io - Main page http://dockerfile.github.io - Trusted builds of FOSS software https://registry.hub.docker.com - Public docker images https://docs.docker.com/reference/builder/ - How to build Dockerfiles https://cloud.google.com/solutions/best-practices-for-building-containers","title":"See Also"},{"location":"dsrc/","text":"\"Dedicated Short Range Communications is a two-way short-to-medium range wireless communications capability that permits very high data transmission critical in communications-based active safety applications\" - https://www.its.dot.gov/factsheets/dsrc_factsheet.htm \"Dedicated short-range communications are one-way or two-way short-range to medium-range wireless communication channels specifically designed for automotive use and a corresponding set of protocols and standards.\" - https://en.wikipedia.org/wiki/Dedicated_short-range_communications Links https://www.its.dot.gov/factsheets/dsrc_factsheet.htm https://en.wikipedia.org/wiki/Dedicated_short-range_communications https://www.fcc.gov/wireless/bureau-divisions/mobility-division/dedicated-short-range-communications-dsrc-service","title":"dsrc"},{"location":"dsrc/#links","text":"https://www.its.dot.gov/factsheets/dsrc_factsheet.htm https://en.wikipedia.org/wiki/Dedicated_short-range_communications https://www.fcc.gov/wireless/bureau-divisions/mobility-division/dedicated-short-range-communications-dsrc-service","title":"Links"},{"location":"dtrace/","text":"\"dynamic tracing compiler and tracing utility\" - man dtrace Links http://www.brendangregg.com/DTrace/dtrace_oneliners.txt","title":"dtrace"},{"location":"dtrace/#links","text":"http://www.brendangregg.com/DTrace/dtrace_oneliners.txt","title":"Links"},{"location":"du/","text":"\"estimate file space usage\" - man du Examples Summarize low level directory uage When a partition fills up this is a good place to begin looking. Some flags may not be available, such as sort -h # -x --one-file-system # -d 3 --max-depth=3 # -h --human-readable sudo du -x -d 3 -h / | sort -h","title":"du"},{"location":"du/#examples","text":"","title":"Examples"},{"location":"du/#summarize-low-level-directory-uage","text":"When a partition fills up this is a good place to begin looking. Some flags may not be available, such as sort -h # -x --one-file-system # -d 3 --max-depth=3 # -h --human-readable sudo du -x -d 3 -h / | sort -h","title":"Summarize low level directory uage"},{"location":"duplicity/","text":"Encrypted bandwidth-efficient backup using the rsync algorithm http://duplicity.nongnu.org/","title":"duplicity"},{"location":"education/","text":"https://www.coursera.org/ https://www.edx.org/ https://www.udemy.com/","title":"education"},{"location":"eleduino/","text":"\"Cool and High Quality raspberry pi accessories at Wholesale Price from China\" - http://www.eleduino.com Eleduino SpotPear touchscreen Taken from https://github.com/notro/rpi-firmware/issues/6#issuecomment-63180647 , this is tested to work on Raspbian 7 (wheezy) and 8 (jessie). update the firmware to support FBTFT sudo apt-get install -y rpi-update sudo REPO_URI=https://github.com/notro/rpi-firmware BRANCH=builtin rpi-update sudo reboot Modify boot args to enable the device sed -i \"s/$/ \\ fbtft_device.custom \\ fbtft_device.name=fb_ili9340 \\ fbtft_device.gpios=dc:22,reset:27 \\ fbtft_device.bgr=1 \\ fbtft_device.speed=48000000/\" /boot/cmdline.txt Enable console on boot sed -i \"s/$/ \\ fbcon=map:10 \\ fbcon=font:ProFont6x11 \\ logo.nologo/\" /boot/cmdline.txt Rotation etc.. sed -i \"s/$/ \\ dma.dmachans=0x7f35 \\ console=tty1 \\ consoleblank=0 \\ fbtft_device.fps=50 \\ fbtft_device.rotate=270/\" /boot/cmdline.txt Sort and unique /boot/cmdline.txt cat /boot/cmdline.txt | \\ tee /root/cmdline.txt-$(date +%s) | \\ tr \" \" \"\\n\" | \\ sort -u | \\ tr \"\\n\" \" \" > /boot/cmdline.txt","title":"Eleduino"},{"location":"eleduino/#eleduino-spotpear-touchscreen","text":"Taken from https://github.com/notro/rpi-firmware/issues/6#issuecomment-63180647 , this is tested to work on Raspbian 7 (wheezy) and 8 (jessie).","title":"Eleduino SpotPear touchscreen"},{"location":"eleduino/#update-the-firmware-to-support-fbtft","text":"sudo apt-get install -y rpi-update sudo REPO_URI=https://github.com/notro/rpi-firmware BRANCH=builtin rpi-update sudo reboot","title":"update the firmware to support FBTFT"},{"location":"eleduino/#modify-boot-args-to-enable-the-device","text":"sed -i \"s/$/ \\ fbtft_device.custom \\ fbtft_device.name=fb_ili9340 \\ fbtft_device.gpios=dc:22,reset:27 \\ fbtft_device.bgr=1 \\ fbtft_device.speed=48000000/\" /boot/cmdline.txt","title":"Modify boot args to enable the device"},{"location":"eleduino/#enable-console-on-boot","text":"sed -i \"s/$/ \\ fbcon=map:10 \\ fbcon=font:ProFont6x11 \\ logo.nologo/\" /boot/cmdline.txt","title":"Enable console on boot"},{"location":"eleduino/#rotation-etc","text":"sed -i \"s/$/ \\ dma.dmachans=0x7f35 \\ console=tty1 \\ consoleblank=0 \\ fbtft_device.fps=50 \\ fbtft_device.rotate=270/\" /boot/cmdline.txt","title":"Rotation etc.."},{"location":"eleduino/#sort-and-unique-bootcmdlinetxt","text":"cat /boot/cmdline.txt | \\ tee /root/cmdline.txt-$(date +%s) | \\ tr \" \" \"\\n\" | \\ sort -u | \\ tr \"\\n\" \" \" > /boot/cmdline.txt","title":"Sort and unique /boot/cmdline.txt"},{"location":"etcd/","text":"\"etcd is a distributed key value store that provides a reliable way to store data across a cluster of machines. It\u2019s open-source and available on GitHub. etcd gracefully handles leader elections during network partitions and will tolerate machine failure, including the leader.\" - https://coreos.com/etcd/ https://coreos.com/etcd/docs/latest/demo.html","title":"etcd"},{"location":"ethtool/","text":"\"ethtool - query or control network driver and hardware settings\" - man ethtool Examples Force ethernet adapter to re-negotiate its speed ethtool -r Show interface error count by type $ sudo ethtool -S ens5f0 | grep -i error rx_errors: 13551 tx_errors: 0 rx_length_errors: 0 rx_crc_errors: 13551 fcoe_last_error: 0 port.tx_errors: 0 port.rx_crc_errors: 13551 port.rx_length_errors: 0","title":"ethtool"},{"location":"ethtool/#examples","text":"","title":"Examples"},{"location":"ethtool/#force-ethernet-adapter-to-re-negotiate-its-speed","text":"ethtool -r","title":"Force ethernet adapter to re-negotiate its speed"},{"location":"ethtool/#show-interface-error-count-by-type","text":"$ sudo ethtool -S ens5f0 | grep -i error rx_errors: 13551 tx_errors: 0 rx_length_errors: 0 rx_crc_errors: 13551 fcoe_last_error: 0 port.tx_errors: 0 port.rx_crc_errors: 13551 port.rx_length_errors: 0","title":"Show interface error count by type"},{"location":"exiftool/","text":"CLI Tool to read and write image metadata for many kinds of images. http://www.sno.phy.queensu.ca/~phil/exiftool/ Tricks Strip all tags exiftool -all= filename.jpg Show tags in a format that you can use to rewrite them exiftool -S $filename For example $ exiftool -S \"$filename\" | grep Daniel Artist: Daniel Austin Hoherd Copyright: \u00a9Daniel Austin Hoherd Creator: Daniel Austin Hoherd Rights: \u00a9Daniel Austin Hoherd $ exiftool -Rights='All rights reserved' \"$filename\" 1 image files updated $ exiftool -Rights \"$filename\" Rights : All rights reserved Expanded basic usage This prints out a lot more information than normal usage, and indicates what type of metadata it is. exiftool -a -u -G:1:2 \"$filename\" Here is an example of each unique column 1 in a file $ exiftool -a -u -G:1:2 \"$filename\" | sort -u -k1,1 [Adobe:Image] DCT Encode Version : 100 [Composite:Camera] Scale Factor To 35 mm Equivalent: 7.0 [Composite:Image] Aperture : 1.8 [Composite:Location] GPS Latitude : 37 deg 15' 53.04\" N [Composite:Time] Date/Time Created : 2019:01:08 15:59:06 [ExifIFD:Camera] Exposure Program : Program AE [ExifIFD:Image] Exposure Time : 1/120 [ExifIFD:Time] Date/Time Original : 2019:01:08 15:59:06 [ExifTool:ExifTool] ExifTool Version Number : 11.11 [File:Image] File Type : JPEG [GPS:Location] GPS Version ID : 2.2.0.0 [GPS:Time] GPS Time Stamp : 23:59:06 [ICC-header:Image] Profile CMM Type : Linotronic [ICC-header:Time] Profile Date Time : 1998:02:09 06:49:00 [ICC-meas:Image] Measurement Observer : CIE 1931 [ICC-view:Image] Viewing Cond Illuminant : 19.6445 20.3718 16.8089 [ICC_Profile:Camera] Device Mfg Desc : IEC http://www.iec.ch [ICC_Profile:Image] Profile Copyright : Copyright (c) 1998 Hewlett-Packard Company [IFD0:Author] Artist : Daniel Austin Hoherd [IFD0:Camera] Make : Apple [IFD0:Image] X Resolution : 240 [IFD0:Time] Modify Date : 2019:01:09 13:50:29 [IFD1:Image] Compression : JPEG (old-style) [IFD1:Preview] Thumbnail Image : (Binary data 12008 bytes, use -b option to extract) [IPTC:Author] By-line : Daniel Austin Hoherd [IPTC:Other] Coded Character Set : UTF8 [IPTC:Time] Date Created : 2019:01:08 [Photoshop:Author] Copyright Flag : True [Photoshop:Image] X Resolution : 240 [Photoshop:Preview] Photoshop Thumbnail : (Binary data 12008 bytes, use -b option to extract) [System:Image] File Name : 2019-01-08-15-59-06-46628465322_d1657e4c95_o.jpg [System:Time] File Modification Date/Time : 2019:01:22 09:00:22-08:00 [XMP-aux:Camera] Distortion Correction Already Applied: True [XMP-crs:Image] Already Applied : True [XMP-dc:Author] Creator : Daniel Austin Hoherd [XMP-dc:Image] Format : image/jpeg [XMP-photoshop:Image] Headline : ljwZuD [XMP-photoshop:Time] Date Created : 2019:01:08 15:59:06.448 [XMP-x:Document] XMP Toolkit : Image::ExifTool 11.11 [XMP-xmp:Image] Creator Tool : Adobe Photoshop Lightroom 6.14 (Macintosh) [XMP-xmp:Time] Create Date : 2019:01:08 15:59:06.448 [XMP-xmpMM:Other] Derived From Document ID : 9880573B7AACBFC189C795E182E8A05D [XMP-xmpMM:Time] History When : 2019:01:09 13:50:29-08:00 [XMP-xmpRights:Author] Marked : True Add missing lens data on Rokinon 85mm Rokinon 85mm is a mechanical lens with no electronics, so no data about photos taken with it are stored in the image. This adds some stock metadata describing characteristics of the lens that are always true, which helps these photos sorting accurately, etc.. exiftool \\ -overwrite_original \\ -LensModel='Rokinon 85mm f/1.4' \\ -FocalLength='85' \\ -LongFocal='85' \\ -ShortFocal='85' \\ filename.dng Correct EXIF time, for instance to sync with GPS time The following example increases all metadata dates by 1 minute and 56 seconds. # exiftool -AllDates-='Y:M:D H:M:S' exiftool -AllDates+='0:0:0 0:1:56' Set all dates to something obviously wrong This is useful when scanning or photographing film or prints where you do not want the current date associated with the image. exiftool -alldates='1900:01:01 01:01:01' * Rename GPX files based on the capture time You will end up with a filename like 2013-09-30-23-35-40.gpx based off of the first trkpt timestamp. exiftool -d '%Y%m%d-%H-%M-%S' '-FileName<${GpxTrkTrksegTrkptTime;tr/ /-/;tr/:/-/;tr(/Z/)()d;}%-c.gpx' *.gpx Rename files to their original date and time using a lower case file extension # %le = lowercase extension # %-c = unique filenames when the timestamp is exactly the same. EG: filename-1.jpg exiftool \"-FileName<CreateDate\" -d \"%Y%m%d-%H-%M-%S%%-c.%%le\" *.jpg Rename files using a combination of tags Using the name of the tag as output by exiftool -S , you can create complicated filenames by combining tags: exiftool -d '%Y%m%d-%H-%M-%S' '-FileName<${CreateDate;}_${Headline;}%-c.%e' Set file modify time to image capture time Useful when you want to sort in your file browser by modification time and get a chronological order of files. exiftool \"-FileModifyDate<DateTimeOriginal\" *.jpg Generate a table of Filename, Camera Model and File Size in bytes, sorted by bytes $ find /src_dir/ -iname '*.dng' | xargs exiftool -p '$filename,$Model,$FileSize#' 2>/dev/null | sort -t, -k3 -n | column -s, -t 2012-01-26-23-19-54-6795223065_2e771d1012_o.jpg iPhone 4S 1242739 2013-02-03-10-01-56-8441346635_df4404a1f6_o.jpg NIKON D5200 1646481 2012-01-22-15-16-38-6746574603_d52311264f_o.jpg Canon EOS REBEL T3i 1671734 2011-01-22-23-44-31-6271225963_f9b95b2d7a_o.jpg NIKON D3S 1773081 2010-01-27-13-07-00-4313649499_835a6649c2_o.jpg NIKON D300 1829578 2016-02-03-07-26-32-24522158414_4aaf116d2a_o.jpg iPhone 6 2319061 2018-10-24-13-39-09-44676649345_1de0f581cd_o.jpg iPhone XS Max 2971254 2015-02-02-19-17-09-24587486051_3032823e4e_o.jpg NIKON D800 3309696 2014-01-27-13-52-41-12951707465_79a8dd3827_o.jpg iPhone 5 3401479 2017-01-22-18-33-28-31693592473_40478df088_o.jpg ILCE-7 4230661 2018-12-23-22-33-40-45536007225_8fdd50691a_o.jpg NIKON D850 4924617 2017-02-06-08-04-18-44658317900_98e04997fb_o.jpg iPhone 6s 8712631 2018-12-28-16-56-42-39713091073_c57ec1a8a8_o.jpg Canon EOS 5D Mark II 8741601 2019-01-08-16-11-49-39716361093_479e6a2323_o.jpg iPhone 8 Plus 12041600 Generate rsync commands for files matching a string Useful for reviewing commands before running them, the following example generates a command for every file, then uses awk to do a numeric comparison on the last field to sort out images under a certain ImageHeight. These rsync commands can be pasted into a terminal to run. (Generating a list of files for use with rsync --files-from would be a better option for this specific use case, but this illustration could be adapted for commands that do not have such an option.) $ exiftool -d \"%s\" -p 'rsync -aP $filename otherhost:~/Pictures/ # $ImageHeight' * 2>/dev/null | awk '$NF >= 2800 {print}' rsync -aP 2017-02-06-08-04-18-44658317900_98e04997fb_o.jpg otherhost:~/Pictures/ # 2869 rsync -aP 2018-02-06-09-50-04-31514483967_a422a3e3aa_o.jpg otherhost:~/Pictures/ # 2880 rsync -aP 2018-02-06-15-04-43-45541501845_8dbdc3b208_o.jpg otherhost:~/Pictures/ # 2880 rsync -aP 2018-02-06-15-05-43-31514485997_e2551fdbbc_o.jpg otherhost:~/Pictures/ # 2880 rsync -aP 2018-12-19-10-53-27-45663859984_0f93ac24ec_o.jpg otherhost:~/Pictures/ # 2880 Rename files to their ShutterCount Filenames will not be changed if ShutterCount field is not populated. exiftool -P '-filename<${ShutterCount;}.%e' *.dng Rename files based on a set of possible names Exiftool will use the last parameter where all variables are present. exiftool -P -d '%F-%H-%M-%S' \\ '-filename<${DateTimeOriginal} - ${Make;}.%e' \\ '-filename<${CreateDate} - ${Make;}.%e' \\ '-filename<${DateTimeOriginal} - ${Make;} - ${Model;}.%e' \\ '-filename<${CreateDate} - ${Make;} - ${Model;}.%e' \\ '-filename<${DateTimeOriginal} - ${Make;} - ${Model;} - ${ShutterCount}.%e' \\ '-filename<${CreateDate} - ${Make;} - ${Model;} - ${ShutterCount}.%e' \\ *.dng Use TestName tag target to test what files would be renamed to This block builds an array of possible tags to use as a filename, creates an exiftool argument string from that array, then tests what files would be named to. This is useful when dealing with files from various sources that don't all use the same tag to store the original media creation time. By using TestName instead of FileName as the target, we observe what would occur, essentially a dry-run, instead of actually renaming the files. There is a funky behavior of %-c when you operate on a file that should ideally not be renamed. Exiftool will toggle back and forth each run appending and removing -1 . This assumes GNU xargs for the -r flag. #!/usr/bin/env bash set -x # The last valid variable from this list is used as the filename source create_date_sources=( TrackCreateDate RIFF:DateTimeOriginal MediaCreateDate FileModifyDate DateTimeOriginal CreateDate ) for opt in \"${create_date_sources[@]}\" ; do args+=( \"-TestName<${opt}\" ) ; done ; args+=( '-d' './%Y/%m/%Y%m%d-%H-%M-%S%%-c.%%le' ) find . -maxdepth 1 -type f ! -name '*.sh' -print0 | xargs -0 -r exiftool \"${args[@]}\" Rename music files in a directory There is a big gotcha here, which is that slashes will create directories where they appear, which can cause serious problems. The ${Tag;s/\\//_/} syntax replaces / with _ , but there may be other characters that can cause unexpected results. This is a great place to use -TestName to inspect what would change before using -FileName to make the changes. exiftool \\ '-FileName<${Artist;s/\\//_/} - ${Title;s/\\//_/}.%e' \\ '-FileName<${Artist;s/\\//_/} - ${Album;s/\\//_/} - ${Title;s/\\//_/}.%e' \\ *.mp3 *.m4a Move short videos to one dir, long videos to another dir In iOS, if you have Live Photo enabled it creates little movies each time you take a photo. While these can be very interesting context around photos, they can be quite irritating if you're playing through a collection of videos where these are mixed with videos of more moderate duration. The following code snip separates videos with a duration of more than 10 seconds from those with equal or lesser duration. # -TestName is used here so it does not destroy data. Replace this with FileName to make this actually work. # $Duration# has the # sign appended to make this tag machine readable so it can accurately be compared. # We must use perl's numeric comparisons (>, <=), not string comparisons (gt, le) # exiftool does not support if else syntax, so for the else condition you must run a second command. long_args=( \"-TestName<${opt}\" '-d' \"${working_path}/long/%Y/%m/%Y%m%d-%H-%M-%S%%-c.%%le\" '-if' '${Duration#} > 10' ) short_args=( \"-TestName<${opt}\" '-d' \"${working_path}/short/%Y/%m/%Y%m%d-%H-%M-%S%%-c.%%le\" '-if' '${Duration#} <= 10' ) find \"${PWD}\" -maxdepth 1 -type f -print0 | xargs -0 -r exiftool \"${long_args[@]}\" find \"${PWD}\" -maxdepth 1 -type f -print0 | xargs -0 -r exiftool \"${short_args[@]}\" See Also graphicsmagick imagemagick jpeginfo sips","title":"exiftool"},{"location":"exiftool/#tricks","text":"","title":"Tricks"},{"location":"exiftool/#strip-all-tags","text":"exiftool -all= filename.jpg","title":"Strip all tags"},{"location":"exiftool/#show-tags-in-a-format-that-you-can-use-to-rewrite-them","text":"exiftool -S $filename For example $ exiftool -S \"$filename\" | grep Daniel Artist: Daniel Austin Hoherd Copyright: \u00a9Daniel Austin Hoherd Creator: Daniel Austin Hoherd Rights: \u00a9Daniel Austin Hoherd $ exiftool -Rights='All rights reserved' \"$filename\" 1 image files updated $ exiftool -Rights \"$filename\" Rights : All rights reserved","title":"Show tags in a format that you can use to rewrite them"},{"location":"exiftool/#expanded-basic-usage","text":"This prints out a lot more information than normal usage, and indicates what type of metadata it is. exiftool -a -u -G:1:2 \"$filename\" Here is an example of each unique column 1 in a file $ exiftool -a -u -G:1:2 \"$filename\" | sort -u -k1,1 [Adobe:Image] DCT Encode Version : 100 [Composite:Camera] Scale Factor To 35 mm Equivalent: 7.0 [Composite:Image] Aperture : 1.8 [Composite:Location] GPS Latitude : 37 deg 15' 53.04\" N [Composite:Time] Date/Time Created : 2019:01:08 15:59:06 [ExifIFD:Camera] Exposure Program : Program AE [ExifIFD:Image] Exposure Time : 1/120 [ExifIFD:Time] Date/Time Original : 2019:01:08 15:59:06 [ExifTool:ExifTool] ExifTool Version Number : 11.11 [File:Image] File Type : JPEG [GPS:Location] GPS Version ID : 2.2.0.0 [GPS:Time] GPS Time Stamp : 23:59:06 [ICC-header:Image] Profile CMM Type : Linotronic [ICC-header:Time] Profile Date Time : 1998:02:09 06:49:00 [ICC-meas:Image] Measurement Observer : CIE 1931 [ICC-view:Image] Viewing Cond Illuminant : 19.6445 20.3718 16.8089 [ICC_Profile:Camera] Device Mfg Desc : IEC http://www.iec.ch [ICC_Profile:Image] Profile Copyright : Copyright (c) 1998 Hewlett-Packard Company [IFD0:Author] Artist : Daniel Austin Hoherd [IFD0:Camera] Make : Apple [IFD0:Image] X Resolution : 240 [IFD0:Time] Modify Date : 2019:01:09 13:50:29 [IFD1:Image] Compression : JPEG (old-style) [IFD1:Preview] Thumbnail Image : (Binary data 12008 bytes, use -b option to extract) [IPTC:Author] By-line : Daniel Austin Hoherd [IPTC:Other] Coded Character Set : UTF8 [IPTC:Time] Date Created : 2019:01:08 [Photoshop:Author] Copyright Flag : True [Photoshop:Image] X Resolution : 240 [Photoshop:Preview] Photoshop Thumbnail : (Binary data 12008 bytes, use -b option to extract) [System:Image] File Name : 2019-01-08-15-59-06-46628465322_d1657e4c95_o.jpg [System:Time] File Modification Date/Time : 2019:01:22 09:00:22-08:00 [XMP-aux:Camera] Distortion Correction Already Applied: True [XMP-crs:Image] Already Applied : True [XMP-dc:Author] Creator : Daniel Austin Hoherd [XMP-dc:Image] Format : image/jpeg [XMP-photoshop:Image] Headline : ljwZuD [XMP-photoshop:Time] Date Created : 2019:01:08 15:59:06.448 [XMP-x:Document] XMP Toolkit : Image::ExifTool 11.11 [XMP-xmp:Image] Creator Tool : Adobe Photoshop Lightroom 6.14 (Macintosh) [XMP-xmp:Time] Create Date : 2019:01:08 15:59:06.448 [XMP-xmpMM:Other] Derived From Document ID : 9880573B7AACBFC189C795E182E8A05D [XMP-xmpMM:Time] History When : 2019:01:09 13:50:29-08:00 [XMP-xmpRights:Author] Marked : True","title":"Expanded basic usage"},{"location":"exiftool/#add-missing-lens-data-on-rokinon-85mm","text":"Rokinon 85mm is a mechanical lens with no electronics, so no data about photos taken with it are stored in the image. This adds some stock metadata describing characteristics of the lens that are always true, which helps these photos sorting accurately, etc.. exiftool \\ -overwrite_original \\ -LensModel='Rokinon 85mm f/1.4' \\ -FocalLength='85' \\ -LongFocal='85' \\ -ShortFocal='85' \\ filename.dng","title":"Add missing lens data on Rokinon 85mm"},{"location":"exiftool/#correct-exif-time-for-instance-to-sync-with-gps-time","text":"The following example increases all metadata dates by 1 minute and 56 seconds. # exiftool -AllDates-='Y:M:D H:M:S' exiftool -AllDates+='0:0:0 0:1:56'","title":"Correct EXIF time, for instance to sync with GPS time"},{"location":"exiftool/#set-all-dates-to-something-obviously-wrong","text":"This is useful when scanning or photographing film or prints where you do not want the current date associated with the image. exiftool -alldates='1900:01:01 01:01:01' *","title":"Set all dates to something obviously wrong"},{"location":"exiftool/#rename-gpx-files-based-on-the-capture-time","text":"You will end up with a filename like 2013-09-30-23-35-40.gpx based off of the first trkpt timestamp. exiftool -d '%Y%m%d-%H-%M-%S' '-FileName<${GpxTrkTrksegTrkptTime;tr/ /-/;tr/:/-/;tr(/Z/)()d;}%-c.gpx' *.gpx","title":"Rename GPX files based on the capture time"},{"location":"exiftool/#rename-files-to-their-original-date-and-time-using-a-lower-case-file-extension","text":"# %le = lowercase extension # %-c = unique filenames when the timestamp is exactly the same. EG: filename-1.jpg exiftool \"-FileName<CreateDate\" -d \"%Y%m%d-%H-%M-%S%%-c.%%le\" *.jpg","title":"Rename files to their original date and time using a lower case file extension"},{"location":"exiftool/#rename-files-using-a-combination-of-tags","text":"Using the name of the tag as output by exiftool -S , you can create complicated filenames by combining tags: exiftool -d '%Y%m%d-%H-%M-%S' '-FileName<${CreateDate;}_${Headline;}%-c.%e'","title":"Rename files using a combination of tags"},{"location":"exiftool/#set-file-modify-time-to-image-capture-time","text":"Useful when you want to sort in your file browser by modification time and get a chronological order of files. exiftool \"-FileModifyDate<DateTimeOriginal\" *.jpg","title":"Set file modify time to image capture time"},{"location":"exiftool/#generate-a-table-of-filename-camera-model-and-file-size-in-bytes-sorted-by-bytes","text":"$ find /src_dir/ -iname '*.dng' | xargs exiftool -p '$filename,$Model,$FileSize#' 2>/dev/null | sort -t, -k3 -n | column -s, -t 2012-01-26-23-19-54-6795223065_2e771d1012_o.jpg iPhone 4S 1242739 2013-02-03-10-01-56-8441346635_df4404a1f6_o.jpg NIKON D5200 1646481 2012-01-22-15-16-38-6746574603_d52311264f_o.jpg Canon EOS REBEL T3i 1671734 2011-01-22-23-44-31-6271225963_f9b95b2d7a_o.jpg NIKON D3S 1773081 2010-01-27-13-07-00-4313649499_835a6649c2_o.jpg NIKON D300 1829578 2016-02-03-07-26-32-24522158414_4aaf116d2a_o.jpg iPhone 6 2319061 2018-10-24-13-39-09-44676649345_1de0f581cd_o.jpg iPhone XS Max 2971254 2015-02-02-19-17-09-24587486051_3032823e4e_o.jpg NIKON D800 3309696 2014-01-27-13-52-41-12951707465_79a8dd3827_o.jpg iPhone 5 3401479 2017-01-22-18-33-28-31693592473_40478df088_o.jpg ILCE-7 4230661 2018-12-23-22-33-40-45536007225_8fdd50691a_o.jpg NIKON D850 4924617 2017-02-06-08-04-18-44658317900_98e04997fb_o.jpg iPhone 6s 8712631 2018-12-28-16-56-42-39713091073_c57ec1a8a8_o.jpg Canon EOS 5D Mark II 8741601 2019-01-08-16-11-49-39716361093_479e6a2323_o.jpg iPhone 8 Plus 12041600","title":"Generate a table of Filename, Camera Model and File Size in bytes, sorted by bytes"},{"location":"exiftool/#generate-rsync-commands-for-files-matching-a-string","text":"Useful for reviewing commands before running them, the following example generates a command for every file, then uses awk to do a numeric comparison on the last field to sort out images under a certain ImageHeight. These rsync commands can be pasted into a terminal to run. (Generating a list of files for use with rsync --files-from would be a better option for this specific use case, but this illustration could be adapted for commands that do not have such an option.) $ exiftool -d \"%s\" -p 'rsync -aP $filename otherhost:~/Pictures/ # $ImageHeight' * 2>/dev/null | awk '$NF >= 2800 {print}' rsync -aP 2017-02-06-08-04-18-44658317900_98e04997fb_o.jpg otherhost:~/Pictures/ # 2869 rsync -aP 2018-02-06-09-50-04-31514483967_a422a3e3aa_o.jpg otherhost:~/Pictures/ # 2880 rsync -aP 2018-02-06-15-04-43-45541501845_8dbdc3b208_o.jpg otherhost:~/Pictures/ # 2880 rsync -aP 2018-02-06-15-05-43-31514485997_e2551fdbbc_o.jpg otherhost:~/Pictures/ # 2880 rsync -aP 2018-12-19-10-53-27-45663859984_0f93ac24ec_o.jpg otherhost:~/Pictures/ # 2880","title":"Generate rsync commands for files matching a string"},{"location":"exiftool/#rename-files-to-their-shuttercount","text":"Filenames will not be changed if ShutterCount field is not populated. exiftool -P '-filename<${ShutterCount;}.%e' *.dng","title":"Rename files to their ShutterCount"},{"location":"exiftool/#rename-files-based-on-a-set-of-possible-names","text":"Exiftool will use the last parameter where all variables are present. exiftool -P -d '%F-%H-%M-%S' \\ '-filename<${DateTimeOriginal} - ${Make;}.%e' \\ '-filename<${CreateDate} - ${Make;}.%e' \\ '-filename<${DateTimeOriginal} - ${Make;} - ${Model;}.%e' \\ '-filename<${CreateDate} - ${Make;} - ${Model;}.%e' \\ '-filename<${DateTimeOriginal} - ${Make;} - ${Model;} - ${ShutterCount}.%e' \\ '-filename<${CreateDate} - ${Make;} - ${Model;} - ${ShutterCount}.%e' \\ *.dng","title":"Rename files based on a set of possible names"},{"location":"exiftool/#use-testname-tag-target-to-test-what-files-would-be-renamed-to","text":"This block builds an array of possible tags to use as a filename, creates an exiftool argument string from that array, then tests what files would be named to. This is useful when dealing with files from various sources that don't all use the same tag to store the original media creation time. By using TestName instead of FileName as the target, we observe what would occur, essentially a dry-run, instead of actually renaming the files. There is a funky behavior of %-c when you operate on a file that should ideally not be renamed. Exiftool will toggle back and forth each run appending and removing -1 . This assumes GNU xargs for the -r flag. #!/usr/bin/env bash set -x # The last valid variable from this list is used as the filename source create_date_sources=( TrackCreateDate RIFF:DateTimeOriginal MediaCreateDate FileModifyDate DateTimeOriginal CreateDate ) for opt in \"${create_date_sources[@]}\" ; do args+=( \"-TestName<${opt}\" ) ; done ; args+=( '-d' './%Y/%m/%Y%m%d-%H-%M-%S%%-c.%%le' ) find . -maxdepth 1 -type f ! -name '*.sh' -print0 | xargs -0 -r exiftool \"${args[@]}\"","title":"Use TestName tag target to test what files would be renamed to"},{"location":"exiftool/#rename-music-files-in-a-directory","text":"There is a big gotcha here, which is that slashes will create directories where they appear, which can cause serious problems. The ${Tag;s/\\//_/} syntax replaces / with _ , but there may be other characters that can cause unexpected results. This is a great place to use -TestName to inspect what would change before using -FileName to make the changes. exiftool \\ '-FileName<${Artist;s/\\//_/} - ${Title;s/\\//_/}.%e' \\ '-FileName<${Artist;s/\\//_/} - ${Album;s/\\//_/} - ${Title;s/\\//_/}.%e' \\ *.mp3 *.m4a","title":"Rename music files in a directory"},{"location":"exiftool/#move-short-videos-to-one-dir-long-videos-to-another-dir","text":"In iOS, if you have Live Photo enabled it creates little movies each time you take a photo. While these can be very interesting context around photos, they can be quite irritating if you're playing through a collection of videos where these are mixed with videos of more moderate duration. The following code snip separates videos with a duration of more than 10 seconds from those with equal or lesser duration. # -TestName is used here so it does not destroy data. Replace this with FileName to make this actually work. # $Duration# has the # sign appended to make this tag machine readable so it can accurately be compared. # We must use perl's numeric comparisons (>, <=), not string comparisons (gt, le) # exiftool does not support if else syntax, so for the else condition you must run a second command. long_args=( \"-TestName<${opt}\" '-d' \"${working_path}/long/%Y/%m/%Y%m%d-%H-%M-%S%%-c.%%le\" '-if' '${Duration#} > 10' ) short_args=( \"-TestName<${opt}\" '-d' \"${working_path}/short/%Y/%m/%Y%m%d-%H-%M-%S%%-c.%%le\" '-if' '${Duration#} <= 10' ) find \"${PWD}\" -maxdepth 1 -type f -print0 | xargs -0 -r exiftool \"${long_args[@]}\" find \"${PWD}\" -maxdepth 1 -type f -print0 | xargs -0 -r exiftool \"${short_args[@]}\"","title":"Move short videos to one dir, long videos to another dir"},{"location":"exiftool/#see-also","text":"graphicsmagick imagemagick jpeginfo sips","title":"See Also"},{"location":"fedramp/","text":"https://en.wikipedia.org/wiki/FedRAMP https://www.fedramp.gov/","title":"Fedramp"},{"location":"ffmpeg/","text":"ffmpeg is a tool for editing movie files. https://ffmpeg.org Examples Convert container format for X in *.mov ; do ffmpeg -i \"${X}\" -c copy -map 0 \"${X%.mov}.mp4\" ; done ; Time Lapse Generate a movie from an image sequence like 001.jpg-999.jpg ffmpeg -r 10 -b 1800 -i %03d.jpg test1800.mp4 Rename files as a sequence: i=0 ; find . -type f | while read -r F ; do let i=${i}+1 ; fn=$(printf %06d ${i}) ; mv \"${F}\" \"${fn}.jpg\" ; done ; Sample some of the middle of the time-lapse ffmpeg -pattern_type sequence -start_number 3000 -r 30 -i %06d.jpg -s 1440x1080 -frames 120 \"$(date +%F_%T).mp4\" Turn these images into a video ffmpeg -pattern_type sequence -r 30 -i %06d.jpg -s 1440x1080 \"$(date +%F_%T).mp4\" Audio Replace Replace the audio of DSC_4436.AVI with 01 Gymnopedie 1.mp3 and limit the duration of the output so the music doesn't play beyond the end of the video. ffmpeg -t 00:00:47.99 -i DSC_4436.AVI -i \"01 Gymnopedie 1.mp3\" -map 0:0 -map 1:0 -vcodec copy -acodec copy output.AVI Slow down video to half speed, drop audio ffmpeg -i DHO_8751.MOV -an -vf \"setpts=(2/1)*PTS\" output.mp4 Extract two seconds worth of frames at 24fps starting at 15m ffmpeg -i movie.mkv -r 24 -t 00:00:02.00 -ss 00:15:00 temp/movie-%4d.jpg Detect errors in files ffmpeg -v error -i 20091024-08-46-00.mpg -f null - 2>> error.log","title":"ffmpeg"},{"location":"ffmpeg/#examples","text":"","title":"Examples"},{"location":"ffmpeg/#convert-container-format","text":"for X in *.mov ; do ffmpeg -i \"${X}\" -c copy -map 0 \"${X%.mov}.mp4\" ; done ;","title":"Convert container format"},{"location":"ffmpeg/#time-lapse","text":"","title":"Time Lapse"},{"location":"ffmpeg/#generate-a-movie-from-an-image-sequence-like-001jpg-999jpg","text":"ffmpeg -r 10 -b 1800 -i %03d.jpg test1800.mp4","title":"Generate a movie from an image sequence like 001.jpg-999.jpg"},{"location":"ffmpeg/#rename-files-as-a-sequence","text":"i=0 ; find . -type f | while read -r F ; do let i=${i}+1 ; fn=$(printf %06d ${i}) ; mv \"${F}\" \"${fn}.jpg\" ; done ;","title":"Rename files as a sequence:"},{"location":"ffmpeg/#sample-some-of-the-middle-of-the-time-lapse","text":"ffmpeg -pattern_type sequence -start_number 3000 -r 30 -i %06d.jpg -s 1440x1080 -frames 120 \"$(date +%F_%T).mp4\"","title":"Sample some of the middle of the time-lapse"},{"location":"ffmpeg/#turn-these-images-into-a-video","text":"ffmpeg -pattern_type sequence -r 30 -i %06d.jpg -s 1440x1080 \"$(date +%F_%T).mp4\"","title":"Turn these images into a video"},{"location":"ffmpeg/#audio-replace","text":"Replace the audio of DSC_4436.AVI with 01 Gymnopedie 1.mp3 and limit the duration of the output so the music doesn't play beyond the end of the video. ffmpeg -t 00:00:47.99 -i DSC_4436.AVI -i \"01 Gymnopedie 1.mp3\" -map 0:0 -map 1:0 -vcodec copy -acodec copy output.AVI","title":"Audio Replace"},{"location":"ffmpeg/#slow-down-video-to-half-speed-drop-audio","text":"ffmpeg -i DHO_8751.MOV -an -vf \"setpts=(2/1)*PTS\" output.mp4","title":"Slow down video to half speed, drop audio"},{"location":"ffmpeg/#extract-two-seconds-worth-of-frames-at-24fps-starting-at-15m","text":"ffmpeg -i movie.mkv -r 24 -t 00:00:02.00 -ss 00:15:00 temp/movie-%4d.jpg","title":"Extract two seconds worth of frames at 24fps starting at 15m"},{"location":"ffmpeg/#detect-errors-in-files","text":"ffmpeg -v error -i 20091024-08-46-00.mpg -f null - 2>> error.log","title":"Detect errors in files"},{"location":"figlet/","text":"Figlet prints horizontal text in ascii drawings. Examples Print text $ figlet hello _ _ _ | |__ ___| | | ___ | '_ \\ / _ \\ | |/ _ \\ | | | | __/ | | (_) | |_| |_|\\___|_|_|\\___/ Show available fonts $ showfigfonts | head 3-d : **** ** */// * /** / /* /** *** ***** ****** /// *///// **///** * /* /** /** / **** //****** //// ////// Use a font $ figlet -f 3-d hello ** ** ** /** /** /** /** ***** /** /** ****** /****** **///** /** /** **////** /**///**/******* /** /**/** /** /** /**/**//// /** /**/** /** /** /**//****** *** ***//****** // // ////// /// /// ////// See Also cowsay","title":"figlet"},{"location":"figlet/#examples","text":"","title":"Examples"},{"location":"figlet/#print-text","text":"$ figlet hello _ _ _ | |__ ___| | | ___ | '_ \\ / _ \\ | |/ _ \\ | | | | __/ | | (_) | |_| |_|\\___|_|_|\\___/","title":"Print text"},{"location":"figlet/#show-available-fonts","text":"$ showfigfonts | head 3-d : **** ** */// * /** / /* /** *** ***** ****** /// *///// **///** * /* /** /** / **** //****** //// //////","title":"Show available fonts"},{"location":"figlet/#use-a-font","text":"$ figlet -f 3-d hello ** ** ** /** /** /** /** ***** /** /** ****** /****** **///** /** /** **////** /**///**/******* /** /**/** /** /** /**/**//// /** /**/** /** /** /**//****** *** ***//****** // // ////// /// /// //////","title":"Use a font"},{"location":"figlet/#see-also","text":"cowsay","title":"See Also"},{"location":"find/","text":"The find util letes you search a filesystem for things that match filesystem attributes. Unfortunately this is one of those tools where BSD and GNU deviate syntactically and featurewise, and GNU mostly wins. Examples Find and delete empty directories 2 levels deep or deeper find \"${PWD}\" -mindepth 2 -type d -empty -delete Find based on a regex find /tank/movies -regextype egrep -iregex '.*\\.(mov|mp4)$' Find files and perform operations on them One at a time: find \"${PWD}\" -type d -exec dot_clean {} \\; Or several in batches, similar to how xargs handles things: find \"${PWD}\" -type d -exec dot_clean {} \\+ Find files that match a glob find \"${PWD}\" -name '????????-??-??-??_[0-9][0-9][0-9]???.dng' Alter permissions on some files that are not already set correctly find . -mindepth 2 -type f ! -perm 444 -exec chmod 444 {} \\+ Find files in the current directory that do not match any of several listed filenames find . -maxdepth 1 -type f ! -iname '.*' ! -name .DS_Store ! -name '*.db' Correctly handle spaces when piping to xargs find /Applications -mindepth 1 -maxdepth 1 -type d -name '* *' -print0 | xargs -0 -n1 echo Find executable files This finds all files where an executable bit is set. With BSD find: find . -type f -perm +111 With GNU find: find . -type f -executable","title":"find"},{"location":"find/#examples","text":"","title":"Examples"},{"location":"find/#find-and-delete-empty-directories-2-levels-deep-or-deeper","text":"find \"${PWD}\" -mindepth 2 -type d -empty -delete","title":"Find and delete empty directories 2 levels deep or deeper"},{"location":"find/#find-based-on-a-regex","text":"find /tank/movies -regextype egrep -iregex '.*\\.(mov|mp4)$'","title":"Find based on a regex"},{"location":"find/#find-files-and-perform-operations-on-them","text":"One at a time: find \"${PWD}\" -type d -exec dot_clean {} \\; Or several in batches, similar to how xargs handles things: find \"${PWD}\" -type d -exec dot_clean {} \\+","title":"Find files and perform operations on them"},{"location":"find/#find-files-that-match-a-glob","text":"find \"${PWD}\" -name '????????-??-??-??_[0-9][0-9][0-9]???.dng'","title":"Find files that match a glob"},{"location":"find/#alter-permissions-on-some-files-that-are-not-already-set-correctly","text":"find . -mindepth 2 -type f ! -perm 444 -exec chmod 444 {} \\+","title":"Alter permissions on some files that are not already set correctly"},{"location":"find/#find-files-in-the-current-directory-that-do-not-match-any-of-several-listed-filenames","text":"find . -maxdepth 1 -type f ! -iname '.*' ! -name .DS_Store ! -name '*.db'","title":"Find files in the current directory that do not match any of several listed filenames"},{"location":"find/#correctly-handle-spaces-when-piping-to-xargs","text":"find /Applications -mindepth 1 -maxdepth 1 -type d -name '* *' -print0 | xargs -0 -n1 echo","title":"Correctly handle spaces when piping to xargs"},{"location":"find/#find-executable-files","text":"This finds all files where an executable bit is set. With BSD find: find . -type f -perm +111 With GNU find: find . -type f -executable","title":"Find executable files"},{"location":"findmnt/","text":"\"findmnt will list all mounted filesytems or search for a filesystem. The findmnt command is able to search in /etc/fstab, /etc/fstab.d, /etc/mtab or /proc/self/mountinfo. If device or mountpoint is not given, all filesystems are shown.\" - man findmnt Examples Simple usage Here is the output of findmnt on an Ubuntu 16.04 Vagrant box: TARGET SOURCE FSTYPE OPTIONS / /dev/sda1 ext4 rw,relatime,data=ordered \u251c\u2500/sys sysfs sysfs rw,nosuid,nodev,noexec,relatime \u2502 \u251c\u2500/sys/kernel/security securityfs securityfs rw,nosuid,nodev,noexec,relatime \u2502 \u251c\u2500/sys/fs/cgroup tmpfs tmpfs ro,nosuid,nodev,noexec,mode=755 \u2502 \u2502 \u251c\u2500/sys/fs/cgroup/systemd cgroup cgroup rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/lib/systemd/systemd-cgroups-agent,name=systemd \u2502 \u2502 \u251c\u2500/sys/fs/cgroup/cpu,cpuacct cgroup cgroup rw,nosuid,nodev,noexec,relatime,cpu,cpuacct \u2502 \u2502 \u251c\u2500/sys/fs/cgroup/perf_event cgroup cgroup rw,nosuid,nodev,noexec,relatime,perf_event \u2502 \u2502 \u251c\u2500/sys/fs/cgroup/hugetlb cgroup cgroup rw,nosuid,nodev,noexec,relatime,hugetlb \u2502 \u2502 \u251c\u2500/sys/fs/cgroup/blkio cgroup cgroup rw,nosuid,nodev,noexec,relatime,blkio \u2502 \u2502 \u251c\u2500/sys/fs/cgroup/devices cgroup cgroup rw,nosuid,nodev,noexec,relatime,devices \u2502 \u2502 \u251c\u2500/sys/fs/cgroup/cpuset cgroup cgroup rw,nosuid,nodev,noexec,relatime,cpuset \u2502 \u2502 \u251c\u2500/sys/fs/cgroup/memory cgroup cgroup rw,nosuid,nodev,noexec,relatime,memory \u2502 \u2502 \u251c\u2500/sys/fs/cgroup/net_cls,net_prio cgroup cgroup rw,nosuid,nodev,noexec,relatime,net_cls,net_prio \u2502 \u2502 \u251c\u2500/sys/fs/cgroup/freezer cgroup cgroup rw,nosuid,nodev,noexec,relatime,freezer \u2502 \u2502 \u2514\u2500/sys/fs/cgroup/pids cgroup cgroup rw,nosuid,nodev,noexec,relatime,pids \u2502 \u251c\u2500/sys/fs/pstore pstore pstore rw,nosuid,nodev,noexec,relatime \u2502 \u251c\u2500/sys/kernel/debug debugfs debugfs rw,relatime \u2502 \u2514\u2500/sys/fs/fuse/connections fusectl fusectl rw,relatime \u251c\u2500/proc proc proc rw,nosuid,nodev,noexec,relatime \u2502 \u2514\u2500/proc/sys/fs/binfmt_misc systemd-1 autofs rw,relatime,fd=33,pgrp=1,timeout=0,minproto=5,maxproto=5,direct \u251c\u2500/dev udev devtmpfs rw,nosuid,relatime,size=500888k,nr_inodes=125222,mode=755 \u2502 \u251c\u2500/dev/pts devpts devpts rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=000 \u2502 \u251c\u2500/dev/shm tmpfs tmpfs rw,nosuid,nodev \u2502 \u251c\u2500/dev/hugepages hugetlbfs hugetlbfs rw,relatime \u2502 \u2514\u2500/dev/mqueue mqueue mqueue rw,relatime \u251c\u2500/run tmpfs tmpfs rw,nosuid,noexec,relatime,size=101596k,mode=755 \u2502 \u251c\u2500/run/lock tmpfs tmpfs rw,nosuid,nodev,noexec,relatime,size=5120k \u2502 \u2514\u2500/run/user/1000 tmpfs tmpfs rw,nosuid,nodev,relatime,size=101596k,mode=700,uid=1000,gid=1000 \u251c\u2500/var/lib/lxcfs lxcfs fuse.lxcfs rw,nosuid,nodev,relatime,user_id=0,group_id=0,allow_other \u2514\u2500/vagrant vagrant vboxsf rw,nodev,relatime Output as key/value pairs per device $ findmnt -P TARGET=\"/sys\" SOURCE=\"sysfs\" FSTYPE=\"sysfs\" OPTIONS=\"rw,nosuid,nodev,noexec,relatime\" TARGET=\"/proc\" SOURCE=\"proc\" FSTYPE=\"proc\" OPTIONS=\"rw,nosuid,nodev,noexec,relatime\" TARGET=\"/dev\" SOURCE=\"udev\" FSTYPE=\"devtmpfs\" OPTIONS=\"rw,nosuid,relatime,size=500888k,nr_inodes=125222,mode=755\" TARGET=\"/dev/pts\" SOURCE=\"devpts\" FSTYPE=\"devpts\" OPTIONS=\"rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=000\" TARGET=\"/run\" SOURCE=\"tmpfs\" FSTYPE=\"tmpfs\" OPTIONS=\"rw,nosuid,noexec,relatime,size=101596k,mode=755\" TARGET=\"/\" SOURCE=\"/dev/sda1\" FSTYPE=\"ext4\" OPTIONS=\"rw,relatime,data=ordered\" TARGET=\"/sys/kernel/security\" SOURCE=\"securityfs\" FSTYPE=\"securityfs\" OPTIONS=\"rw,nosuid,nodev,noexec,relatime\" TARGET=\"/dev/shm\" SOURCE=\"tmpfs\" FSTYPE=\"tmpfs\" OPTIONS=\"rw,nosuid,nodev\" TARGET=\"/run/lock\" SOURCE=\"tmpfs\" FSTYPE=\"tmpfs\" OPTIONS=\"rw,nosuid,nodev,noexec,relatime,size=5120k\" TARGET=\"/sys/fs/cgroup\" SOURCE=\"tmpfs\" FSTYPE=\"tmpfs\" OPTIONS=\"ro,nosuid,nodev,noexec,mode=755\" TARGET=\"/sys/fs/cgroup/systemd\" SOURCE=\"cgroup\" FSTYPE=\"cgroup\" OPTIONS=\"rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/lib/systemd/systemd-cgroups-agent,name=systemd\" TARGET=\"/sys/fs/pstore\" SOURCE=\"pstore\" FSTYPE=\"pstore\" OPTIONS=\"rw,nosuid,nodev,noexec,relatime\" TARGET=\"/sys/fs/cgroup/net_cls,net_prio\" SOURCE=\"cgroup\" FSTYPE=\"cgroup\" OPTIONS=\"rw,nosuid,nodev,noexec,relatime,net_cls,net_prio\" TARGET=\"/sys/fs/cgroup/perf_event\" SOURCE=\"cgroup\" FSTYPE=\"cgroup\" OPTIONS=\"rw,nosuid,nodev,noexec,relatime,perf_event\" TARGET=\"/sys/fs/cgroup/cpu,cpuacct\" SOURCE=\"cgroup\" FSTYPE=\"cgroup\" OPTIONS=\"rw,nosuid,nodev,noexec,relatime,cpu,cpuacct\" TARGET=\"/sys/fs/cgroup/hugetlb\" SOURCE=\"cgroup\" FSTYPE=\"cgroup\" OPTIONS=\"rw,nosuid,nodev,noexec,relatime,hugetlb\" TARGET=\"/sys/fs/cgroup/memory\" SOURCE=\"cgroup\" FSTYPE=\"cgroup\" OPTIONS=\"rw,nosuid,nodev,noexec,relatime,memory\" TARGET=\"/sys/fs/cgroup/devices\" SOURCE=\"cgroup\" FSTYPE=\"cgroup\" OPTIONS=\"rw,nosuid,nodev,noexec,relatime,devices\" TARGET=\"/sys/fs/cgroup/freezer\" SOURCE=\"cgroup\" FSTYPE=\"cgroup\" OPTIONS=\"rw,nosuid,nodev,noexec,relatime,freezer\" TARGET=\"/sys/fs/cgroup/cpuset\" SOURCE=\"cgroup\" FSTYPE=\"cgroup\" OPTIONS=\"rw,nosuid,nodev,noexec,relatime,cpuset\" TARGET=\"/sys/fs/cgroup/blkio\" SOURCE=\"cgroup\" FSTYPE=\"cgroup\" OPTIONS=\"rw,nosuid,nodev,noexec,relatime,blkio\" TARGET=\"/sys/fs/cgroup/pids\" SOURCE=\"cgroup\" FSTYPE=\"cgroup\" OPTIONS=\"rw,nosuid,nodev,noexec,relatime,pids\" TARGET=\"/proc/sys/fs/binfmt_misc\" SOURCE=\"systemd-1\" FSTYPE=\"autofs\" OPTIONS=\"rw,relatime,fd=26,pgrp=1,timeout=0,minproto=5,maxproto=5,direct\" TARGET=\"/sys/kernel/debug\" SOURCE=\"debugfs\" FSTYPE=\"debugfs\" OPTIONS=\"rw,relatime\" TARGET=\"/dev/hugepages\" SOURCE=\"hugetlbfs\" FSTYPE=\"hugetlbfs\" OPTIONS=\"rw,relatime\" TARGET=\"/dev/mqueue\" SOURCE=\"mqueue\" FSTYPE=\"mqueue\" OPTIONS=\"rw,relatime\" TARGET=\"/sys/fs/fuse/connections\" SOURCE=\"fusectl\" FSTYPE=\"fusectl\" OPTIONS=\"rw,relatime\" TARGET=\"/var/lib/lxcfs\" SOURCE=\"lxcfs\" FSTYPE=\"fuse.lxcfs\" OPTIONS=\"rw,nosuid,nodev,relatime,user_id=0,group_id=0,allow_other\" TARGET=\"/vagrant\" SOURCE=\"vagrant\" FSTYPE=\"vboxsf\" OPTIONS=\"rw,nodev,relatime\" TARGET=\"/run/user/1000\" SOURCE=\"tmpfs\" FSTYPE=\"tmpfs\" OPTIONS=\"rw,nosuid,nodev,relatime,size=101596k,mode=700,uid=1000,gid=1000\" Output as JSON $ findmnt -J { \"filesystems\": [ {\"target\": \"/\", \"source\": \"/dev/sda1\", \"fstype\": \"ext4\", \"options\": \"rw,relatime,data=ordered\", \"children\": [ {\"target\": \"/sys\", \"source\": \"sysfs\", \"fstype\": \"sysfs\", \"options\": \"rw,nosuid,nodev,noexec,relatime\", \"children\": [ {\"target\": \"/sys/kernel/security\", \"source\": \"securityfs\", \"fstype\": \"securityfs\", \"options\": \"rw,nosuid,nodev,noexec,relatime\"}, {\"target\": \"/sys/fs/cgroup\", \"source\": \"tmpfs\", \"fstype\": \"tmpfs\", \"options\": \"ro,nosuid,nodev,noexec,mode=755\", \"children\": [ {\"target\": \"/sys/fs/cgroup/systemd\", \"source\": \"cgroup\", \"fstype\": \"cgroup\", \"options\": \"rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/lib/systemd/systemd-cgroups-agent,name=systemd\"}, {\"target\": \"/sys/fs/cgroup/net_cls,net_prio\", \"source\": \"cgroup\", \"fstype\": \"cgroup\", \"options\": \"rw,nosuid,nodev,noexec,relatime,net_cls,net_prio\"}, {\"target\": \"/sys/fs/cgroup/perf_event\", \"source\": \"cgroup\", \"fstype\": \"cgroup\", \"options\": \"rw,nosuid,nodev,noexec,relatime,perf_event\"}, {\"target\": \"/sys/fs/cgroup/cpu,cpuacct\", \"source\": \"cgroup\", \"fstype\": \"cgroup\", \"options\": \"rw,nosuid,nodev,noexec,relatime,cpu,cpuacct\"}, {\"target\": \"/sys/fs/cgroup/hugetlb\", \"source\": \"cgroup\", \"fstype\": \"cgroup\", \"options\": \"rw,nosuid,nodev,noexec,relatime,hugetlb\"}, {\"target\": \"/sys/fs/cgroup/memory\", \"source\": \"cgroup\", \"fstype\": \"cgroup\", \"options\": \"rw,nosuid,nodev,noexec,relatime,memory\"}, {\"target\": \"/sys/fs/cgroup/devices\", \"source\": \"cgroup\", \"fstype\": \"cgroup\", \"options\": \"rw,nosuid,nodev,noexec,relatime,devices\"}, {\"target\": \"/sys/fs/cgroup/freezer\", \"source\": \"cgroup\", \"fstype\": \"cgroup\", \"options\": \"rw,nosuid,nodev,noexec,relatime,freezer\"}, {\"target\": \"/sys/fs/cgroup/cpuset\", \"source\": \"cgroup\", \"fstype\": \"cgroup\", \"options\": \"rw,nosuid,nodev,noexec,relatime,cpuset\"}, {\"target\": \"/sys/fs/cgroup/blkio\", \"source\": \"cgroup\", \"fstype\": \"cgroup\", \"options\": \"rw,nosuid,nodev,noexec,relatime,blkio\"}, {\"target\": \"/sys/fs/cgroup/pids\", \"source\": \"cgroup\", \"fstype\": \"cgroup\", \"options\": \"rw,nosuid,nodev,noexec,relatime,pids\"} ] }, {\"target\": \"/sys/fs/pstore\", \"source\": \"pstore\", \"fstype\": \"pstore\", \"options\": \"rw,nosuid,nodev,noexec,relatime\"}, {\"target\": \"/sys/kernel/debug\", \"source\": \"debugfs\", \"fstype\": \"debugfs\", \"options\": \"rw,relatime\"}, {\"target\": \"/sys/fs/fuse/connections\", \"source\": \"fusectl\", \"fstype\": \"fusectl\", \"options\": \"rw,relatime\"} ] }, {\"target\": \"/proc\", \"source\": \"proc\", \"fstype\": \"proc\", \"options\": \"rw,nosuid,nodev,noexec,relatime\", \"children\": [ {\"target\": \"/proc/sys/fs/binfmt_misc\", \"source\": \"systemd-1\", \"fstype\": \"autofs\", \"options\": \"rw,relatime,fd=26,pgrp=1,timeout=0,minproto=5,maxproto=5,direct\"} ] }, {\"target\": \"/dev\", \"source\": \"udev\", \"fstype\": \"devtmpfs\", \"options\": \"rw,nosuid,relatime,size=500888k,nr_inodes=125222,mode=755\", \"children\": [ {\"target\": \"/dev/pts\", \"source\": \"devpts\", \"fstype\": \"devpts\", \"options\": \"rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=000\"}, {\"target\": \"/dev/shm\", \"source\": \"tmpfs\", \"fstype\": \"tmpfs\", \"options\": \"rw,nosuid,nodev\"}, {\"target\": \"/dev/hugepages\", \"source\": \"hugetlbfs\", \"fstype\": \"hugetlbfs\", \"options\": \"rw,relatime\"}, {\"target\": \"/dev/mqueue\", \"source\": \"mqueue\", \"fstype\": \"mqueue\", \"options\": \"rw,relatime\"} ] }, {\"target\": \"/run\", \"source\": \"tmpfs\", \"fstype\": \"tmpfs\", \"options\": \"rw,nosuid,noexec,relatime,size=101596k,mode=755\", \"children\": [ {\"target\": \"/run/lock\", \"source\": \"tmpfs\", \"fstype\": \"tmpfs\", \"options\": \"rw,nosuid,nodev,noexec,relatime,size=5120k\"}, {\"target\": \"/run/user/1000\", \"source\": \"tmpfs\", \"fstype\": \"tmpfs\", \"options\": \"rw,nosuid,nodev,relatime,size=101596k,mode=700,uid=1000,gid=1000\"} ] }, {\"target\": \"/var/lib/lxcfs\", \"source\": \"lxcfs\", \"fstype\": \"fuse.lxcfs\", \"options\": \"rw,nosuid,nodev,relatime,user_id=0,group_id=0,allow_other\"}, {\"target\": \"/vagrant\", \"source\": \"vagrant\", \"fstype\": \"vboxsf\", \"options\": \"rw,nodev,relatime\"}, ] } ] } See also lsblk","title":"findmnt"},{"location":"findmnt/#examples","text":"","title":"Examples"},{"location":"findmnt/#simple-usage","text":"Here is the output of findmnt on an Ubuntu 16.04 Vagrant box: TARGET SOURCE FSTYPE OPTIONS / /dev/sda1 ext4 rw,relatime,data=ordered \u251c\u2500/sys sysfs sysfs rw,nosuid,nodev,noexec,relatime \u2502 \u251c\u2500/sys/kernel/security securityfs securityfs rw,nosuid,nodev,noexec,relatime \u2502 \u251c\u2500/sys/fs/cgroup tmpfs tmpfs ro,nosuid,nodev,noexec,mode=755 \u2502 \u2502 \u251c\u2500/sys/fs/cgroup/systemd cgroup cgroup rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/lib/systemd/systemd-cgroups-agent,name=systemd \u2502 \u2502 \u251c\u2500/sys/fs/cgroup/cpu,cpuacct cgroup cgroup rw,nosuid,nodev,noexec,relatime,cpu,cpuacct \u2502 \u2502 \u251c\u2500/sys/fs/cgroup/perf_event cgroup cgroup rw,nosuid,nodev,noexec,relatime,perf_event \u2502 \u2502 \u251c\u2500/sys/fs/cgroup/hugetlb cgroup cgroup rw,nosuid,nodev,noexec,relatime,hugetlb \u2502 \u2502 \u251c\u2500/sys/fs/cgroup/blkio cgroup cgroup rw,nosuid,nodev,noexec,relatime,blkio \u2502 \u2502 \u251c\u2500/sys/fs/cgroup/devices cgroup cgroup rw,nosuid,nodev,noexec,relatime,devices \u2502 \u2502 \u251c\u2500/sys/fs/cgroup/cpuset cgroup cgroup rw,nosuid,nodev,noexec,relatime,cpuset \u2502 \u2502 \u251c\u2500/sys/fs/cgroup/memory cgroup cgroup rw,nosuid,nodev,noexec,relatime,memory \u2502 \u2502 \u251c\u2500/sys/fs/cgroup/net_cls,net_prio cgroup cgroup rw,nosuid,nodev,noexec,relatime,net_cls,net_prio \u2502 \u2502 \u251c\u2500/sys/fs/cgroup/freezer cgroup cgroup rw,nosuid,nodev,noexec,relatime,freezer \u2502 \u2502 \u2514\u2500/sys/fs/cgroup/pids cgroup cgroup rw,nosuid,nodev,noexec,relatime,pids \u2502 \u251c\u2500/sys/fs/pstore pstore pstore rw,nosuid,nodev,noexec,relatime \u2502 \u251c\u2500/sys/kernel/debug debugfs debugfs rw,relatime \u2502 \u2514\u2500/sys/fs/fuse/connections fusectl fusectl rw,relatime \u251c\u2500/proc proc proc rw,nosuid,nodev,noexec,relatime \u2502 \u2514\u2500/proc/sys/fs/binfmt_misc systemd-1 autofs rw,relatime,fd=33,pgrp=1,timeout=0,minproto=5,maxproto=5,direct \u251c\u2500/dev udev devtmpfs rw,nosuid,relatime,size=500888k,nr_inodes=125222,mode=755 \u2502 \u251c\u2500/dev/pts devpts devpts rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=000 \u2502 \u251c\u2500/dev/shm tmpfs tmpfs rw,nosuid,nodev \u2502 \u251c\u2500/dev/hugepages hugetlbfs hugetlbfs rw,relatime \u2502 \u2514\u2500/dev/mqueue mqueue mqueue rw,relatime \u251c\u2500/run tmpfs tmpfs rw,nosuid,noexec,relatime,size=101596k,mode=755 \u2502 \u251c\u2500/run/lock tmpfs tmpfs rw,nosuid,nodev,noexec,relatime,size=5120k \u2502 \u2514\u2500/run/user/1000 tmpfs tmpfs rw,nosuid,nodev,relatime,size=101596k,mode=700,uid=1000,gid=1000 \u251c\u2500/var/lib/lxcfs lxcfs fuse.lxcfs rw,nosuid,nodev,relatime,user_id=0,group_id=0,allow_other \u2514\u2500/vagrant vagrant vboxsf rw,nodev,relatime","title":"Simple usage"},{"location":"findmnt/#output-as-keyvalue-pairs-per-device","text":"$ findmnt -P TARGET=\"/sys\" SOURCE=\"sysfs\" FSTYPE=\"sysfs\" OPTIONS=\"rw,nosuid,nodev,noexec,relatime\" TARGET=\"/proc\" SOURCE=\"proc\" FSTYPE=\"proc\" OPTIONS=\"rw,nosuid,nodev,noexec,relatime\" TARGET=\"/dev\" SOURCE=\"udev\" FSTYPE=\"devtmpfs\" OPTIONS=\"rw,nosuid,relatime,size=500888k,nr_inodes=125222,mode=755\" TARGET=\"/dev/pts\" SOURCE=\"devpts\" FSTYPE=\"devpts\" OPTIONS=\"rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=000\" TARGET=\"/run\" SOURCE=\"tmpfs\" FSTYPE=\"tmpfs\" OPTIONS=\"rw,nosuid,noexec,relatime,size=101596k,mode=755\" TARGET=\"/\" SOURCE=\"/dev/sda1\" FSTYPE=\"ext4\" OPTIONS=\"rw,relatime,data=ordered\" TARGET=\"/sys/kernel/security\" SOURCE=\"securityfs\" FSTYPE=\"securityfs\" OPTIONS=\"rw,nosuid,nodev,noexec,relatime\" TARGET=\"/dev/shm\" SOURCE=\"tmpfs\" FSTYPE=\"tmpfs\" OPTIONS=\"rw,nosuid,nodev\" TARGET=\"/run/lock\" SOURCE=\"tmpfs\" FSTYPE=\"tmpfs\" OPTIONS=\"rw,nosuid,nodev,noexec,relatime,size=5120k\" TARGET=\"/sys/fs/cgroup\" SOURCE=\"tmpfs\" FSTYPE=\"tmpfs\" OPTIONS=\"ro,nosuid,nodev,noexec,mode=755\" TARGET=\"/sys/fs/cgroup/systemd\" SOURCE=\"cgroup\" FSTYPE=\"cgroup\" OPTIONS=\"rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/lib/systemd/systemd-cgroups-agent,name=systemd\" TARGET=\"/sys/fs/pstore\" SOURCE=\"pstore\" FSTYPE=\"pstore\" OPTIONS=\"rw,nosuid,nodev,noexec,relatime\" TARGET=\"/sys/fs/cgroup/net_cls,net_prio\" SOURCE=\"cgroup\" FSTYPE=\"cgroup\" OPTIONS=\"rw,nosuid,nodev,noexec,relatime,net_cls,net_prio\" TARGET=\"/sys/fs/cgroup/perf_event\" SOURCE=\"cgroup\" FSTYPE=\"cgroup\" OPTIONS=\"rw,nosuid,nodev,noexec,relatime,perf_event\" TARGET=\"/sys/fs/cgroup/cpu,cpuacct\" SOURCE=\"cgroup\" FSTYPE=\"cgroup\" OPTIONS=\"rw,nosuid,nodev,noexec,relatime,cpu,cpuacct\" TARGET=\"/sys/fs/cgroup/hugetlb\" SOURCE=\"cgroup\" FSTYPE=\"cgroup\" OPTIONS=\"rw,nosuid,nodev,noexec,relatime,hugetlb\" TARGET=\"/sys/fs/cgroup/memory\" SOURCE=\"cgroup\" FSTYPE=\"cgroup\" OPTIONS=\"rw,nosuid,nodev,noexec,relatime,memory\" TARGET=\"/sys/fs/cgroup/devices\" SOURCE=\"cgroup\" FSTYPE=\"cgroup\" OPTIONS=\"rw,nosuid,nodev,noexec,relatime,devices\" TARGET=\"/sys/fs/cgroup/freezer\" SOURCE=\"cgroup\" FSTYPE=\"cgroup\" OPTIONS=\"rw,nosuid,nodev,noexec,relatime,freezer\" TARGET=\"/sys/fs/cgroup/cpuset\" SOURCE=\"cgroup\" FSTYPE=\"cgroup\" OPTIONS=\"rw,nosuid,nodev,noexec,relatime,cpuset\" TARGET=\"/sys/fs/cgroup/blkio\" SOURCE=\"cgroup\" FSTYPE=\"cgroup\" OPTIONS=\"rw,nosuid,nodev,noexec,relatime,blkio\" TARGET=\"/sys/fs/cgroup/pids\" SOURCE=\"cgroup\" FSTYPE=\"cgroup\" OPTIONS=\"rw,nosuid,nodev,noexec,relatime,pids\" TARGET=\"/proc/sys/fs/binfmt_misc\" SOURCE=\"systemd-1\" FSTYPE=\"autofs\" OPTIONS=\"rw,relatime,fd=26,pgrp=1,timeout=0,minproto=5,maxproto=5,direct\" TARGET=\"/sys/kernel/debug\" SOURCE=\"debugfs\" FSTYPE=\"debugfs\" OPTIONS=\"rw,relatime\" TARGET=\"/dev/hugepages\" SOURCE=\"hugetlbfs\" FSTYPE=\"hugetlbfs\" OPTIONS=\"rw,relatime\" TARGET=\"/dev/mqueue\" SOURCE=\"mqueue\" FSTYPE=\"mqueue\" OPTIONS=\"rw,relatime\" TARGET=\"/sys/fs/fuse/connections\" SOURCE=\"fusectl\" FSTYPE=\"fusectl\" OPTIONS=\"rw,relatime\" TARGET=\"/var/lib/lxcfs\" SOURCE=\"lxcfs\" FSTYPE=\"fuse.lxcfs\" OPTIONS=\"rw,nosuid,nodev,relatime,user_id=0,group_id=0,allow_other\" TARGET=\"/vagrant\" SOURCE=\"vagrant\" FSTYPE=\"vboxsf\" OPTIONS=\"rw,nodev,relatime\" TARGET=\"/run/user/1000\" SOURCE=\"tmpfs\" FSTYPE=\"tmpfs\" OPTIONS=\"rw,nosuid,nodev,relatime,size=101596k,mode=700,uid=1000,gid=1000\"","title":"Output as key/value pairs per device"},{"location":"findmnt/#output-as-json","text":"$ findmnt -J { \"filesystems\": [ {\"target\": \"/\", \"source\": \"/dev/sda1\", \"fstype\": \"ext4\", \"options\": \"rw,relatime,data=ordered\", \"children\": [ {\"target\": \"/sys\", \"source\": \"sysfs\", \"fstype\": \"sysfs\", \"options\": \"rw,nosuid,nodev,noexec,relatime\", \"children\": [ {\"target\": \"/sys/kernel/security\", \"source\": \"securityfs\", \"fstype\": \"securityfs\", \"options\": \"rw,nosuid,nodev,noexec,relatime\"}, {\"target\": \"/sys/fs/cgroup\", \"source\": \"tmpfs\", \"fstype\": \"tmpfs\", \"options\": \"ro,nosuid,nodev,noexec,mode=755\", \"children\": [ {\"target\": \"/sys/fs/cgroup/systemd\", \"source\": \"cgroup\", \"fstype\": \"cgroup\", \"options\": \"rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/lib/systemd/systemd-cgroups-agent,name=systemd\"}, {\"target\": \"/sys/fs/cgroup/net_cls,net_prio\", \"source\": \"cgroup\", \"fstype\": \"cgroup\", \"options\": \"rw,nosuid,nodev,noexec,relatime,net_cls,net_prio\"}, {\"target\": \"/sys/fs/cgroup/perf_event\", \"source\": \"cgroup\", \"fstype\": \"cgroup\", \"options\": \"rw,nosuid,nodev,noexec,relatime,perf_event\"}, {\"target\": \"/sys/fs/cgroup/cpu,cpuacct\", \"source\": \"cgroup\", \"fstype\": \"cgroup\", \"options\": \"rw,nosuid,nodev,noexec,relatime,cpu,cpuacct\"}, {\"target\": \"/sys/fs/cgroup/hugetlb\", \"source\": \"cgroup\", \"fstype\": \"cgroup\", \"options\": \"rw,nosuid,nodev,noexec,relatime,hugetlb\"}, {\"target\": \"/sys/fs/cgroup/memory\", \"source\": \"cgroup\", \"fstype\": \"cgroup\", \"options\": \"rw,nosuid,nodev,noexec,relatime,memory\"}, {\"target\": \"/sys/fs/cgroup/devices\", \"source\": \"cgroup\", \"fstype\": \"cgroup\", \"options\": \"rw,nosuid,nodev,noexec,relatime,devices\"}, {\"target\": \"/sys/fs/cgroup/freezer\", \"source\": \"cgroup\", \"fstype\": \"cgroup\", \"options\": \"rw,nosuid,nodev,noexec,relatime,freezer\"}, {\"target\": \"/sys/fs/cgroup/cpuset\", \"source\": \"cgroup\", \"fstype\": \"cgroup\", \"options\": \"rw,nosuid,nodev,noexec,relatime,cpuset\"}, {\"target\": \"/sys/fs/cgroup/blkio\", \"source\": \"cgroup\", \"fstype\": \"cgroup\", \"options\": \"rw,nosuid,nodev,noexec,relatime,blkio\"}, {\"target\": \"/sys/fs/cgroup/pids\", \"source\": \"cgroup\", \"fstype\": \"cgroup\", \"options\": \"rw,nosuid,nodev,noexec,relatime,pids\"} ] }, {\"target\": \"/sys/fs/pstore\", \"source\": \"pstore\", \"fstype\": \"pstore\", \"options\": \"rw,nosuid,nodev,noexec,relatime\"}, {\"target\": \"/sys/kernel/debug\", \"source\": \"debugfs\", \"fstype\": \"debugfs\", \"options\": \"rw,relatime\"}, {\"target\": \"/sys/fs/fuse/connections\", \"source\": \"fusectl\", \"fstype\": \"fusectl\", \"options\": \"rw,relatime\"} ] }, {\"target\": \"/proc\", \"source\": \"proc\", \"fstype\": \"proc\", \"options\": \"rw,nosuid,nodev,noexec,relatime\", \"children\": [ {\"target\": \"/proc/sys/fs/binfmt_misc\", \"source\": \"systemd-1\", \"fstype\": \"autofs\", \"options\": \"rw,relatime,fd=26,pgrp=1,timeout=0,minproto=5,maxproto=5,direct\"} ] }, {\"target\": \"/dev\", \"source\": \"udev\", \"fstype\": \"devtmpfs\", \"options\": \"rw,nosuid,relatime,size=500888k,nr_inodes=125222,mode=755\", \"children\": [ {\"target\": \"/dev/pts\", \"source\": \"devpts\", \"fstype\": \"devpts\", \"options\": \"rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=000\"}, {\"target\": \"/dev/shm\", \"source\": \"tmpfs\", \"fstype\": \"tmpfs\", \"options\": \"rw,nosuid,nodev\"}, {\"target\": \"/dev/hugepages\", \"source\": \"hugetlbfs\", \"fstype\": \"hugetlbfs\", \"options\": \"rw,relatime\"}, {\"target\": \"/dev/mqueue\", \"source\": \"mqueue\", \"fstype\": \"mqueue\", \"options\": \"rw,relatime\"} ] }, {\"target\": \"/run\", \"source\": \"tmpfs\", \"fstype\": \"tmpfs\", \"options\": \"rw,nosuid,noexec,relatime,size=101596k,mode=755\", \"children\": [ {\"target\": \"/run/lock\", \"source\": \"tmpfs\", \"fstype\": \"tmpfs\", \"options\": \"rw,nosuid,nodev,noexec,relatime,size=5120k\"}, {\"target\": \"/run/user/1000\", \"source\": \"tmpfs\", \"fstype\": \"tmpfs\", \"options\": \"rw,nosuid,nodev,relatime,size=101596k,mode=700,uid=1000,gid=1000\"} ] }, {\"target\": \"/var/lib/lxcfs\", \"source\": \"lxcfs\", \"fstype\": \"fuse.lxcfs\", \"options\": \"rw,nosuid,nodev,relatime,user_id=0,group_id=0,allow_other\"}, {\"target\": \"/vagrant\", \"source\": \"vagrant\", \"fstype\": \"vboxsf\", \"options\": \"rw,nodev,relatime\"}, ] } ] }","title":"Output as JSON"},{"location":"findmnt/#see-also","text":"lsblk","title":"See also"},{"location":"flask/","text":"\"Flask is a microframework for Python based on Werkzeug, Jinja 2 and good intentions.\" - http://flask.pocoo.org/ https://blog.miguelgrinberg.com/post/the-flask-mega-tutorial-part-i-hello-world","title":"flask"},{"location":"fluentd/","text":"\"Fluentd is an open source data collector for unified logging layer.\" - https://www.fluentd.org/","title":"fluentd"},{"location":"fortune/","text":"fortune is a unix command that displays a random fortune on the CLI. Make a fortune file Create a file that has each fortune separated by a line containing only a % symbol. Run strfile fortunes.txt , which will create fortunes.txt.dat You can then see the fortunes with fortune fortunes.txt . This also works with many files in a single directory: for file in *.txt ; do strfile \"${file}\" ; done ; fortune .","title":"fortune"},{"location":"fortune/#make-a-fortune-file","text":"Create a file that has each fortune separated by a line containing only a % symbol. Run strfile fortunes.txt , which will create fortunes.txt.dat You can then see the fortunes with fortune fortunes.txt . This also works with many files in a single directory: for file in *.txt ; do strfile \"${file}\" ; done ; fortune .","title":"Make a fortune file"},{"location":"genicam/","text":"\"The Generic Interface for Cameras standard is the base for plug & play handling of cameras and devices.\" - http://www.emva.org/standards-technology/genicam/","title":"GenICam"},{"location":"geodata/","text":"http://geojson.org https://tools.ietf.org/html/rfc7946 https://www.gpsbabel.org","title":"geodata"},{"location":"gige-vision/","text":"\"GigE Vision is an interface standard introduced in 2006 for high-performance industrial cameras. It provides a framework for transmitting high-speed video and related control data over Ethernet networks. The distribution of software or development, manufacture or sale of hardware that implement the standard, require the payment of annual licensing fees.\" - https://en.wikipedia.org/wiki/GigE_Vision Technology GigE Vision Control Protocol (GVCP) GigE Vision Stream Protocol (GVSP) GigE Device Discovery Mechanism See also http://www.emva.org/standards-technology/genicam/","title":"GigE Vision"},{"location":"gige-vision/#technology","text":"GigE Vision Control Protocol (GVCP) GigE Vision Stream Protocol (GVSP) GigE Device Discovery Mechanism","title":"Technology"},{"location":"gige-vision/#see-also","text":"http://www.emva.org/standards-technology/genicam/","title":"See also"},{"location":"git/","text":"baddass version control Links http://gitref.org/ http://repo.or.cz/w/git.git/blob/HEAD:/contrib/completion/git-completion.bash http://git-scm.com/book/en/Getting-Started-Git-Basics http://git-scm.com/book/en/Git-on-the-Server-Setting-Up-the-Server http://scottchacon.com/2011/08/31/github-flow.html https://guides.github.com/introduction/flow/index.html http://nvie.com/posts/a-successful-git-branching-model/ https://training.github.com/kit/intermediate/ https://github.com/git/git/tree/master/Documentation/RelNotes https://www.gitignore.io/ Examples git init Create a git repository for the CWD git init echo \"\" >> .gitignore echo \"# Ignore other unneeded files. *.swp *~ .DS_Store\" >> .gitignore git clone Clone a local repo git clone /path/to/repo Clone a remote git repo via ssh git clone user@ssh_server:/opt/git/project git filesystem operations Add everything in the CWD to the git repo git add . Rename a file in the git repo This also renames the filesystem file. git mv README.rdoc README.md Delete a file from the repo git rm filename git status Check the status of git git status git commit Commit the current changes git commit -m \"Initial commit\" Commit all changes with commit -a git commit -a -m \"Improve the README file\" Skip git commit hooks git commit --no-verify git tag https://git-scm.com/book/en/v2/Git-Basics-Tagging Git supports two types of tags: lightweight and annotated. Create an annotated tag Annotated tags, are stored as full objects in the Git database. git tag -m \"Improve X and Y.\" v0.5.3 Create a light tag This is basically DNS A records for git SHAs. The SHA is referenced by the tag, no other info is stored. Using them is generally frowned upon because tags tend to be used where context is important, so the annotations that go with an annotated tag are more suitable. git tag v0.5.3 Delete a local tag git tag -d v0.5.3 Delete a remote tag git push --delete origin v0.5.3 git config git config interacts with configs. There are three scopes: --local, --global, --system. Local = per-repo settings. IE: stored in .git/config directory for the repo Global = per-user settings. IE: stored in ~/.gitconfig System = per-system settings, found in /etc/ or wherever git is looking for system settings. Git client setup This creates and modifies ~/.gitconfig with some parameters: git config --global user.name \"Daniel Hoherd\" git config --global user.email daniel.hoherd@gmail.com git config --global alias.co checkout git config --global core.editor \"vim\" git config --global merge.tool vimdiff git config --global log.date iso Edit a .git/config file with some params git config --replace-all svn-remote.svn.url https://svn.example.com/ops/ git config --replace-all svn-remote.svn.fetch ops:refs/remotes/trunk git config --add svn-remote.svn.preserve-empty-dirs true git config --unset svn-remote.svn.branches git config --unset svn-remote.svn.tags git config --add svn.authorsfile /srv-cluster/git-svn/git/author.txt Export your .gitconfig in a git config friendly format git config --list git diff Show differences between objects and stuff. diff between staged and committed This is useful when you're adding files that were not previously in the repo alongside changes to existing files, since a bare git diff before adding the files will only show changes to files that were already in the repo. git diff --staged diff that shows per-word colored differences git diff --color-words Machine readable word diff git diff --word-diff Diff and ignore whitespace This does not ignore line ending changes or blank line insertion and removals. git diff -w Show diffs between master and a given date git diff $(git rev-list -n1 --before=\"1 month ago\" master) Show what has changed since a point in time git whatchanged --since=\"18 hours ago\" -p or... git whatchanged --since=\"18 hours ago\" --until=\"6 hours ago\" -p git blame git blame shows information about the commit associated with each line of a file. Simple usage git blame <filename> Show non-whitespace changes in blame When somebody has reformatted code but didn't make any code changes, this will show the prior commits where something more than whitespace changed. git blame -w <filename> git log Shows commit history. View the commit history git log Show one log entry git log -1 Show git commits that contain a given string This searches the content of the diff, not the commit message. git log -S search_string Show commit messages that match a given regex git log --grep='[Ww]hitespace' Show logs for a given dir in the last 3 days git log --since=3.days modules/profile_sensu Show raw log history for 5 most recent commits Useful for seeing TZ settings. git log --format=raw -5 Really pretty logs log --graph --oneline --decorate --all git show Show the changes from a specific SHA git show f73f9ec7c07e Show a complete file as of a given SHA This is an absolute path from the git root, not relative to CWD. This command will show the whole file as of the given SHA. git show f73f9ec7c07e:dir/filename.yaml git branches Branches are an integral part of git. They allow you to work on distinct changes without mixing them all up together. Create a branch git checkout -b readme-fix Check which branch you're in git branch Rename (move) a branch git branch -m oldname newname git merge This lets you merge two branches. Merge branch with master git checkout master git merge readme-fix-branch git branch -d readme-fix-branch disable fast-forward merges You can control how the history is kept when merging. By default, fast-forward merges occur, which replays the commits on the branch that is being merged into. By disabling this you can see several commits being merged from one branch into another, making it easier to roll back that whole series of commits without digging through the history to see where each commit from the branch came from. git config --global merge.ff false remotes Add a remote git remote add upstream https://github.com/danielhoherd/homepass Push to a specific remote # push to the master branch of the remote named upstream git push upstream master Alter the source of origin If you move your repo to another location, use this command to change the upstream URL: git remote set-url origin https://user@newhost/newpath/reponame git reset git reset allows you to reset your state to what it was at a previous point. Reset to a prior state based on what has been done locally The reflog is a log of what steps have been performed locally. You can view the reflog, then reset to a prior state. git reflog # show all HEAD changes git reset --hard 45e0ae5 # reset all git tracked state to 45e0ae5 Alternately, you can use a date: git reflog --date=iso # absolute date based reflog references git reset \"HEAD@{2015-03-25 14:45:30 -0700}\" --hard Hard reset of local changes This will abandon all local changes and resolve merge conflicts git fetch origin git reset --hard origin/master git clean Remove all untracked files and directories This is useful after your reset to a prior state. It deletes all files and directories that show up in the untracked section of git status git clean -ffdx Misc tricks Refresh all Git repos in a path find /var/www/html/mediawiki/ -name .git | while read -r X ; do pushd \"$(dirname \"${X}\")\" && \\ [ $(git remote -v | wc -l) -gt 0 ] && \\ git pull && \\ popd ; done ; Show a numbered list of remote branches sorted by last commit date git branch -r | grep -v HEAD | xargs -r -n1 git log -1 \\ --pretty=format:'%ad %h%d %an | %s %n' --date=iso -1 | sort | nl -ba Branch cleanup git gc --prune=now git remote prune origin","title":"Git"},{"location":"git/#links","text":"http://gitref.org/ http://repo.or.cz/w/git.git/blob/HEAD:/contrib/completion/git-completion.bash http://git-scm.com/book/en/Getting-Started-Git-Basics http://git-scm.com/book/en/Git-on-the-Server-Setting-Up-the-Server http://scottchacon.com/2011/08/31/github-flow.html https://guides.github.com/introduction/flow/index.html http://nvie.com/posts/a-successful-git-branching-model/ https://training.github.com/kit/intermediate/ https://github.com/git/git/tree/master/Documentation/RelNotes https://www.gitignore.io/","title":"Links"},{"location":"git/#examples","text":"","title":"Examples"},{"location":"git/#git-init","text":"","title":"git init"},{"location":"git/#create-a-git-repository-for-the-cwd","text":"git init echo \"\" >> .gitignore echo \"# Ignore other unneeded files. *.swp *~ .DS_Store\" >> .gitignore","title":"Create a git repository for the CWD"},{"location":"git/#git-clone","text":"","title":"git clone"},{"location":"git/#clone-a-local-repo","text":"git clone /path/to/repo","title":"Clone a local repo"},{"location":"git/#clone-a-remote-git-repo-via-ssh","text":"git clone user@ssh_server:/opt/git/project","title":"Clone a remote git repo via ssh"},{"location":"git/#git-filesystem-operations","text":"","title":"git filesystem operations"},{"location":"git/#add-everything-in-the-cwd-to-the-git-repo","text":"git add .","title":"Add everything in the CWD to the git repo"},{"location":"git/#rename-a-file-in-the-git-repo","text":"This also renames the filesystem file. git mv README.rdoc README.md","title":"Rename a file in the git repo"},{"location":"git/#delete-a-file-from-the-repo","text":"git rm filename","title":"Delete a file from the repo"},{"location":"git/#git-status","text":"","title":"git status"},{"location":"git/#check-the-status-of-git","text":"git status","title":"Check the status of git"},{"location":"git/#git-commit","text":"","title":"git commit"},{"location":"git/#commit-the-current-changes","text":"git commit -m \"Initial commit\"","title":"Commit the current changes"},{"location":"git/#commit-all-changes-with-commit-a","text":"git commit -a -m \"Improve the README file\"","title":"Commit all changes with commit -a"},{"location":"git/#skip-git-commit-hooks","text":"git commit --no-verify","title":"Skip git commit hooks"},{"location":"git/#git-tag","text":"https://git-scm.com/book/en/v2/Git-Basics-Tagging Git supports two types of tags: lightweight and annotated.","title":"git tag"},{"location":"git/#create-an-annotated-tag","text":"Annotated tags, are stored as full objects in the Git database. git tag -m \"Improve X and Y.\" v0.5.3","title":"Create an annotated tag"},{"location":"git/#create-a-light-tag","text":"This is basically DNS A records for git SHAs. The SHA is referenced by the tag, no other info is stored. Using them is generally frowned upon because tags tend to be used where context is important, so the annotations that go with an annotated tag are more suitable. git tag v0.5.3","title":"Create a light tag"},{"location":"git/#delete-a-local-tag","text":"git tag -d v0.5.3","title":"Delete a local tag"},{"location":"git/#delete-a-remote-tag","text":"git push --delete origin v0.5.3","title":"Delete a remote tag"},{"location":"git/#git-config","text":"git config interacts with configs. There are three scopes: --local, --global, --system. Local = per-repo settings. IE: stored in .git/config directory for the repo Global = per-user settings. IE: stored in ~/.gitconfig System = per-system settings, found in /etc/ or wherever git is looking for system settings.","title":"git config"},{"location":"git/#git-client-setup","text":"This creates and modifies ~/.gitconfig with some parameters: git config --global user.name \"Daniel Hoherd\" git config --global user.email daniel.hoherd@gmail.com git config --global alias.co checkout git config --global core.editor \"vim\" git config --global merge.tool vimdiff git config --global log.date iso","title":"Git client setup"},{"location":"git/#edit-a-gitconfig-file-with-some-params","text":"git config --replace-all svn-remote.svn.url https://svn.example.com/ops/ git config --replace-all svn-remote.svn.fetch ops:refs/remotes/trunk git config --add svn-remote.svn.preserve-empty-dirs true git config --unset svn-remote.svn.branches git config --unset svn-remote.svn.tags git config --add svn.authorsfile /srv-cluster/git-svn/git/author.txt","title":"Edit a .git/config file with some params"},{"location":"git/#export-your-gitconfig-in-a-git-config-friendly-format","text":"git config --list","title":"Export your .gitconfig in a git config friendly format"},{"location":"git/#git-diff","text":"Show differences between objects and stuff.","title":"git diff"},{"location":"git/#diff-between-staged-and-committed","text":"This is useful when you're adding files that were not previously in the repo alongside changes to existing files, since a bare git diff before adding the files will only show changes to files that were already in the repo. git diff --staged","title":"diff between staged and committed"},{"location":"git/#diff-that-shows-per-word-colored-differences","text":"git diff --color-words","title":"diff that shows per-word colored differences"},{"location":"git/#machine-readable-word-diff","text":"git diff --word-diff","title":"Machine readable word diff"},{"location":"git/#diff-and-ignore-whitespace","text":"This does not ignore line ending changes or blank line insertion and removals. git diff -w","title":"Diff and ignore whitespace"},{"location":"git/#show-diffs-between-master-and-a-given-date","text":"git diff $(git rev-list -n1 --before=\"1 month ago\" master)","title":"Show diffs between master and a given date"},{"location":"git/#show-what-has-changed-since-a-point-in-time","text":"git whatchanged --since=\"18 hours ago\" -p or... git whatchanged --since=\"18 hours ago\" --until=\"6 hours ago\" -p","title":"Show what has changed since a point in time"},{"location":"git/#git-blame","text":"git blame shows information about the commit associated with each line of a file.","title":"git blame"},{"location":"git/#simple-usage","text":"git blame <filename>","title":"Simple usage"},{"location":"git/#show-non-whitespace-changes-in-blame","text":"When somebody has reformatted code but didn't make any code changes, this will show the prior commits where something more than whitespace changed. git blame -w <filename>","title":"Show non-whitespace changes in blame"},{"location":"git/#git-log","text":"Shows commit history.","title":"git log"},{"location":"git/#view-the-commit-history","text":"git log","title":"View the commit history"},{"location":"git/#show-one-log-entry","text":"git log -1","title":"Show one log entry"},{"location":"git/#show-git-commits-that-contain-a-given-string","text":"This searches the content of the diff, not the commit message. git log -S search_string","title":"Show git commits that contain a given string"},{"location":"git/#show-commit-messages-that-match-a-given-regex","text":"git log --grep='[Ww]hitespace'","title":"Show commit messages that match a given regex"},{"location":"git/#show-logs-for-a-given-dir-in-the-last-3-days","text":"git log --since=3.days modules/profile_sensu","title":"Show logs for a given dir in the last 3 days"},{"location":"git/#show-raw-log-history-for-5-most-recent-commits","text":"Useful for seeing TZ settings. git log --format=raw -5","title":"Show raw log history for 5 most recent commits"},{"location":"git/#really-pretty-logs","text":"log --graph --oneline --decorate --all","title":"Really pretty logs"},{"location":"git/#git-show","text":"","title":"git show"},{"location":"git/#show-the-changes-from-a-specific-sha","text":"git show f73f9ec7c07e","title":"Show the changes from a specific SHA"},{"location":"git/#show-a-complete-file-as-of-a-given-sha","text":"This is an absolute path from the git root, not relative to CWD. This command will show the whole file as of the given SHA. git show f73f9ec7c07e:dir/filename.yaml","title":"Show a complete file as of a given SHA"},{"location":"git/#git-branches","text":"Branches are an integral part of git. They allow you to work on distinct changes without mixing them all up together.","title":"git branches"},{"location":"git/#create-a-branch","text":"git checkout -b readme-fix","title":"Create a branch"},{"location":"git/#check-which-branch-youre-in","text":"git branch","title":"Check which branch you're in"},{"location":"git/#rename-move-a-branch","text":"git branch -m oldname newname","title":"Rename (move) a branch"},{"location":"git/#git-merge","text":"This lets you merge two branches.","title":"git merge"},{"location":"git/#merge-branch-with-master","text":"git checkout master git merge readme-fix-branch git branch -d readme-fix-branch","title":"Merge branch with master"},{"location":"git/#disable-fast-forward-merges","text":"You can control how the history is kept when merging. By default, fast-forward merges occur, which replays the commits on the branch that is being merged into. By disabling this you can see several commits being merged from one branch into another, making it easier to roll back that whole series of commits without digging through the history to see where each commit from the branch came from. git config --global merge.ff false","title":"disable fast-forward merges"},{"location":"git/#remotes","text":"","title":"remotes"},{"location":"git/#add-a-remote","text":"git remote add upstream https://github.com/danielhoherd/homepass","title":"Add a remote"},{"location":"git/#push-to-a-specific-remote","text":"# push to the master branch of the remote named upstream git push upstream master","title":"Push to a specific remote"},{"location":"git/#alter-the-source-of-origin","text":"If you move your repo to another location, use this command to change the upstream URL: git remote set-url origin https://user@newhost/newpath/reponame","title":"Alter the source of origin"},{"location":"git/#git-reset","text":"git reset allows you to reset your state to what it was at a previous point.","title":"git reset"},{"location":"git/#reset-to-a-prior-state-based-on-what-has-been-done-locally","text":"The reflog is a log of what steps have been performed locally. You can view the reflog, then reset to a prior state. git reflog # show all HEAD changes git reset --hard 45e0ae5 # reset all git tracked state to 45e0ae5 Alternately, you can use a date: git reflog --date=iso # absolute date based reflog references git reset \"HEAD@{2015-03-25 14:45:30 -0700}\" --hard","title":"Reset to a prior state based on what has been done locally"},{"location":"git/#hard-reset-of-local-changes","text":"This will abandon all local changes and resolve merge conflicts git fetch origin git reset --hard origin/master","title":"Hard reset of local changes"},{"location":"git/#git-clean","text":"","title":"git clean"},{"location":"git/#remove-all-untracked-files-and-directories","text":"This is useful after your reset to a prior state. It deletes all files and directories that show up in the untracked section of git status git clean -ffdx","title":"Remove all untracked files and directories"},{"location":"git/#misc-tricks","text":"","title":"Misc tricks"},{"location":"git/#refresh-all-git-repos-in-a-path","text":"find /var/www/html/mediawiki/ -name .git | while read -r X ; do pushd \"$(dirname \"${X}\")\" && \\ [ $(git remote -v | wc -l) -gt 0 ] && \\ git pull && \\ popd ; done ;","title":"Refresh all Git repos in a path"},{"location":"git/#show-a-numbered-list-of-remote-branches-sorted-by-last-commit-date","text":"git branch -r | grep -v HEAD | xargs -r -n1 git log -1 \\ --pretty=format:'%ad %h%d %an | %s %n' --date=iso -1 | sort | nl -ba","title":"Show a numbered list of remote branches sorted by last commit date"},{"location":"git/#branch-cleanup","text":"git gc --prune=now git remote prune origin","title":"Branch cleanup"},{"location":"gitlab/","text":"\"A single application for the complete DevOps lifecycle\" - https://about.gitlab.com Examples Skip CI via git push option Works in git >= 2.10: git push -o ci.skip Run CI steps locally Using Gitlab Runner you can run stages of your CI pipeline locally. EG: if you have docker installed, you can run the following command to run the build step of your pipeline: gitlab-runner exec docker build The gitlab-runner command has good inline options. Skip CI via commit message https://docs.gitlab.com/ce/ci/yaml/#skipping-jobs #!/usr/bin/env bash # Skip CI if all changed files are inside dir \"foo/\" set -x regex='^foo\\/' files=( $(git diff --cached --name-only --diff-filter=ACM ) ) for X in \"${files[@]}\" ; do # If any file is not inside dir foo exit 0 if [[ ! \"$X\" =~ $regex ]] ; then exit 0 fi done # If we've made it here, all changed files are inside dir foo/ # so we append '[no ci]' to commit message to skip CI in Gitlab echo \"[no ci]\" >> \"$1\" Run privileged mode gitlab-runners in GKE This is needed to run docker commands, which is useful for building containers, running ansible molecule, etc.. The runners.tags includes ${ORG_NAME} which is great for making sure jobs run on your own runners instead of publicly shared runners. This is important because DOCKER_HOST is different in Kubernetes than it is on public dind runners. export REGISTRATION_TOKEN=\"foobar\" export ORG_NAME=\"acme\" helm \\ upgrade gitlab-runner gitlab/gitlab-runner \\ --tiller-namespace gitlab-managed-apps \\ --name \"gitlab-runner-${ORG_NAME}-$(date +%s)\" \\ --set 'runners.privileged=true' \\ --set \"runners.tags=${ORG_NAME}\\,dind\\,docker\\,gke\" \\ --set 'gitlabUrl=https://gitlab.com/' \\ --set \"runnerRegistrationToken=${REGISTRATION_TOKEN}\" the runners.privileged=true is the magic that is needed to enable docker commands in your .gitlab-ci.yml files to succeed. this --set flag creates the pod environment: spec: containers: env: - name: KUBERNETES_PRIVILEGED value: \"true\" Pros and cons Pros You can create a new repo by locally initializing a git repo, setting a remote_url to where you want your project to be, and pushing your code. The server gives you a notification that the project has been created and gives you a URL to it. Built in docker registry for every project Built in CI with on-prem runners Cons Push-button GKE is configured at the project level, not the group level, so setting up k8s runners is more involved than it could be. User permissions do not have a distinct group entity, they are managed by creating a project sub-group which functions as both a place to put code and a permission level. This shows up in a variety of places, and I suspect is the reason we cannot export groups over SAML. There is no command line tool equivalent to the hub command that github has, which makes it easier to script pull requests etc.. Terraform provider for Gitlab is pretty limited compared to Github. Links https://docs.gitlab.com/ce/administration/ https://docs.gitlab.com/ce/ci/ https://docs.gitlab.com/ce/workflow/gitlab_flow.html - Good git branching and review strategy for teams. https://docs.gitlab.com/ee/ci/docker/using_docker_build.html https://docs.gitlab.com/ee/ci/multi_project_pipelines.html https://docs.gitlab.com/ee/ci/variables/ https://medium.com/devopslinks/gitlab-pipeline-to-run-cross-multiple-projects-3563af5d6dca","title":"Gitlab"},{"location":"gitlab/#examples","text":"","title":"Examples"},{"location":"gitlab/#skip-ci-via-git-push-option","text":"Works in git >= 2.10: git push -o ci.skip","title":"Skip CI via git push option"},{"location":"gitlab/#run-ci-steps-locally","text":"Using Gitlab Runner you can run stages of your CI pipeline locally. EG: if you have docker installed, you can run the following command to run the build step of your pipeline: gitlab-runner exec docker build The gitlab-runner command has good inline options.","title":"Run CI steps locally"},{"location":"gitlab/#skip-ci-via-commit-message","text":"https://docs.gitlab.com/ce/ci/yaml/#skipping-jobs #!/usr/bin/env bash # Skip CI if all changed files are inside dir \"foo/\" set -x regex='^foo\\/' files=( $(git diff --cached --name-only --diff-filter=ACM ) ) for X in \"${files[@]}\" ; do # If any file is not inside dir foo exit 0 if [[ ! \"$X\" =~ $regex ]] ; then exit 0 fi done # If we've made it here, all changed files are inside dir foo/ # so we append '[no ci]' to commit message to skip CI in Gitlab echo \"[no ci]\" >> \"$1\"","title":"Skip CI via commit message"},{"location":"gitlab/#run-privileged-mode-gitlab-runners-in-gke","text":"This is needed to run docker commands, which is useful for building containers, running ansible molecule, etc.. The runners.tags includes ${ORG_NAME} which is great for making sure jobs run on your own runners instead of publicly shared runners. This is important because DOCKER_HOST is different in Kubernetes than it is on public dind runners. export REGISTRATION_TOKEN=\"foobar\" export ORG_NAME=\"acme\" helm \\ upgrade gitlab-runner gitlab/gitlab-runner \\ --tiller-namespace gitlab-managed-apps \\ --name \"gitlab-runner-${ORG_NAME}-$(date +%s)\" \\ --set 'runners.privileged=true' \\ --set \"runners.tags=${ORG_NAME}\\,dind\\,docker\\,gke\" \\ --set 'gitlabUrl=https://gitlab.com/' \\ --set \"runnerRegistrationToken=${REGISTRATION_TOKEN}\" the runners.privileged=true is the magic that is needed to enable docker commands in your .gitlab-ci.yml files to succeed. this --set flag creates the pod environment: spec: containers: env: - name: KUBERNETES_PRIVILEGED value: \"true\"","title":"Run privileged mode gitlab-runners in GKE"},{"location":"gitlab/#pros-and-cons","text":"","title":"Pros and cons"},{"location":"gitlab/#pros","text":"You can create a new repo by locally initializing a git repo, setting a remote_url to where you want your project to be, and pushing your code. The server gives you a notification that the project has been created and gives you a URL to it. Built in docker registry for every project Built in CI with on-prem runners","title":"Pros"},{"location":"gitlab/#cons","text":"Push-button GKE is configured at the project level, not the group level, so setting up k8s runners is more involved than it could be. User permissions do not have a distinct group entity, they are managed by creating a project sub-group which functions as both a place to put code and a permission level. This shows up in a variety of places, and I suspect is the reason we cannot export groups over SAML. There is no command line tool equivalent to the hub command that github has, which makes it easier to script pull requests etc.. Terraform provider for Gitlab is pretty limited compared to Github.","title":"Cons"},{"location":"gitlab/#links","text":"https://docs.gitlab.com/ce/administration/ https://docs.gitlab.com/ce/ci/ https://docs.gitlab.com/ce/workflow/gitlab_flow.html - Good git branching and review strategy for teams. https://docs.gitlab.com/ee/ci/docker/using_docker_build.html https://docs.gitlab.com/ee/ci/multi_project_pipelines.html https://docs.gitlab.com/ee/ci/variables/ https://medium.com/devopslinks/gitlab-pipeline-to-run-cross-multiple-projects-3563af5d6dca","title":"Links"},{"location":"gitolite/","text":"\"Gitolite allows you to setup git hosting on a central server, with fine-grained access control and many more powerful features.\" - http://gitolite.com Examples Get info about available repositories ssh git@gitserver info","title":"gitolite"},{"location":"gitolite/#examples","text":"","title":"Examples"},{"location":"gitolite/#get-info-about-available-repositories","text":"ssh git@gitserver info","title":"Get info about available repositories"},{"location":"glances/","text":"\"Glances is a cross-platform system monitoring tool written in Python.\" - https://nicolargo.github.io/glances/ See also Top variant list","title":"glances"},{"location":"glances/#see-also","text":"Top variant list","title":"See also"},{"location":"golang/","text":"\"Go is an open source programming language that makes it easy to build simple, reliable, and efficient software.\" - https://golang.org/ Links http://www.golangbootcamp.com/book/intro Examples View default go environment go env Build code found on github get github.com/solarkennedy/uq build github.com/solarkennedy/uq Links https://golang.org/cmd/go/ https://gopkg.in https://play.golang.org/","title":"golang"},{"location":"golang/#links","text":"http://www.golangbootcamp.com/book/intro","title":"Links"},{"location":"golang/#examples","text":"","title":"Examples"},{"location":"golang/#view-default-go-environment","text":"go env","title":"View default go environment"},{"location":"golang/#build-code-found-on-github","text":"get github.com/solarkennedy/uq build github.com/solarkennedy/uq","title":"Build code found on github"},{"location":"golang/#links_1","text":"https://golang.org/cmd/go/ https://gopkg.in https://play.golang.org/","title":"Links"},{"location":"google-cloud/","text":"\"Google Cloud SDK is a set of tools that you can use to manage resources and applications hosted on Google Cloud Platform. These include the gcloud, gsutil, and bq command line tools. The gcloud command-line tool is downloaded along with the Cloud SDK\" - https://cloud.google.com/sdk/docs/ gcloud CLI Examples List google cloud projects gcloud projects list Switch to a different project gcloud config set project <project_name> Grant a user permission to a docker registry gsutil iam ch 'user:user@example.com:objectViewer' 'gs://artifacts.example.appspot.com/' List google compute zones gcloud compute zones list List images available in Google Container Registry gcloud container images list Pull a docker container from Google Container Registry gcloud docker -- pull gcr.io/project-id/hello-world Control access to registries \"Container Registry uses a Cloud Storage bucket as the backend for serving container images. You can control who has access to your Container Registry images by adjusting permissions for the Cloud Storage bucket. Caution: Container Registry only recognizes permissions set on the Cloud Storage bucket. Container Registry will ignore permissions set on individual objects within the Cloud Storage bucket. You manage access control in Cloud Storage by using the GCP Console or the gsutil command-line tool. Refer to the gsutil acl and gsutil defacl documentation for more information.\" - https://cloud.google.com/container-registry/docs/access-control Authenticate a private GCR registry in kubernetes This is likely not copy/paste material, but the flow is generally correct. PARTNER=other_company PROJECT=\"our_company-$PARTNER\" USER=service-account-user-for-$PARTNER EMAIL=\"$USER@$PROJECT.iam.gserviceaccount.com\" gcloud iam service-accounts create $USER gcloud iam service-accounts keys create \\ --display-name \"$USER\" \\ --iam-account \"$EMAIL\" \\ key.json gcloud projects add-iam-policy-binding \"$PROJECT\" \\ --member \"serviceAccount:$EMAIL\" \\ --role \"roles/storage.objectAdmin\" done kubectl create secret \"docker-pull-$PROJECT\" \"$PROJECT\" \\ --docker-server \"https://gcr.io\" \\ --docker-username _json_key \\ --docker-email \"$EMAIL\" \\ --docker-password \"$(cat key.json)\" Then use the value of docker-pull-${PROJECT} as your ImagePullSecret . Links https://cloud.google.com/container-registry/docs/quickstart https://cloud.google.com/container-builder/docs/build-config https://cloud.google.com/container-builder/docs/create-custom-build-steps https://github.com/GoogleCloudPlatform/cloud-builders","title":"Google Cloud"},{"location":"google-cloud/#gcloud-cli-examples","text":"","title":"gcloud CLI Examples"},{"location":"google-cloud/#list-google-cloud-projects","text":"gcloud projects list","title":"List google cloud projects"},{"location":"google-cloud/#switch-to-a-different-project","text":"gcloud config set project <project_name>","title":"Switch to a different project"},{"location":"google-cloud/#grant-a-user-permission-to-a-docker-registry","text":"gsutil iam ch 'user:user@example.com:objectViewer' 'gs://artifacts.example.appspot.com/'","title":"Grant a user permission to a docker registry"},{"location":"google-cloud/#list-google-compute-zones","text":"gcloud compute zones list","title":"List google compute zones"},{"location":"google-cloud/#list-images-available-in-google-container-registry","text":"gcloud container images list","title":"List images available in Google Container Registry"},{"location":"google-cloud/#pull-a-docker-container-from-google-container-registry","text":"gcloud docker -- pull gcr.io/project-id/hello-world","title":"Pull a docker container from Google Container Registry"},{"location":"google-cloud/#control-access-to-registries","text":"\"Container Registry uses a Cloud Storage bucket as the backend for serving container images. You can control who has access to your Container Registry images by adjusting permissions for the Cloud Storage bucket. Caution: Container Registry only recognizes permissions set on the Cloud Storage bucket. Container Registry will ignore permissions set on individual objects within the Cloud Storage bucket. You manage access control in Cloud Storage by using the GCP Console or the gsutil command-line tool. Refer to the gsutil acl and gsutil defacl documentation for more information.\" - https://cloud.google.com/container-registry/docs/access-control","title":"Control access to registries"},{"location":"google-cloud/#authenticate-a-private-gcr-registry-in-kubernetes","text":"This is likely not copy/paste material, but the flow is generally correct. PARTNER=other_company PROJECT=\"our_company-$PARTNER\" USER=service-account-user-for-$PARTNER EMAIL=\"$USER@$PROJECT.iam.gserviceaccount.com\" gcloud iam service-accounts create $USER gcloud iam service-accounts keys create \\ --display-name \"$USER\" \\ --iam-account \"$EMAIL\" \\ key.json gcloud projects add-iam-policy-binding \"$PROJECT\" \\ --member \"serviceAccount:$EMAIL\" \\ --role \"roles/storage.objectAdmin\" done kubectl create secret \"docker-pull-$PROJECT\" \"$PROJECT\" \\ --docker-server \"https://gcr.io\" \\ --docker-username _json_key \\ --docker-email \"$EMAIL\" \\ --docker-password \"$(cat key.json)\" Then use the value of docker-pull-${PROJECT} as your ImagePullSecret .","title":"Authenticate a private GCR registry in kubernetes"},{"location":"google-cloud/#links","text":"https://cloud.google.com/container-registry/docs/quickstart https://cloud.google.com/container-builder/docs/build-config https://cloud.google.com/container-builder/docs/create-custom-build-steps https://github.com/GoogleCloudPlatform/cloud-builders","title":"Links"},{"location":"google-earth/","text":"\"Google Earth is a computer program that renders a 3D representation of Earth based on satellite imagery. The program maps the Earth by superimposing satellite images, aerial photography, and GIS data onto a 3D globe, allowing users to see cities and landscapes from various angles.\" - https://en.wikipedia.org/wiki/Google_Earth Links simplekml - \"The python package simplekml was created to generate kml (or kmz). It was designed to alleviate the burden of having to study KML in order to achieve anything worthwhile with it. If you have a simple understanding of the structure of KML, then simplekml is easy to run with and create usable KML.\" https://doarama.com","title":"Google Earth"},{"location":"google-earth/#links","text":"simplekml - \"The python package simplekml was created to generate kml (or kmz). It was designed to alleviate the burden of having to study KML in order to achieve anything worthwhile with it. If you have a simple understanding of the structure of KML, then simplekml is easy to run with and create usable KML.\" https://doarama.com","title":"Links"},{"location":"google-sheets/","text":"Links gspread - Python module for interacting with google spreadsheets Techniques Function list - https://support.google.com/docs/table/25273?hl=en Conditional Formatting Regex matching to color Colorize rows with conditional formatting by using an expression like this: =REGEXMATCH($E:$E, \"some_regex\") This regex is not anchored, so there is no need to prepend or append .* Cell references in this case are relative unless prepended by a \\$ . So, if you want to match the cell you are working on you would use A1:A1 . Color every other row =MOD(ROW(),2) Import an RSS feed =IMPORTFEED(\"https://api.flickr.com/services/feeds/photos_public.gne\", B2, TRUE, 10) Sum lines that match a string This uses syntax similar to a glob search, but uses ~ instead of \\ =COUNTIF(D:D,\"3*\") Automatically resolve the DOW from a date =CHOOSE( weekday(A4), \"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\")","title":"Google Sheets"},{"location":"google-sheets/#links","text":"gspread - Python module for interacting with google spreadsheets","title":"Links"},{"location":"google-sheets/#techniques","text":"Function list - https://support.google.com/docs/table/25273?hl=en","title":"Techniques"},{"location":"google-sheets/#conditional-formatting","text":"","title":"Conditional Formatting"},{"location":"google-sheets/#regex-matching-to-color","text":"Colorize rows with conditional formatting by using an expression like this: =REGEXMATCH($E:$E, \"some_regex\") This regex is not anchored, so there is no need to prepend or append .* Cell references in this case are relative unless prepended by a \\$ . So, if you want to match the cell you are working on you would use A1:A1 .","title":"Regex matching to color"},{"location":"google-sheets/#color-every-other-row","text":"=MOD(ROW(),2)","title":"Color every other row"},{"location":"google-sheets/#import-an-rss-feed","text":"=IMPORTFEED(\"https://api.flickr.com/services/feeds/photos_public.gne\", B2, TRUE, 10)","title":"Import an RSS feed"},{"location":"google-sheets/#sum-lines-that-match-a-string","text":"This uses syntax similar to a glob search, but uses ~ instead of \\ =COUNTIF(D:D,\"3*\")","title":"Sum lines that match a string"},{"location":"google-sheets/#automatically-resolve-the-dow-from-a-date","text":"=CHOOSE( weekday(A4), \"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\")","title":"Automatically resolve the DOW from a date"},{"location":"google/","text":"\"Google LLC is an American multinational technology company that specializes in Internet-related services and products.\" - https://en.wikipedia.org/wiki/Google Links Google App Script Extending Google Docs Document Service Web Store Developer Dashboard Service Accounts API Explorer","title":"Google"},{"location":"google/#links","text":"Google App Script Extending Google Docs Document Service Web Store Developer Dashboard Service Accounts API Explorer","title":"Links"},{"location":"graphicsmagick/","text":"\"GraphicsMagick is the swiss army knife of image processing.\" - http://www.graphicsmagick.org/ This software purports to be more favorable than ImageMagick . See Also exiftool imagemagick jpeginfo sips","title":"GraphicsMagick"},{"location":"graphicsmagick/#see-also","text":"exiftool imagemagick jpeginfo sips","title":"See Also"},{"location":"grub/","text":"\"GNU GRUB is a Multiboot boot loader. It was derived from GRUB, the GRand Unified Bootloader, which was originally designed and implemented by Erich Stefan Boleyn.\" - https://www.gnu.org/software/grub/ Examples Disable onboard frame buffer I used this configuration to get text mode linux to boot on a Mac Mini with a bad graphics card that would not load a desktop environment. The machine was locking up at boot in OS X. Hardware test would boot with a striped color anomalies, but would never finish. Ubuntu Xenial would not boot correctly even to text mode without these settings. In /etc/default/grub : GRUB_CMDLINE_LINUX_DEFAULT=\"video=vesafb:off nofb vga=normal nomodeset\" Notable commands, files and dirs /boot/grub/grub.cfg - The grub config that is actually used at boot /etc/grub.d - A directory with some of the configs that are combined to create /boot/grub/grub.cfg /etc/default/grub - Default grub options update-grub - Used to regenerate /boot/grub/grub.cfg grub-set-default - Used to configure the default menu entry during reboot, only for bare metal machines grub-set-default-legacy-ec2 - Used to configure the default menu entry on ec2 machines grub-set-default-legacy-ec2","title":"GNU GRUB"},{"location":"grub/#examples","text":"","title":"Examples"},{"location":"grub/#disable-onboard-frame-buffer","text":"I used this configuration to get text mode linux to boot on a Mac Mini with a bad graphics card that would not load a desktop environment. The machine was locking up at boot in OS X. Hardware test would boot with a striped color anomalies, but would never finish. Ubuntu Xenial would not boot correctly even to text mode without these settings. In /etc/default/grub : GRUB_CMDLINE_LINUX_DEFAULT=\"video=vesafb:off nofb vga=normal nomodeset\"","title":"Disable onboard frame buffer"},{"location":"grub/#notable-commands-files-and-dirs","text":"/boot/grub/grub.cfg - The grub config that is actually used at boot /etc/grub.d - A directory with some of the configs that are combined to create /boot/grub/grub.cfg /etc/default/grub - Default grub options update-grub - Used to regenerate /boot/grub/grub.cfg grub-set-default - Used to configure the default menu entry during reboot, only for bare metal machines grub-set-default-legacy-ec2 - Used to configure the default menu entry on ec2 machines grub-set-default-legacy-ec2","title":"Notable commands, files and dirs"},{"location":"hadoop/","text":"\"The Apache Hadoop software library is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models.\" - http://hadoop.apache.org/ Links Oozie - Oozie is a workflow scheduler system to manage Apache Hadoop jobs.","title":"Hadoop"},{"location":"hadoop/#links","text":"Oozie - Oozie is a workflow scheduler system to manage Apache Hadoop jobs.","title":"Links"},{"location":"handbrake/","text":"Handbrake is a tool for ripping DVD's into MPEG or AVI files. CLI Examples https://trac.handbrake.fr/wiki/CLIGuide Deinterlacing for iPad HandbrakeCLI -Z \"AppleTV\" --deinterlace fast --maxWidth 1024 -i infile -o outfile Show information about the source media for use with extended flags HandBrakeCLI -t 0 -i VIDEO_TS Generate a 1000 frame preview of the AppleTV preset --stop-at is relative to the start, so it describes the number of frames in the output. HandBrakeCLI -i 2046/VIDEO_TS/ --start-at frame:5000 --stop-at frame:1000 -o foo.mp4 -Z AppleTV Fix 4:3 aspect ratio with the expanded syntax of the AppleTV preset HandBrakeCLI \\ -e x264 \\ -q 20.0 \\ -a 1,1 \\ -E faac,ac3 \\ -B 160,160 \\ -6 dpl2,auto \\ -R 48,Auto -D 0.0,0.0 \\ -f mp4 \\ -4 \\ -X 960 \\ --loose-anamorphic \\ -m \\ -x cabac=0:ref=2:me=umh:b-adapt=2:weightb=0:trellis=0:weightp=0 \\ --custom-anamorphic \\ --pixel-aspect 4:3","title":"Handbrake"},{"location":"handbrake/#cli-examples","text":"https://trac.handbrake.fr/wiki/CLIGuide","title":"CLI Examples"},{"location":"handbrake/#deinterlacing-for-ipad","text":"HandbrakeCLI -Z \"AppleTV\" --deinterlace fast --maxWidth 1024 -i infile -o outfile","title":"Deinterlacing for iPad"},{"location":"handbrake/#show-information-about-the-source-media-for-use-with-extended-flags","text":"HandBrakeCLI -t 0 -i VIDEO_TS","title":"Show information about the source media for use with extended flags"},{"location":"handbrake/#generate-a-1000-frame-preview-of-the-appletv-preset","text":"--stop-at is relative to the start, so it describes the number of frames in the output. HandBrakeCLI -i 2046/VIDEO_TS/ --start-at frame:5000 --stop-at frame:1000 -o foo.mp4 -Z AppleTV","title":"Generate a 1000 frame preview of the AppleTV preset"},{"location":"handbrake/#fix-43-aspect-ratio-with-the-expanded-syntax-of-the-appletv-preset","text":"HandBrakeCLI \\ -e x264 \\ -q 20.0 \\ -a 1,1 \\ -E faac,ac3 \\ -B 160,160 \\ -6 dpl2,auto \\ -R 48,Auto -D 0.0,0.0 \\ -f mp4 \\ -4 \\ -X 960 \\ --loose-anamorphic \\ -m \\ -x cabac=0:ref=2:me=umh:b-adapt=2:weightb=0:trellis=0:weightp=0 \\ --custom-anamorphic \\ --pixel-aspect 4:3","title":"Fix 4:3 aspect ratio with the expanded syntax of the AppleTV preset"},{"location":"hashids/","text":"\"Hashids is a small open-source library that generates short, unique, non-sequential ids from numbers. It converts numbers like 347 into strings like 'yr8', or array of numbers like [27, 986] into '3kTMd'. You can also decode those ids back. This is useful in bundling several parameters into one or simply using them as short UIDs.\" - https://hashids.org/ Examples Python https://github.com/davidaurelio/hashids-python from hashids import Hashids hashids = Hashids(salt=\"this is my salt\") id = hashids.encode(1, 2, 3) # str('laHquq') numbers = hashids.decode(id)","title":"Hashids"},{"location":"hashids/#examples","text":"","title":"Examples"},{"location":"hashids/#python","text":"https://github.com/davidaurelio/hashids-python from hashids import Hashids hashids = Hashids(salt=\"this is my salt\") id = hashids.encode(1, 2, 3) # str('laHquq') numbers = hashids.decode(id)","title":"Python"},{"location":"helm/","text":"\"The Kubernetes Package Manager\" - https://github.com/kubernetes/helm Tips Show notes for a deployed service Notes are printed when you install a service, but they can be viewed again by running helm status <release_name> where is one of the releases from helm list . Install the incubator repo https://github.com/helm/charts#how-do-i-enable-the-incubator-repository helm repo add incubator https://kubernetes-charts-incubator.storage.googleapis.com/","title":"helm"},{"location":"helm/#tips","text":"","title":"Tips"},{"location":"helm/#show-notes-for-a-deployed-service","text":"Notes are printed when you install a service, but they can be viewed again by running helm status <release_name> where is one of the releases from helm list .","title":"Show notes for a deployed service"},{"location":"helm/#install-the-incubator-repo","text":"https://github.com/helm/charts#how-do-i-enable-the-incubator-repository helm repo add incubator https://kubernetes-charts-incubator.storage.googleapis.com/","title":"Install the incubator repo"},{"location":"home-assistant/","text":"\"Home Assistant is an open-source home automation platform running on Python 3. Track and control all devices at home and automate control.\" - https://home-assistant.io/ \"Hass.io turns your Raspberry Pi (or another device) into the ultimate home automation hub powered by Home Assistant. With Hass.io you can focus on integrating your devices and writing automations.\" - https://home-assistant.io/hassio/","title":"Home Assistant"},{"location":"hp/","text":"Information about HP Inc and Hewlett-Packard Company hardware and software. Links http://cciss.sourceforge.net - RAID software for linux iLO firmware upgrade is done from in linux using CP012567.scexe Procurve ethernet switches Microserver N40L or N54L for zfs or FreeNAS Custom Microserver BIOS: http://www.avforums.com/forums/networking-nas/1521657-hp-n36l-n40l-n54l-microserver-updated-ahci-bios-support.html","title":"HP"},{"location":"hp/#links","text":"http://cciss.sourceforge.net - RAID software for linux iLO firmware upgrade is done from in linux using CP012567.scexe Procurve ethernet switches Microserver N40L or N54L for zfs or FreeNAS Custom Microserver BIOS: http://www.avforums.com/forums/networking-nas/1521657-hp-n36l-n40l-n54l-microserver-updated-ahci-bios-support.html","title":"Links"},{"location":"htop/","text":"\"an interactive process viewer for Unix systems.\" - http://hisham.hm/htop/ See also Top variant list","title":"htop"},{"location":"htop/#see-also","text":"Top variant list","title":"See also"},{"location":"httpstat/","text":"\"curl statistics made simple\" - https://github.com/reorx/httpstat Usage Simple usage $ httpstat http://hoherd.com/ Connected to 192.30.252.153:80 from 127.0.0.1:61646 HTTP/1.1 200 OK Server: GitHub.com Date: Mon, 29 Jan 2018 23:24:52 GMT Content-Type: text/html; charset=utf-8 Content-Length: 405 Vary: Accept-Encoding Vary: Accept-Encoding Last-Modified: Tue, 04 Apr 2017 16:43:44 GMT Access-Control-Allow-Origin: * X-GitHub-Request-Id: F0D0:1973:5CF2FD:846C00:5A6FAD44 Expires: Mon, 29 Jan 2018 23:34:52 GMT Cache-Control: max-age=600 Accept-Ranges: bytes Body stored in: /var/folders/2t/rnzxpxd54y7832mx_xjvxl30bb2qzp/T/tmphVaBFx DNS Lookup TCP Connection Server Processing Content Transfer [ 5ms | 0ms | 237ms | 1ms ] | | | | namelookup:5ms | | | connect:5ms | | starttransfer:242ms | total:243ms See also aria2 curl - what httpstat wraps to get its stats httpstat - download and show a some useful connection information wget","title":"httpstat"},{"location":"httpstat/#usage","text":"","title":"Usage"},{"location":"httpstat/#simple-usage","text":"$ httpstat http://hoherd.com/ Connected to 192.30.252.153:80 from 127.0.0.1:61646 HTTP/1.1 200 OK Server: GitHub.com Date: Mon, 29 Jan 2018 23:24:52 GMT Content-Type: text/html; charset=utf-8 Content-Length: 405 Vary: Accept-Encoding Vary: Accept-Encoding Last-Modified: Tue, 04 Apr 2017 16:43:44 GMT Access-Control-Allow-Origin: * X-GitHub-Request-Id: F0D0:1973:5CF2FD:846C00:5A6FAD44 Expires: Mon, 29 Jan 2018 23:34:52 GMT Cache-Control: max-age=600 Accept-Ranges: bytes Body stored in: /var/folders/2t/rnzxpxd54y7832mx_xjvxl30bb2qzp/T/tmphVaBFx DNS Lookup TCP Connection Server Processing Content Transfer [ 5ms | 0ms | 237ms | 1ms ] | | | | namelookup:5ms | | | connect:5ms | | starttransfer:242ms | total:243ms","title":"Simple usage"},{"location":"httpstat/#see-also","text":"aria2 curl - what httpstat wraps to get its stats httpstat - download and show a some useful connection information wget","title":"See also"},{"location":"iTunes/","text":"https://github.com/liamks/pyitunes Metadata Skips are counted after within 2-10 seconds in iTunes, 3-10(?) seconds in iOS.","title":"iTunes"},{"location":"iTunes/#metadata","text":"Skips are counted after within 2-10 seconds in iTunes, 3-10(?) seconds in iOS.","title":"Metadata"},{"location":"iftop/","text":"\"display bandwidth usage on an interface\" - http://www.ex-parrot.com/pdw/iftop/ Network interface top shows network usage between hosts and by port numbers, but unfortunately not by process. For that, sysdig or iptraf . See also Top variant list","title":"iftop"},{"location":"iftop/#see-also","text":"Top variant list","title":"See also"},{"location":"image-formats/","text":"arw - Sony Alpha raw format cr2 - Canon raw format dng - Adobe digital negative gif - Compuserve Graphics Interchange Format jpg - Most common image format on the internet nef - Nikon raw format png - Lossless rasterized image format tiff - Lossless rasterized image format webp - https://en.wikipedia.org/wiki/WebP","title":"Graphical Image Formats"},{"location":"imagemagick/","text":"\"ImageMagick is a free and open-source software suite for displaying, converting, and editing raster image and vector image files. It can read and write over 200 image file formats.\" - https://en.wikipedia.org/wiki/ImageMagick ImageMagick is a framework for manipulating images. convert is the CLI front-end for it, and there are other modules/libraries for php, perl, etc.. Limitations Does '''NOT''' handle DNG files. In OS X use sips to do this. Techniques Overlay the date on the image for X in *.jpg ; do convert ${X} -font Times-Roman -pointsize 70 -fill black -annotate +100+100 %[exif:DateTimeOriginal] ${X}-date.jpg done for X in *date.jpg ; do convert ${X} -font Times-Roman -pointsize 70 -fill white -annotate +98+98 %[exif:DateTimeOriginal] ${X}-date2.jpg done Delete all tags and metadata convert -strip infile.jpg outfile.jpg Generate blurry, dark terminal backgrounds from normal backgrounds for X in * ; do convert -resize 1100x1100 \"${X}\" -blur 0x4 -fill black -colorize 75% terminal.\"${X}\" done Generate shady versions of desktop pictures in OS X for X in /Library/Desktop\\ Pictures/*.jpg ; do IMG=$(basename \"${X}\") convert -resize 1100x1100 \"${X}\" -blur 0x4 -set option:modulate:colorspace hsb -modulate 20 ~/Pictures/terminal.\"${IMG}\" done Crop the center of images out for X in /Volumes/data-b/Timelapse/20120407-14* ; do convert \"${X}\" -gravity Center -crop 1920x1080+0+0 $(basename ${X}) done Average many photos for a long-exposure style shot convert *.jpg -average average.jpg Multiply several images convert *.jpg -background white -compose multiply -flatten multiply.jpg Combine images always using the minimum value convert *.jpg -background white -compose darken -flatten minimum.jpg Combine images always using the maximum value convert *.jpg -background black -compose lighten -flatten maximum.jpg Swap red and blue channels (for IR photos) convert infile.jpg -separate -swap 0,2 -combine swapped.jpg Animate some images convert -delay 20 -loop 0 *.jpg animation.gif See Also exiftool graphicsmagick jpeginfo sips","title":"ImageMagick"},{"location":"imagemagick/#limitations","text":"Does '''NOT''' handle DNG files. In OS X use sips to do this.","title":"Limitations"},{"location":"imagemagick/#techniques","text":"","title":"Techniques"},{"location":"imagemagick/#overlay-the-date-on-the-image","text":"for X in *.jpg ; do convert ${X} -font Times-Roman -pointsize 70 -fill black -annotate +100+100 %[exif:DateTimeOriginal] ${X}-date.jpg done for X in *date.jpg ; do convert ${X} -font Times-Roman -pointsize 70 -fill white -annotate +98+98 %[exif:DateTimeOriginal] ${X}-date2.jpg done","title":"Overlay the date on the image"},{"location":"imagemagick/#delete-all-tags-and-metadata","text":"convert -strip infile.jpg outfile.jpg","title":"Delete all tags and metadata"},{"location":"imagemagick/#generate-blurry-dark-terminal-backgrounds-from-normal-backgrounds","text":"for X in * ; do convert -resize 1100x1100 \"${X}\" -blur 0x4 -fill black -colorize 75% terminal.\"${X}\" done","title":"Generate blurry, dark terminal backgrounds from normal backgrounds"},{"location":"imagemagick/#generate-shady-versions-of-desktop-pictures-in-os-x","text":"for X in /Library/Desktop\\ Pictures/*.jpg ; do IMG=$(basename \"${X}\") convert -resize 1100x1100 \"${X}\" -blur 0x4 -set option:modulate:colorspace hsb -modulate 20 ~/Pictures/terminal.\"${IMG}\" done","title":"Generate shady versions of desktop pictures in OS X"},{"location":"imagemagick/#crop-the-center-of-images-out","text":"for X in /Volumes/data-b/Timelapse/20120407-14* ; do convert \"${X}\" -gravity Center -crop 1920x1080+0+0 $(basename ${X}) done","title":"Crop the center of images out"},{"location":"imagemagick/#average-many-photos-for-a-long-exposure-style-shot","text":"convert *.jpg -average average.jpg","title":"Average many photos for a long-exposure style shot"},{"location":"imagemagick/#multiply-several-images","text":"convert *.jpg -background white -compose multiply -flatten multiply.jpg","title":"Multiply several images"},{"location":"imagemagick/#combine-images-always-using-the-minimum-value","text":"convert *.jpg -background white -compose darken -flatten minimum.jpg","title":"Combine images always using the minimum value"},{"location":"imagemagick/#combine-images-always-using-the-maximum-value","text":"convert *.jpg -background black -compose lighten -flatten maximum.jpg","title":"Combine images always using the maximum value"},{"location":"imagemagick/#swap-red-and-blue-channels-for-ir-photos","text":"convert infile.jpg -separate -swap 0,2 -combine swapped.jpg","title":"Swap red and blue channels (for IR photos)"},{"location":"imagemagick/#animate-some-images","text":"convert -delay 20 -loop 0 *.jpg animation.gif","title":"Animate some images"},{"location":"imagemagick/#see-also","text":"exiftool graphicsmagick jpeginfo sips","title":"See Also"},{"location":"img2xterm/","text":"Converts images into xterm 256 color output for viewing when there is no graphical display. Unfortuntely as of 2016-09-07 I can't find this in an easily distributable package. https://github.com/rossy/img2xterm","title":"img2xterm"},{"location":"innotop/","text":"\"innotop is a 'top' clone for MySQL with many features and flexibility.\" - https://github.com/innotop/innotop See also Top variant list","title":"innotop"},{"location":"innotop/#see-also","text":"Top variant list","title":"See also"},{"location":"inotify/","text":"\"inotify - monitoring file system events\" - man inotify \"inotifywatch - gather filesystem access statistics using inotify\" - man inotifywatch \"The inotify cron daemon (incrond) is a daemon which monitors filesystem events and executes commands defined in system and user tables. It's (sic) use is generally similar to cron(8).\" - man incrond Examples Continuously show filesystem events on a file This shows a datestamp when /var/log/syslog is modified. Theoretically we could use %N to get millisecond precision, but it doesn't work. sudo sudo inotifywait -m --timefmt '%F %T.%z' --format '%T %w %e %f' /var/log/syslog","title":"inotify"},{"location":"inotify/#examples","text":"","title":"Examples"},{"location":"inotify/#continuously-show-filesystem-events-on-a-file","text":"This shows a datestamp when /var/log/syslog is modified. Theoretically we could use %N to get millisecond precision, but it doesn't work. sudo sudo inotifywait -m --timefmt '%F %T.%z' --format '%T %w %e %f' /var/log/syslog","title":"Continuously show filesystem events on a file"},{"location":"internet/","text":"Health https://map.internetintel.oracle.com/ http://www.internettrafficreport.com/ https://outage.report/ https://www.slac.stanford.edu/comp/net/wan-mon/netmon.html - Many more relevant links","title":"The Internet"},{"location":"internet/#health","text":"https://map.internetintel.oracle.com/ http://www.internettrafficreport.com/ https://outage.report/ https://www.slac.stanford.edu/comp/net/wan-mon/netmon.html - Many more relevant links","title":"Health"},{"location":"iotop/","text":"iotop tracks disk I/O by process, and prints a summary report that is refreshed every interval. Linux Examples Show cumulative stats for processes actually using IO iotop -oa See also Top variant list","title":"iotop"},{"location":"iotop/#linux-examples","text":"","title":"Linux Examples"},{"location":"iotop/#show-cumulative-stats-for-processes-actually-using-io","text":"iotop -oa","title":"Show cumulative stats for processes actually using IO"},{"location":"iotop/#see-also","text":"Top variant list","title":"See also"},{"location":"ip/","text":"This is about the ip command in Linux. Examples The commands here can be shortened like cisco or other network device commands. Get the default interface ip route get 8.8.8.8 Show IP neighbors ip neighbor show Show all configured IP addresses This is more reliable than ifconfig, which sometimes omits entries. ip -f inet addr Show information about eth0 This shows information about eth0 and all the virtual interfaces brought up on the physical interface. ip addr show eth0 Monitor IP changes ip mon all Show interfaces that would route to a given network address ip addr show to 10.1.8.0/24 Show negotiated speeds for all interfaces ip -o link show | awk -F: '{print $2}' | while read -r X ; do sudo ethtool ${X} | egrep 'Settings|Speed' | xargs echo done | column -t -s: Add a static route ip route add 192.168.100.0/24 via 0.0.0.0 dev eth0 Set MTU for a specific route http://lartc.org/howto/lartc.cookbook.mtu-discovery.html ip route add default via 10.0.0.1 mtu 296","title":"ip"},{"location":"ip/#examples","text":"The commands here can be shortened like cisco or other network device commands.","title":"Examples"},{"location":"ip/#get-the-default-interface","text":"ip route get 8.8.8.8","title":"Get the default interface"},{"location":"ip/#show-ip-neighbors","text":"ip neighbor show","title":"Show IP neighbors"},{"location":"ip/#show-all-configured-ip-addresses","text":"This is more reliable than ifconfig, which sometimes omits entries. ip -f inet addr","title":"Show all configured IP addresses"},{"location":"ip/#show-information-about-eth0","text":"This shows information about eth0 and all the virtual interfaces brought up on the physical interface. ip addr show eth0","title":"Show information about eth0"},{"location":"ip/#monitor-ip-changes","text":"ip mon all","title":"Monitor IP changes"},{"location":"ip/#show-interfaces-that-would-route-to-a-given-network-address","text":"ip addr show to 10.1.8.0/24","title":"Show interfaces that would route to a given network address"},{"location":"ip/#show-negotiated-speeds-for-all-interfaces","text":"ip -o link show | awk -F: '{print $2}' | while read -r X ; do sudo ethtool ${X} | egrep 'Settings|Speed' | xargs echo done | column -t -s:","title":"Show negotiated speeds for all interfaces"},{"location":"ip/#add-a-static-route","text":"ip route add 192.168.100.0/24 via 0.0.0.0 dev eth0","title":"Add a static route"},{"location":"ip/#set-mtu-for-a-specific-route","text":"http://lartc.org/howto/lartc.cookbook.mtu-discovery.html ip route add default via 10.0.0.1 mtu 296","title":"Set MTU for a specific route"},{"location":"iperf/","text":"\"iperf3: A TCP, UDP, and SCTP network bandwidth measurement tool\" - https://github.com/esnet/iperf Links Sample usage - https://fasterdata.es.net/performance-testing/network-troubleshooting-tools/iperf/ Example usage Example server (receiving side) iperf3 -s Example client (sending side) iperf3 -c remote-hostname --interval 0.5 --omit 2 --time 30 --set-mss 1460","title":"iperf"},{"location":"iperf/#links","text":"Sample usage - https://fasterdata.es.net/performance-testing/network-troubleshooting-tools/iperf/","title":"Links"},{"location":"iperf/#example-usage","text":"","title":"Example usage"},{"location":"iperf/#example-server-receiving-side","text":"iperf3 -s","title":"Example server (receiving side)"},{"location":"iperf/#example-client-sending-side","text":"iperf3 -c remote-hostname --interval 0.5 --omit 2 --time 30 --set-mss 1460","title":"Example client (sending side)"},{"location":"ipmi/","text":"The Intelligent Platform Management Interface (IPMI) is a set of computer interface specifications for an autonomous computer subsystem that provides management and monitoring capabilities independently of the host system's CPU, firmware (BIOS or UEFI) and operating system. Managing servers with IPMI Default Users The default users are 'Administrator' for HPs, 'root' for Dells, and 'ADMIN' for Silicon Mechanics. Server Setup IPMI uses COM2 aka ttyS1 for the serial port on Dells and HPs, COM3 aka ttyS2 on Silicon Mechanics. Common Remote Commands See if a server is on ipmitool -I lanplus -U $USER -P $PASSWORD -H $IPMI_IP chassis power status Turn a server on ipmitool -I lanplus -U $USER -P $PASSWORD -H $IPMI_IP chassis power on Turn a server off ipmitool -I lanplus -U $USER -P $PASSWORD -H $IPMI_IP chassis power off Tell a server to PXEBoot ipmitool -I lanplus -U $USER -P $PASSWORD -H $IPMI_IP chassis power off ipmitool -I lanplus -U $USER -P $PASSWORD -H $IPMI_IP chassis bootdev pxe ipmitool -I lanplus -U $USER -P $PASSWORD -H $IPMI_IP chassis power on Connect to the serial console ipmitool -I lanplus -U $USER -P $PASSWORD -H $IPMI_IP sol activate Display the system event log ipmitool -I lanplus -U $USER -P $PASSWORD -H $IPMI_IP sel list Clear the system event log ipmitool -I lanplus -U $USER -P $PASSWORD -H $IPMI_IP sel clear Display sensor information ipmitool -I lanplus -U $USER -P $PASSWORD -H $IPMI_IP sdr list Disconnect another serial console session ipmitool -I lanplus -U $USER -P $PASSWORD -H $IPMI_IP sol deactivate Show bootdev help ipmitool -H 10.5.8.30 -U ADMIN -P ADMIN chassis bootdev none options=help Common Local Commands These commands require root access in most environments. View all configured LAN parameters ipmitool lan print You can view individual \"channels\" which are logical interfaces by giving the number: ipmitool lan print 1 Configure IPMI static IP information IPMI_IP_ADDR=192.168.1.250 IPMI_IP_NETMASK=255.255.255.0 IPMI_IP_GW_IP_ADDR=192.168.1.1 IPMI_IP_GW_MAC_ADDR=00:01:02:aa:bb:cc ipmitool lan set 1 ipsrc static ipmitool lan set 1 ipaddr \"${IPMI_IP_ADDR}\" ipmitool lan set 1 netmask \"${IPMI_IP_NETMASK}\" ipmitool lan set 1 defgw ipaddr \"${IPMI_IP_GW_IP_ADDR}\" ipmitool lan set 1 defgw macaddr \"${IPMI_IP_GW_MAC_ADDR}\" ipmitool lan set 1 arp respond on Configure IPMI admin user USERNAME=admin PASSWORD=hunter2 ipmitool user set name 2 \"$USERNAME\" ipmitool user set password 2 \"$PASSWORD\" Reset the BMC If a host loses it's IPMI (iLO, etc.) IP connectivity, issue this command from the host itself ipmitool mc reset cold How to fix /dev/ipmi errors For errors like Could not open device at /dev/ipmi0 or /dev/ipmi/0 or /dev/ipmidev/0 : modprobe ipmi_msghandler modprobe ipmi_devintf modprobe ipmi_si Reset the ADMIN password to ADMIN on a Supermicro BMC sudo ipmitool -I open user set password 2 ADMIN Reset all settings to defaults This may not work with your BMC, but has been known to work with some supermicro BMCs. ipmitool raw 0x30 0x40 Conigure sol on a systemd server # cat /lib/systemd/system/ttyS1.service [Unit] Description=Serial Console Service: ttyS1 [Service] ExecStart=/sbin/getty -L 115200 ttyS1 vt102 Restart=always [Install] WantedBy=system.target # systemctl enable ttyS1.service # systemctl start ttyS1.service See Also http://www.intel.com/design/servers/ipmi/","title":"IPMI"},{"location":"ipmi/#managing-servers-with-ipmi","text":"","title":"Managing servers with IPMI"},{"location":"ipmi/#default-users","text":"The default users are 'Administrator' for HPs, 'root' for Dells, and 'ADMIN' for Silicon Mechanics.","title":"Default Users"},{"location":"ipmi/#server-setup","text":"IPMI uses COM2 aka ttyS1 for the serial port on Dells and HPs, COM3 aka ttyS2 on Silicon Mechanics.","title":"Server Setup"},{"location":"ipmi/#common-remote-commands","text":"","title":"Common Remote Commands"},{"location":"ipmi/#see-if-a-server-is-on","text":"ipmitool -I lanplus -U $USER -P $PASSWORD -H $IPMI_IP chassis power status","title":"See if a server is on"},{"location":"ipmi/#turn-a-server-on","text":"ipmitool -I lanplus -U $USER -P $PASSWORD -H $IPMI_IP chassis power on","title":"Turn a server on"},{"location":"ipmi/#turn-a-server-off","text":"ipmitool -I lanplus -U $USER -P $PASSWORD -H $IPMI_IP chassis power off","title":"Turn a server off"},{"location":"ipmi/#tell-a-server-to-pxeboot","text":"ipmitool -I lanplus -U $USER -P $PASSWORD -H $IPMI_IP chassis power off ipmitool -I lanplus -U $USER -P $PASSWORD -H $IPMI_IP chassis bootdev pxe ipmitool -I lanplus -U $USER -P $PASSWORD -H $IPMI_IP chassis power on","title":"Tell a server to PXEBoot"},{"location":"ipmi/#connect-to-the-serial-console","text":"ipmitool -I lanplus -U $USER -P $PASSWORD -H $IPMI_IP sol activate","title":"Connect to the serial console"},{"location":"ipmi/#display-the-system-event-log","text":"ipmitool -I lanplus -U $USER -P $PASSWORD -H $IPMI_IP sel list","title":"Display the system event log"},{"location":"ipmi/#clear-the-system-event-log","text":"ipmitool -I lanplus -U $USER -P $PASSWORD -H $IPMI_IP sel clear","title":"Clear the system event log"},{"location":"ipmi/#display-sensor-information","text":"ipmitool -I lanplus -U $USER -P $PASSWORD -H $IPMI_IP sdr list","title":"Display sensor information"},{"location":"ipmi/#disconnect-another-serial-console-session","text":"ipmitool -I lanplus -U $USER -P $PASSWORD -H $IPMI_IP sol deactivate","title":"Disconnect another serial console session"},{"location":"ipmi/#show-bootdev-help","text":"ipmitool -H 10.5.8.30 -U ADMIN -P ADMIN chassis bootdev none options=help","title":"Show bootdev help"},{"location":"ipmi/#common-local-commands","text":"These commands require root access in most environments.","title":"Common Local Commands"},{"location":"ipmi/#view-all-configured-lan-parameters","text":"ipmitool lan print You can view individual \"channels\" which are logical interfaces by giving the number: ipmitool lan print 1","title":"View all configured LAN parameters"},{"location":"ipmi/#configure-ipmi-static-ip-information","text":"IPMI_IP_ADDR=192.168.1.250 IPMI_IP_NETMASK=255.255.255.0 IPMI_IP_GW_IP_ADDR=192.168.1.1 IPMI_IP_GW_MAC_ADDR=00:01:02:aa:bb:cc ipmitool lan set 1 ipsrc static ipmitool lan set 1 ipaddr \"${IPMI_IP_ADDR}\" ipmitool lan set 1 netmask \"${IPMI_IP_NETMASK}\" ipmitool lan set 1 defgw ipaddr \"${IPMI_IP_GW_IP_ADDR}\" ipmitool lan set 1 defgw macaddr \"${IPMI_IP_GW_MAC_ADDR}\" ipmitool lan set 1 arp respond on","title":"Configure IPMI static IP information"},{"location":"ipmi/#configure-ipmi-admin-user","text":"USERNAME=admin PASSWORD=hunter2 ipmitool user set name 2 \"$USERNAME\" ipmitool user set password 2 \"$PASSWORD\"","title":"Configure IPMI admin user"},{"location":"ipmi/#reset-the-bmc","text":"If a host loses it's IPMI (iLO, etc.) IP connectivity, issue this command from the host itself ipmitool mc reset cold","title":"Reset the BMC"},{"location":"ipmi/#how-to-fix-devipmi-errors","text":"For errors like Could not open device at /dev/ipmi0 or /dev/ipmi/0 or /dev/ipmidev/0 : modprobe ipmi_msghandler modprobe ipmi_devintf modprobe ipmi_si","title":"How to fix /dev/ipmi errors"},{"location":"ipmi/#reset-the-admin-password-to-admin-on-a-supermicro-bmc","text":"sudo ipmitool -I open user set password 2 ADMIN","title":"Reset the ADMIN password to ADMIN on a Supermicro BMC"},{"location":"ipmi/#reset-all-settings-to-defaults","text":"This may not work with your BMC, but has been known to work with some supermicro BMCs. ipmitool raw 0x30 0x40","title":"Reset all settings to defaults"},{"location":"ipmi/#conigure-sol-on-a-systemd-server","text":"# cat /lib/systemd/system/ttyS1.service [Unit] Description=Serial Console Service: ttyS1 [Service] ExecStart=/sbin/getty -L 115200 ttyS1 vt102 Restart=always [Install] WantedBy=system.target # systemctl enable ttyS1.service # systemctl start ttyS1.service","title":"Conigure sol on a systemd server"},{"location":"ipmi/#see-also","text":"http://www.intel.com/design/servers/ipmi/","title":"See Also"},{"location":"iptables/","text":"iptables is the built-in linux firewall. Examples How to simulate a slow network link: http://blogs.kde.org/node/1878 Allow MySQL iptables -A INPUT -i eth0 -p tcp -m tcp --dport 3306 -j ACCEPT SSH blocking Better idea: fail2ban Basically, it lets people connect with SSH 5 times within a minute, but with a mandatory 5 second wait before connection attempts. Once they hit 5 attempts in a minute they get banned for an hour. Several IP ranges are exceptions where access is always allowed. iptables -I INPUT 1 -s 172.16.0.0/16 -m state --state NEW -p tcp -m tcp --dport 22 -j ACCEPT iptables -I INPUT 2 -s 17.1.2.0/27 -m state --state NEW -p tcp -m tcp --dport 22 -j ACCEPT iptables -I INPUT 3 -s 18.3.4.0/27 -m state --state NEW -p tcp -m tcp --dport 22 -j ACCEPT iptables -I INPUT 4 -s 19.5.6.0/24 -m state --state NEW -p tcp -m tcp --dport 22 -j ACCEPT iptables -N SSH iptables -N SSH_ABL iptables -A SSH -m recent --name SSH_ABL --update --seconds 3600 -j REJECT iptables -A SSH -m recent --name SSH --rcheck --seconds 60 --hitcount 5 -j SSH_ABL iptables -A SSH_ABL -m recent --name SSH_ABL --set -j LOG --log-level warn --log-prefix \"ABL: +SSH: \" iptables -A SSH_ABL -j REJECT iptables -A SSH -m recent --name SSH --rcheck --seconds 5 -j LOG --log-level warn --log-prefix \"RATE: \" iptables -A SSH -m recent --name SSH --update --seconds 5 -j REJECT iptables -A SSH -m recent --name SSH_ABL --remove -j LOG --log-level warn --log-prefix \"ABL: -SSH: \" iptables -A SSH -m recent --name SSH --set -j ACCEPT iptables -A INPUT -m state --state NEW -p tcp -m tcp --dport 22 -j SSH iptables -L Show all tables Not all tables are shown by default. To view all, issue the following commands as root: iptables -vL -t filter iptables -vL -t nat iptables -vL -t mangle iptables -vL -t raw iptables -vL -t security","title":"iptables"},{"location":"iptables/#examples","text":"How to simulate a slow network link: http://blogs.kde.org/node/1878","title":"Examples"},{"location":"iptables/#allow-mysql","text":"iptables -A INPUT -i eth0 -p tcp -m tcp --dport 3306 -j ACCEPT","title":"Allow MySQL"},{"location":"iptables/#ssh-blocking","text":"Better idea: fail2ban Basically, it lets people connect with SSH 5 times within a minute, but with a mandatory 5 second wait before connection attempts. Once they hit 5 attempts in a minute they get banned for an hour. Several IP ranges are exceptions where access is always allowed. iptables -I INPUT 1 -s 172.16.0.0/16 -m state --state NEW -p tcp -m tcp --dport 22 -j ACCEPT iptables -I INPUT 2 -s 17.1.2.0/27 -m state --state NEW -p tcp -m tcp --dport 22 -j ACCEPT iptables -I INPUT 3 -s 18.3.4.0/27 -m state --state NEW -p tcp -m tcp --dport 22 -j ACCEPT iptables -I INPUT 4 -s 19.5.6.0/24 -m state --state NEW -p tcp -m tcp --dport 22 -j ACCEPT iptables -N SSH iptables -N SSH_ABL iptables -A SSH -m recent --name SSH_ABL --update --seconds 3600 -j REJECT iptables -A SSH -m recent --name SSH --rcheck --seconds 60 --hitcount 5 -j SSH_ABL iptables -A SSH_ABL -m recent --name SSH_ABL --set -j LOG --log-level warn --log-prefix \"ABL: +SSH: \" iptables -A SSH_ABL -j REJECT iptables -A SSH -m recent --name SSH --rcheck --seconds 5 -j LOG --log-level warn --log-prefix \"RATE: \" iptables -A SSH -m recent --name SSH --update --seconds 5 -j REJECT iptables -A SSH -m recent --name SSH_ABL --remove -j LOG --log-level warn --log-prefix \"ABL: -SSH: \" iptables -A SSH -m recent --name SSH --set -j ACCEPT iptables -A INPUT -m state --state NEW -p tcp -m tcp --dport 22 -j SSH iptables -L","title":"SSH blocking"},{"location":"iptables/#show-all-tables","text":"Not all tables are shown by default. To view all, issue the following commands as root: iptables -vL -t filter iptables -vL -t nat iptables -vL -t mangle iptables -vL -t raw iptables -vL -t security","title":"Show all tables"},{"location":"jargon/","text":"AMoD: Autonomous Mobility on Demand bikeshedding : The term was coined as a metaphor to illuminate Parkinson's Law of Triviality. Parkinson observed that a committee whose job is to approve plans for a nuclear power plant may spend the majority of its time on relatively unimportant but easy-to-grasp issues, such as what materials to use for the staff bikeshed, while neglecting the design of the power plant itself, which is far more important but also far more difficult to criticize constructively. cargo cult : software containing elements that are included because of successful utilization elsewhere, unnecessary for the task at hand. Conway's Law : \"organizations which design systems ... are constrained to produce designs which are copies of the communication structures of these organizations.\" Cunningham's law : The best way to get the right answer on the Internet is not to ask a question, it's to post the wrong answer. cybernetics : \"Cybernetics\" comes from a Greek word meaning \"the art of steering\". Cybernetics is about having a goal and taking action to achieve that goal. [deterministic]: In mathematics and physics, a deterministic system is a system in which no randomness is involved in the development of future states of the system. A deterministic model will thus always produce the same output from a given starting condition or initial state. DRY : Don't Repeat Yourself. \"Every piece of knowledge must have a single, unambiguous, authoritative representation within a system\" idempotent : Idempotence is the property of certain operations in mathematics and computer science, that they can be applied multiple times without changing the result beyond the initial application. KISS : Keep It Simple, Stupid! martian packet : A Martian packet is an IP packet which specifies a source or destination address that is reserved for special-use by Internet Assigned Numbers Authority. monotonic : A function or set of values that always increases or always decreases. PRD : product requirements document teleology : Teleology or finality is a reason or explanation for something in function of its end, purpose, or goal. warrant canary : Text on a website that states the company or person has never been served with a secret government subpoena. Once the statement is removed, the users can assume the company or person has been served and has been told not to talk about it. YAGNI : a principle that states a programmer should not add functionality until deemed necessary. yak shaving : Any apparently useless activity which, by allowing you to overcome intermediate difficulties, allows you to solve a larger problem.","title":"jargon"},{"location":"jdupes/","text":"\"finds and performs actions upon duplicate files\" - man jdupes jdupes is based on fdupes and is not written in java as the name may lead you to assume. Examples Recursively find all duplicates in a dir jdupes -r /path/to/dir Create hard-links of any duplicate files jdupes -r -L /path/ Links https://github.com/jbruchon/jdupes","title":"jdupes"},{"location":"jdupes/#examples","text":"","title":"Examples"},{"location":"jdupes/#recursively-find-all-duplicates-in-a-dir","text":"jdupes -r /path/to/dir","title":"Recursively find all duplicates in a dir"},{"location":"jdupes/#create-hard-links-of-any-duplicate-files","text":"jdupes -r -L /path/","title":"Create hard-links of any duplicate files"},{"location":"jdupes/#links","text":"https://github.com/jbruchon/jdupes","title":"Links"},{"location":"jmespath/","text":"\"JMESPath is a query language for JSON.\" - http://jmespath.org Links http://jmespath.org/examples.html http://jmespath.org/tutorial.html https://github.com/jmespath/jmespath.terminal https://github.com/jmespath/jp Examples Grab some kubernetes fields and remap them to be less deep kubectl get po --all-namespaces -o json | jp \"items[*].{name: metadata.name, namespace: metadata.namespace, imagePullSecrets: spec.imagePullSecrets[*].name}\" Or filter only to non-default namespace where imagePullSecrets is populated kubectl get po --all-namespaces -o json | jp \"items[?metadata.namespace != 'default' && spec.imagePullSecrets != null].{name: metadata.name, namespace: metadata.namespace, imagePullSecrets: spec.imagePullSecrets[*].name}\"","title":"jmespath"},{"location":"jmespath/#links","text":"http://jmespath.org/examples.html http://jmespath.org/tutorial.html https://github.com/jmespath/jmespath.terminal https://github.com/jmespath/jp","title":"Links"},{"location":"jmespath/#examples","text":"","title":"Examples"},{"location":"jmespath/#grab-some-kubernetes-fields-and-remap-them-to-be-less-deep","text":"kubectl get po --all-namespaces -o json | jp \"items[*].{name: metadata.name, namespace: metadata.namespace, imagePullSecrets: spec.imagePullSecrets[*].name}\" Or filter only to non-default namespace where imagePullSecrets is populated kubectl get po --all-namespaces -o json | jp \"items[?metadata.namespace != 'default' && spec.imagePullSecrets != null].{name: metadata.name, namespace: metadata.namespace, imagePullSecrets: spec.imagePullSecrets[*].name}\"","title":"Grab some kubernetes fields and remap them to be less deep"},{"location":"jot/","text":"jot is a BSD CLI tool to generate sequences or random data, usually numbers. Tricks Generate 37 evenly space floating point numbers (with two significant digits) between -10 and +10 jot 37 -10 10.00 Generate two 12 alphanumeric random character passwords with rs jot -rc 24 48 123 | rs -g 0 12 Generate 5 capital alphanumeric strings jot -rc 500 48 90 | grep '[A-Z0-9]' | rs -g 5 32","title":"jot"},{"location":"jot/#tricks","text":"","title":"Tricks"},{"location":"jot/#generate-37-evenly-space-floating-point-numbers-with-two-significant-digits-between-10-and-10","text":"jot 37 -10 10.00","title":"Generate 37 evenly space floating point numbers (with two significant digits) between -10 and +10"},{"location":"jot/#generate-two-12-alphanumeric-random-character-passwords-with-rs","text":"jot -rc 24 48 123 | rs -g 0 12","title":"Generate two 12 alphanumeric random character passwords with rs"},{"location":"jot/#generate-5-capital-alphanumeric-strings","text":"jot -rc 500 48 90 | grep '[A-Z0-9]' | rs -g 5 32","title":"Generate 5 capital alphanumeric strings"},{"location":"jpeginfo/","text":"\"jpeginfo - prints information and tests integrity of JPEG/JFIF files.\" - man jpeginfo Example Test integrity of all files in a dir jpeginfo -c * Test integrity of files and delete any corrupt files jpeginfo -c -d * See Also exiftool graphicsmagick imagemagick sips","title":"jpeginfo"},{"location":"jpeginfo/#example","text":"","title":"Example"},{"location":"jpeginfo/#test-integrity-of-all-files-in-a-dir","text":"jpeginfo -c *","title":"Test integrity of all files in a dir"},{"location":"jpeginfo/#test-integrity-of-files-and-delete-any-corrupt-files","text":"jpeginfo -c -d *","title":"Test integrity of files and delete any corrupt files"},{"location":"jpeginfo/#see-also","text":"exiftool graphicsmagick imagemagick sips","title":"See Also"},{"location":"jq/","text":"\"jq is a lightweight and flexible command-line JSON processor.\" - https://stedolan.github.io/jq/ Examples Sort a json file jq -S . foo.json Grab first element of an array, and print the value of 'timestamp' of that element. echo ' [ { \"foo\": \"this is foo string\", \"timestamp\": \"this is the timestamp\" }, { \"second element\": \"second element value\" } ]' | jq '.[0].timestamp' Construct Flickr URLs from an API call curl -s \"https://api.flickr.com/services/rest/?\"\\ \"&api_key=9c72f03c0583a34bd703bd82d8773cc0\"\\ \"&format=json\"\\ \"&method=flickr.photos.getRecent\"\\ \"&nojsoncallback=1\" | jq -S ' .[\"photos\"][\"photo\"][] | \"https://flickr.com/photos/\" + .owner + \"/\" + .id ' Use mco to find packages of a certain version on a certain OS This example could be used as an alternative to grep, where only the value of a key/value pair is matched. mco rpc package status package=apt -j -F lsbdistcodename=trusty | jq -c ' .[] | select(.data.ensure == \"1.0.1ubuntu2\") | { version: .data.ensure, hostname: .sender } ' Print only objects whose name matches a string This example echoes some yaml, uses python to convert it to json, then filters matching data using jq . It could be used as an alternative to grep, where only the key of a key/value pair is matched. echo \" data: - This is a string, not an object, and contains the substrings foo and bar - name: foo_name value: foo_value - name: bar_name value: bar_value\" | python -c \"import yaml, sys, json; print json.dumps(yaml.safe_load(sys.stdin))\" | jq ' .[\"data\"][] | select(type==\"object\") | select (.name | . and contains(\"bar_n\")) ' Build a json entry from scratch This uses bash paramber expansion and subshell syntax, and may not work in other shells. create_json() { local user=${1:-${USER}} local host=${2:-${HOSTNAME}} local more_stuff=${3:-$(uname -a)} json=$( jq -c -n \\ --arg timestamp \"$(date \"+%F %T%z\")\" \\ --arg host \"${host}\" \\ --arg user \"${user}\" \\ --arg more_stuff \"${more_stuff}\" \\ '{ timestamp: $timestamp, host: $host, user: $user, more_stuff: $more_stuff }' ) echo \"$json\" } Render yaml with anchors as json data This example shows how you can use python and jq to view the result of dereferenced yaml anchors, a construct that is not supported by json. This example is less about how to use jq syntaxes, and more about how it can be used to view data that is otherwise difficult to sort through. echo \" job1: &template directory: /tmp extra_parameters: nosuid,noatime remote_host: 10.1.1.1 user: nobody job2: <<: *template remote_host: 10.2.2.2 job3: <<: *template remote_host: 10.3.3.3 \" | python -c \"import yaml, sys, json; print json.dumps(yaml.safe_load(sys.stdin))\" | jq -S . Select matches, and print a subset of values jq '.[] | select(.data.ensure != \"purged\") | [.sender,.data.ensure]' $* Output bare values for use as inputs This is a contrived example, the better way to get this info would be awless list instances --format tsv --columns name,privateip,launched $ awless list instances --format json | jq -r '.[] | \"\\(.Name) \\(.PrivateIP) \\(.Launched)\"' | column -t salt-master 172.18.9.48 2015-04-10T21:28:03Z consul-server-01 172.18.9.116 2015-05-15T06:13:19Z consul-server-02 172.18.9.117 2015-05-15T06:13:19Z consul-server-03 172.18.9.118 2015-05-15T06:13:19Z Show labels for each locally stored docker SHA docker images --format '{{.ID}}' | while read -r X ; do docker inspect $X | jq '.[] | [ .RepoTags, .Config.Labels ]' done See Also Tutorial","title":"jq"},{"location":"jq/#examples","text":"","title":"Examples"},{"location":"jq/#sort-a-json-file","text":"jq -S . foo.json","title":"Sort a json file"},{"location":"jq/#grab-first-element-of-an-array-and-print-the-value-of-timestamp-of-that-element","text":"echo ' [ { \"foo\": \"this is foo string\", \"timestamp\": \"this is the timestamp\" }, { \"second element\": \"second element value\" } ]' | jq '.[0].timestamp'","title":"Grab first element of an array, and print the value of 'timestamp' of that element."},{"location":"jq/#construct-flickr-urls-from-an-api-call","text":"curl -s \"https://api.flickr.com/services/rest/?\"\\ \"&api_key=9c72f03c0583a34bd703bd82d8773cc0\"\\ \"&format=json\"\\ \"&method=flickr.photos.getRecent\"\\ \"&nojsoncallback=1\" | jq -S ' .[\"photos\"][\"photo\"][] | \"https://flickr.com/photos/\" + .owner + \"/\" + .id '","title":"Construct Flickr URLs from an API call"},{"location":"jq/#use-mco-to-find-packages-of-a-certain-version-on-a-certain-os","text":"This example could be used as an alternative to grep, where only the value of a key/value pair is matched. mco rpc package status package=apt -j -F lsbdistcodename=trusty | jq -c ' .[] | select(.data.ensure == \"1.0.1ubuntu2\") | { version: .data.ensure, hostname: .sender } '","title":"Use mco to find packages of a certain version on a certain OS"},{"location":"jq/#print-only-objects-whose-name-matches-a-string","text":"This example echoes some yaml, uses python to convert it to json, then filters matching data using jq . It could be used as an alternative to grep, where only the key of a key/value pair is matched. echo \" data: - This is a string, not an object, and contains the substrings foo and bar - name: foo_name value: foo_value - name: bar_name value: bar_value\" | python -c \"import yaml, sys, json; print json.dumps(yaml.safe_load(sys.stdin))\" | jq ' .[\"data\"][] | select(type==\"object\") | select (.name | . and contains(\"bar_n\")) '","title":"Print only objects whose name matches a string"},{"location":"jq/#build-a-json-entry-from-scratch","text":"This uses bash paramber expansion and subshell syntax, and may not work in other shells. create_json() { local user=${1:-${USER}} local host=${2:-${HOSTNAME}} local more_stuff=${3:-$(uname -a)} json=$( jq -c -n \\ --arg timestamp \"$(date \"+%F %T%z\")\" \\ --arg host \"${host}\" \\ --arg user \"${user}\" \\ --arg more_stuff \"${more_stuff}\" \\ '{ timestamp: $timestamp, host: $host, user: $user, more_stuff: $more_stuff }' ) echo \"$json\" }","title":"Build a json entry from scratch"},{"location":"jq/#render-yaml-with-anchors-as-json-data","text":"This example shows how you can use python and jq to view the result of dereferenced yaml anchors, a construct that is not supported by json. This example is less about how to use jq syntaxes, and more about how it can be used to view data that is otherwise difficult to sort through. echo \" job1: &template directory: /tmp extra_parameters: nosuid,noatime remote_host: 10.1.1.1 user: nobody job2: <<: *template remote_host: 10.2.2.2 job3: <<: *template remote_host: 10.3.3.3 \" | python -c \"import yaml, sys, json; print json.dumps(yaml.safe_load(sys.stdin))\" | jq -S .","title":"Render yaml with anchors as json data"},{"location":"jq/#select-matches-and-print-a-subset-of-values","text":"jq '.[] | select(.data.ensure != \"purged\") | [.sender,.data.ensure]' $*","title":"Select matches, and print a subset of values"},{"location":"jq/#output-bare-values-for-use-as-inputs","text":"This is a contrived example, the better way to get this info would be awless list instances --format tsv --columns name,privateip,launched $ awless list instances --format json | jq -r '.[] | \"\\(.Name) \\(.PrivateIP) \\(.Launched)\"' | column -t salt-master 172.18.9.48 2015-04-10T21:28:03Z consul-server-01 172.18.9.116 2015-05-15T06:13:19Z consul-server-02 172.18.9.117 2015-05-15T06:13:19Z consul-server-03 172.18.9.118 2015-05-15T06:13:19Z","title":"Output bare values for use as inputs"},{"location":"jq/#see-also","text":"Tutorial","title":"See Also"},{"location":"json/","text":"\"JSON (JavaScript Object Notation) is a lightweight data-interchange format.\" - https://www.json.org/ See also toml - \"TOML aims to be a minimal configuration file format that's easy to read due to obvious semantics.\" yaml - Better human readability, more options. (JSON is actually a subset of yaml.) Links https://goessner.net/articles/JsonPath/index.html - JSONpath is used by kubernetes as a native way to restructure kubctl output. https://stedolan.github.io/jq/ - jq is generally useful for working with JSON in a shell. http://jmespath.org/ - jmespath is used in AWS APIs to restructure data.","title":"JSON"},{"location":"json/#see-also","text":"toml - \"TOML aims to be a minimal configuration file format that's easy to read due to obvious semantics.\" yaml - Better human readability, more options. (JSON is actually a subset of yaml.)","title":"See also"},{"location":"json/#links","text":"https://goessner.net/articles/JsonPath/index.html - JSONpath is used by kubernetes as a native way to restructure kubctl output. https://stedolan.github.io/jq/ - jq is generally useful for working with JSON in a shell. http://jmespath.org/ - jmespath is used in AWS APIs to restructure data.","title":"Links"},{"location":"jsonpath/","text":"\"XPath for JSON\" - https://goessner.net/articles/JsonPath/index.html Examples Show the API server of your current kubernetes context kubectl config view --minify -o=jsonpath='{.clusters[0].cluster.server}' Links http://jsonpath.com/ - online expression evaluator https://kubernetes.io/docs/reference/kubectl/jsonpath/","title":"JSONPath"},{"location":"jsonpath/#examples","text":"","title":"Examples"},{"location":"jsonpath/#show-the-api-server-of-your-current-kubernetes-context","text":"kubectl config view --minify -o=jsonpath='{.clusters[0].cluster.server}'","title":"Show the API server of your current kubernetes context"},{"location":"jsonpath/#links","text":"http://jsonpath.com/ - online expression evaluator https://kubernetes.io/docs/reference/kubectl/jsonpath/","title":"Links"},{"location":"juniper/","text":"\"We bring simplicity to networking with products, solutions, and services that connect the world.\" - https://www.juniper.net/us/en/company/ Examples Default credentials root: admin:abc123 Load configuration from terminal I have experienced errors when pasting into screen /dev/tty.usbserial , but having iTerm2 paste at 256 bytes per second appears to fix them. Amnesiac (ttyu0) login: root --- JUNOS 13.2X51-D35.3 built 2015-04-09 20:48:22 UTC root@:RE:0% cli {master:0} root> configure Entering configuration mode Users currently editing the configuration: autodconfig (pid 1291) on since 2018-01-06 17:32:28 UTC exclusive {master:0}[edit] root# load override terminal [Type ^D at a new line to end input] Then: paste your config press ctrl-d run commit Some example terminal buffer login: root --- JUNOS 13.2X51-D35.3 built 2015-04-09 20:48:22 UTC root@:RE:0% cli {master:0} root> set cli screen-length 75 Screen length set to 75 {master:0} root> configure Entering configuration mode {master:0}[edit] root# show ? Possible completions: <[Enter]> Execute this command > access Network access configuration > access-profile Access profile for this instance > accounting-options Accounting data configuration + apply-groups Groups from which to inherit configuration data > chassis Chassis configuration > class-of-service Class-of-service configuration > diameter Diameter protocol layer > event-options Event processing configuration > firewall Define a firewall configuration > forwarding-options Configure options to control packet forwarding > groups Configuration groups > interfaces Interface configuration > jsrc JSRC partition configuration > jsrc-partition JSRC partition configuration > multi-chassis > multicast-snooping-options Multicast snooping option configuration > poe Power-over-Ethernet options > policy-options Policy option configuration > protocols Routing protocol configuration > routing-instances Routing instance configuration > routing-options Protocol-independent routing option configuration > security Security configuration > services System services > snmp Simple Network Management Protocol configuration > switch-options Options for default routing-instance of type virtual-switch > system System parameters > unified-edge > virtual-chassis Virtual chassis configuration > vlans VLAN configuration | Pipe through a command {master:0}[edit] root# show chassis auto-image-upgrade; {master:0}[edit] Show forwarding table This is exactly the same as typing netstat -nr at the system shell. show route forwarding-table detail Remove virtual-chassis from an EX4300 request virtual-chassis vc-port delete pic-slot 1 port 0 request virtual-chassis vc-port delete pic-slot 1 port 1 request virtual-chassis vc-port delete pic-slot 1 port 2 request virtual-chassis vc-port delete pic-slot 1 port 3 You can validate this by looking for the following interfaces. If you do not see the et-0/1/{0..3} interfaces then the virtual-chassis may still exist in full or in part. root> show interfaces terse | match et- et-0/1/0 up up et-0/1/0.0 up up eth-switch et-0/1/1 up up et-0/1/1.0 up up eth-switch et-0/1/2 up up et-0/1/2.0 up up aenet --> ae0.0 et-0/1/3 up up et-0/1/3.0 up up aenet --> ae0.0 EX4300 OS install https://kb.juniper.net/InfoCenter/index?page=content&id=KB20551&cat=SWITCHING&actp=LIST#USB After copying the install image to a usb disk and inserting it into the EX4300 mount_msdosfs /dev/da1s1 /mnt cp /mnt/jinstall-ex-4300-14.1X53-D45.3-domestic-signed.tgz /var/tmp/ cli request system software add /var/tmp/jinstall-ex-4300-14.1X53-D45.3-domestic-signed.tgz Clear a DHCP client lease root@junos> show dhcp server binding IP address Session Id Hardware address Expires State Interface 10.8.52.6 2 00:1e:7c:f8:be:34 85166 BOUND irb.100 10.8.52.9 5 6c:c1:11:4e:52:8c 86299 BOUND irb.100 10.8.52.5 3 d8:fa:97:b8:1a:dd 85222 BOUND irb.100 {master:0} root@junos> clear dhcp server binding 2 {master:0} root@junos> show dhcp server binding IP address Session Id Hardware address Expires State Interface 10.8.52.9 5 6c:c1:11:4e:52:8c 86275 BOUND irb.100 10.8.52.5 3 d8:fa:97:b8:1a:dd 85198 BOUND irb.100 {master:0} Show config diff before commit You can view the changes that will be committed, which is useful when the commit is invalid and you need to inspect it. root# show | compare [edit access address-assignment pool p1 family inet] host server1-ipmi { ... } + host server2 { + hardware-address 00:11:22:33:44:33; + ip-address 172.17.1.6; + } + host server3 { + hardware-address 00:11:22:33:44:35; + ip-address 172.17.1.7; + } + host server4 { + hardware-address 00:11:22:33:44:1d; + ip-address 172.17.1.8; + } Abort config changes without committing If you made changes you want to abandon, while still in configure mode run 'rollback 0' {master:0}[edit] root@ex4300# rollback 0 load complete {master:0}[edit] root@voyage-van-3-ex4300# exit Exiting configuration mode Links https://www.juniper.net/documentation/en_US/junos/topics/concept/junos-cli-overview.html https://www.juniper.net/documentation/en_US/junos/topics/task/configuration/virtual-chassis-mx-series-vc-ports-deleting.html https://junos-ansible-modules.readthedocs.io/en/2.1.0/","title":"Juniper"},{"location":"juniper/#examples","text":"","title":"Examples"},{"location":"juniper/#default-credentials","text":"root: admin:abc123","title":"Default credentials"},{"location":"juniper/#load-configuration-from-terminal","text":"I have experienced errors when pasting into screen /dev/tty.usbserial , but having iTerm2 paste at 256 bytes per second appears to fix them. Amnesiac (ttyu0) login: root --- JUNOS 13.2X51-D35.3 built 2015-04-09 20:48:22 UTC root@:RE:0% cli {master:0} root> configure Entering configuration mode Users currently editing the configuration: autodconfig (pid 1291) on since 2018-01-06 17:32:28 UTC exclusive {master:0}[edit] root# load override terminal [Type ^D at a new line to end input] Then: paste your config press ctrl-d run commit","title":"Load configuration from terminal"},{"location":"juniper/#some-example-terminal-buffer","text":"login: root --- JUNOS 13.2X51-D35.3 built 2015-04-09 20:48:22 UTC root@:RE:0% cli {master:0} root> set cli screen-length 75 Screen length set to 75 {master:0} root> configure Entering configuration mode {master:0}[edit] root# show ? Possible completions: <[Enter]> Execute this command > access Network access configuration > access-profile Access profile for this instance > accounting-options Accounting data configuration + apply-groups Groups from which to inherit configuration data > chassis Chassis configuration > class-of-service Class-of-service configuration > diameter Diameter protocol layer > event-options Event processing configuration > firewall Define a firewall configuration > forwarding-options Configure options to control packet forwarding > groups Configuration groups > interfaces Interface configuration > jsrc JSRC partition configuration > jsrc-partition JSRC partition configuration > multi-chassis > multicast-snooping-options Multicast snooping option configuration > poe Power-over-Ethernet options > policy-options Policy option configuration > protocols Routing protocol configuration > routing-instances Routing instance configuration > routing-options Protocol-independent routing option configuration > security Security configuration > services System services > snmp Simple Network Management Protocol configuration > switch-options Options for default routing-instance of type virtual-switch > system System parameters > unified-edge > virtual-chassis Virtual chassis configuration > vlans VLAN configuration | Pipe through a command {master:0}[edit] root# show chassis auto-image-upgrade; {master:0}[edit]","title":"Some example terminal buffer"},{"location":"juniper/#show-forwarding-table","text":"This is exactly the same as typing netstat -nr at the system shell. show route forwarding-table detail","title":"Show forwarding table"},{"location":"juniper/#remove-virtual-chassis-from-an-ex4300","text":"request virtual-chassis vc-port delete pic-slot 1 port 0 request virtual-chassis vc-port delete pic-slot 1 port 1 request virtual-chassis vc-port delete pic-slot 1 port 2 request virtual-chassis vc-port delete pic-slot 1 port 3 You can validate this by looking for the following interfaces. If you do not see the et-0/1/{0..3} interfaces then the virtual-chassis may still exist in full or in part. root> show interfaces terse | match et- et-0/1/0 up up et-0/1/0.0 up up eth-switch et-0/1/1 up up et-0/1/1.0 up up eth-switch et-0/1/2 up up et-0/1/2.0 up up aenet --> ae0.0 et-0/1/3 up up et-0/1/3.0 up up aenet --> ae0.0","title":"Remove virtual-chassis from an EX4300"},{"location":"juniper/#ex4300-os-install","text":"https://kb.juniper.net/InfoCenter/index?page=content&id=KB20551&cat=SWITCHING&actp=LIST#USB After copying the install image to a usb disk and inserting it into the EX4300 mount_msdosfs /dev/da1s1 /mnt cp /mnt/jinstall-ex-4300-14.1X53-D45.3-domestic-signed.tgz /var/tmp/ cli request system software add /var/tmp/jinstall-ex-4300-14.1X53-D45.3-domestic-signed.tgz","title":"EX4300 OS install"},{"location":"juniper/#clear-a-dhcp-client-lease","text":"root@junos> show dhcp server binding IP address Session Id Hardware address Expires State Interface 10.8.52.6 2 00:1e:7c:f8:be:34 85166 BOUND irb.100 10.8.52.9 5 6c:c1:11:4e:52:8c 86299 BOUND irb.100 10.8.52.5 3 d8:fa:97:b8:1a:dd 85222 BOUND irb.100 {master:0} root@junos> clear dhcp server binding 2 {master:0} root@junos> show dhcp server binding IP address Session Id Hardware address Expires State Interface 10.8.52.9 5 6c:c1:11:4e:52:8c 86275 BOUND irb.100 10.8.52.5 3 d8:fa:97:b8:1a:dd 85198 BOUND irb.100 {master:0}","title":"Clear a DHCP client lease"},{"location":"juniper/#show-config-diff-before-commit","text":"You can view the changes that will be committed, which is useful when the commit is invalid and you need to inspect it. root# show | compare [edit access address-assignment pool p1 family inet] host server1-ipmi { ... } + host server2 { + hardware-address 00:11:22:33:44:33; + ip-address 172.17.1.6; + } + host server3 { + hardware-address 00:11:22:33:44:35; + ip-address 172.17.1.7; + } + host server4 { + hardware-address 00:11:22:33:44:1d; + ip-address 172.17.1.8; + }","title":"Show config diff before commit"},{"location":"juniper/#abort-config-changes-without-committing","text":"If you made changes you want to abandon, while still in configure mode run 'rollback 0' {master:0}[edit] root@ex4300# rollback 0 load complete {master:0}[edit] root@voyage-van-3-ex4300# exit Exiting configuration mode","title":"Abort config changes without committing"},{"location":"juniper/#links","text":"https://www.juniper.net/documentation/en_US/junos/topics/concept/junos-cli-overview.html https://www.juniper.net/documentation/en_US/junos/topics/task/configuration/virtual-chassis-mx-series-vc-ports-deleting.html https://junos-ansible-modules.readthedocs.io/en/2.1.0/","title":"Links"},{"location":"jupyter/","text":"\"Project Jupyter exists to develop open-source software, open-standards, and services for interactive computing across dozens of programming languages.\" - http://jupyter.org/ Links A gallery of interesting Jupyter Notebooks Interactive coding challenges Presenting Code Using Jupyter Notebook Slides","title":"Jupyter"},{"location":"jupyter/#links","text":"A gallery of interesting Jupyter Notebooks Interactive coding challenges Presenting Code Using Jupyter Notebook Slides","title":"Links"},{"location":"jwt/","text":"\"JSON Web Token (JWT) is an open standard ( RFC 7519 ) that defines a compact and self-contained way for securely transmitting information between parties as a JSON object. This information can be verified and trusted because it is digitally signed. JWTs can be signed using a secret (with the HMAC algorithm) or a public/private key pair using RSA.\" - https://jwt.io/introduction/","title":"jwt"},{"location":"kaniko/","text":"\"kaniko is a tool to build container images from a Dockerfile, inside a container or Kubernetes cluster. kaniko doesn't depend on a Docker daemon and executes each command within a Dockerfile completely in userspace. This enables building container images in environments that can't easily or securely run a Docker daemon, such as a standard Kubernetes cluster.\" - https://github.com/GoogleContainerTools/kaniko","title":"kaniko"},{"location":"keel/","text":"\"Automated Kubernetes deployment updates\" - https://github.com/keel-hq/keel \"Kubectl is the new SSH. If you are using it to update production workloads, you are doing it wrong.\" - https://keel.sh/","title":"keel"},{"location":"keybase/","text":"Keybase is a free, open source security app. It's also a public directory of people. Keybase.io Introducing the Keybase filesystem Github.com/Keybase My ID https://keybase.io/hoherd keybase id hoherd","title":"keybase"},{"location":"keybase/#my-id","text":"https://keybase.io/hoherd keybase id hoherd","title":"My ID"},{"location":"kubernetes/","text":"\"Kubernetes is an open-source platform for automating deployment, scaling, and operations of application containers across clusters of hosts, providing container-centric infrastructure.\" - https://kubernetes.io/docs/whatisk8s Glossary More terms in the k8s glossary: https://kubernetes.io/docs/reference/glossary/ Container Network Interface (CNI) - https://github.com/containernetworking/cni Container Runtime Interface (CRI) - https://github.com/containerd/cri/ Container Storage Interface (CSI) - https://github.com/container-storage-interface/spec Horizontal Pod Autoscaling (HPA) cli usage Learn about kubernetes kubectl explain roles Multiple kubeadm configs The default config is ~/.kube/config , but if you want to use multiple configs you can do this: export KUBECONFIG=\"${HOME}/code/kubespray/artifacts/admin.conf:${HOME}/.kube/config\" I have seen weird problems when the order of configs is changed, such as certificate-authority-data and client-certificate-data being missing. kubeadm \"kubeadm: easily bootstrap a secure Kubernetes cluster.\" - kubeadm --help https://github.com/kubernetes/kubeadm Show your kubeadm tokens $ sudo kubeadm token list TOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPS ubyc9a.1eq2ihwtnz7c7c9e 23h 2018-05-24T16:19:33-04:00 authentication,signing The default bootstrap token generated by 'kubeadm init'. system:bootstrappers:kubeadm:default-node-token See sudo kubeadm token -h for more usage. kubectl \"kubectl controls the Kubernetes cluster manager.\" - kubectl --help https://github.com/kubernetes/kubectl kubectl get - show all resource types with short-hand versions. kubectl completion -h - show how to configure completion for your shell. kubectl config get-contexts - show which k8s configuration contexts you can control. kubectl config use-context foo - switch to the foo context. kubectl get nodes - show the nodes in the k8s cluster. kubectl get pods - show deployed pods. there can be many pods per deployment. kubectl get pods -n kube-system - show pods in a specific namespace. kubectl get pods,hpa,deployment --all-namespaces - get several resource types at once, from all namespaces kubectl describe pod foo kubectl get deployment kubectl describe deployment foo kubectl get ns - show namespaces. kubectl get pv - show physical volumes. kubectl get svc -n kube-system - show a table of important details about running services in the kube-system namespace. kubectl get pods -o yaml - show the yaml configs for the currently running status of every pod. kubectl explain pods.spec - show documentation about pod specifications. kubectl describe pods/echoserver - describe the pod whose Name is echoserver. kubectl get rs - show replica sets. kubectl expose deployment <deployment_name> --type=NodePort - create a service for the given deployment. kubectl scale deployment <deployment_name> --replicas=5 - scale a deployment to 5 pods. kubectl rollout history deployment <deployment_name> kubectl get cm - get a list of config maps. Watch what's going on in your cluster watch kubectl get pods --all-namespaces -o wide Show logs for a given pod since N hours ago kubectl logs <podname> --since=12h The --since arg can take [s]econds, [m]inutes and [h]ours. Longer durations should use --since-time=<rfc3339 timestamp> Show logs for a given pod since a given date The --since-time arg takes RFC3339 datetime. EG: 1991-08-03T13:31:46-07:00 . This format requirement is strict, and is incompatible with the GNU date --rfc-3339=seconds output, which uses a space instead of a T to separate the full date from the full time, and +%FT%F%z , which does not include a colon between hours and minutes. kubectl logs <pod_name> --since-time=\"$(date --iso-8601=seconds -d '-5 weeks')\" Output custom column names for a command $ kubectl get pvc --all-namespaces -o custom-columns=NAME:metadata.name,SIZE:spec.resources.requests.storage NAME SIZE foo-logs 256Gi test-volume-2 1Gi some-awesome-service 5Gi Installations The standard way to install k8s by yourself is to use kubeadm . Manually on Ubuntu 16 # as root swapoff -a # https://github.com/kubernetes/kubernetes/issues/53533 curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add - curl -fsSL https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - echo \"deb [arch=amd64] https://download.docker.com/linux/ubuntu xenial stable\" > /etc/apt/sources.list.d/docker.list echo \"deb http://apt.kubernetes.io/ kubernetes-xenial main\" > /etc/apt/sources.list.d/kubernetes.list apt update apt dist-upgrade -y apt install -y apt-transport-https ca-certificates curl software-properties-common apt install -y docker-ce apt install -y kubelet kubeadm kubectl kubeadm init kubeadm init guide: https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#instructions DNS Kubernetes lets you resolve resources via DNS https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/ https://kubernetes.io/docs/tasks/administer-cluster/dns-debugging-resolution/ Enable k8s dns logging kubectl -n kube-system edit configmap coredns # Add 'log' to the 'Corefile' config DNS Entity map Kubernetes Service: `<service>.<namespace>.svc.cluster.local (eg: httpbin.default.svc.cluster.local ) kubectl get svc --all-namespaces -o json | jq -r '.items[] | \"\\(.metadata.name).\\(.metadata.namespace).svc.cluster.local\"' Kubernetes service srv records: . .svc.cluster.local (eg: _http._tcp.httpbin.default.svc.cluster.local) Pod by IP address: . .pod.cluster.local (eg: 192-168-1-6.default.pod.cluster.local) Pod by name: ...? Links http://on-demand.gputechconf.com/gtc/2018/presentation/s8893-the-path-to-gpu-as-a-service-in-kubernetes.pdf http://on-demand.gputechconf.com/gtc/2018/video/S8893/ http://slack.kubernetes.io/ https://blog.hypriot.com/post/setup-kubernetes-raspberry-pi-cluster https://docs.projectcalico.org/latest/introduction/ https://github.com/kelseyhightower/kubernetes-the-hard-way - \"The target audience for this tutorial is someone planning to support a production Kubernetes cluster and wants to understand how everything fits together.\" https://github.com/kinvolk/kubernetes-the-hard-way-vagrant - \"A port of Kelsey Hightower's 'Kubernetes the Hard Way' tutorial to Vagrant.\" https://github.com/kubernetes/dashboard#kubernetes-dashboard https://github.com/kubernetes/kompose - Compose to Kubernetes https://kubernetes.io/docs/concepts/cluster-administration/addons/ https://kubernetes.io/docs/concepts/cluster-administration/logging/ https://kubernetes.io/docs/concepts/services-networking/network-policies/ https://kubernetes.io/docs/concepts/workloads/controllers/deployment/ https://kubernetes.io/docs/concepts/workloads/pods/pod/index.html https://kubernetes.io/docs/getting-started-guides/minikube/ https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm https://www.cncf.io/certification/expert/CKA/ https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#hook-details https://github.com/ClusterHQ/flocker - Flocker is an open-source Container Data Volume Manager for your Dockerized applications. https://cloudplatform.googleblog.com/2018/05/Beyond-CPU-horizontal-pod-autoscaling-comes-to-Google-Kubernetes-Engine.html https://github.com/vapor-ware/ksync - Sync local filesystem with a target container https://metallb.universe.tf - For those of us not running in a cloud, metallb can serve as a k8s native LB.","title":"kubernetes"},{"location":"kubernetes/#glossary","text":"More terms in the k8s glossary: https://kubernetes.io/docs/reference/glossary/ Container Network Interface (CNI) - https://github.com/containernetworking/cni Container Runtime Interface (CRI) - https://github.com/containerd/cri/ Container Storage Interface (CSI) - https://github.com/container-storage-interface/spec Horizontal Pod Autoscaling (HPA)","title":"Glossary"},{"location":"kubernetes/#cli-usage","text":"","title":"cli usage"},{"location":"kubernetes/#learn-about-kubernetes","text":"kubectl explain roles","title":"Learn about kubernetes"},{"location":"kubernetes/#multiple-kubeadm-configs","text":"The default config is ~/.kube/config , but if you want to use multiple configs you can do this: export KUBECONFIG=\"${HOME}/code/kubespray/artifacts/admin.conf:${HOME}/.kube/config\" I have seen weird problems when the order of configs is changed, such as certificate-authority-data and client-certificate-data being missing.","title":"Multiple kubeadm configs"},{"location":"kubernetes/#kubeadm","text":"\"kubeadm: easily bootstrap a secure Kubernetes cluster.\" - kubeadm --help https://github.com/kubernetes/kubeadm","title":"kubeadm"},{"location":"kubernetes/#show-your-kubeadm-tokens","text":"$ sudo kubeadm token list TOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPS ubyc9a.1eq2ihwtnz7c7c9e 23h 2018-05-24T16:19:33-04:00 authentication,signing The default bootstrap token generated by 'kubeadm init'. system:bootstrappers:kubeadm:default-node-token See sudo kubeadm token -h for more usage.","title":"Show your kubeadm tokens"},{"location":"kubernetes/#kubectl","text":"\"kubectl controls the Kubernetes cluster manager.\" - kubectl --help https://github.com/kubernetes/kubectl kubectl get - show all resource types with short-hand versions. kubectl completion -h - show how to configure completion for your shell. kubectl config get-contexts - show which k8s configuration contexts you can control. kubectl config use-context foo - switch to the foo context. kubectl get nodes - show the nodes in the k8s cluster. kubectl get pods - show deployed pods. there can be many pods per deployment. kubectl get pods -n kube-system - show pods in a specific namespace. kubectl get pods,hpa,deployment --all-namespaces - get several resource types at once, from all namespaces kubectl describe pod foo kubectl get deployment kubectl describe deployment foo kubectl get ns - show namespaces. kubectl get pv - show physical volumes. kubectl get svc -n kube-system - show a table of important details about running services in the kube-system namespace. kubectl get pods -o yaml - show the yaml configs for the currently running status of every pod. kubectl explain pods.spec - show documentation about pod specifications. kubectl describe pods/echoserver - describe the pod whose Name is echoserver. kubectl get rs - show replica sets. kubectl expose deployment <deployment_name> --type=NodePort - create a service for the given deployment. kubectl scale deployment <deployment_name> --replicas=5 - scale a deployment to 5 pods. kubectl rollout history deployment <deployment_name> kubectl get cm - get a list of config maps.","title":"kubectl"},{"location":"kubernetes/#watch-whats-going-on-in-your-cluster","text":"watch kubectl get pods --all-namespaces -o wide","title":"Watch what's going on in your cluster"},{"location":"kubernetes/#show-logs-for-a-given-pod-since-n-hours-ago","text":"kubectl logs <podname> --since=12h The --since arg can take [s]econds, [m]inutes and [h]ours. Longer durations should use --since-time=<rfc3339 timestamp>","title":"Show logs for a given pod since N hours ago"},{"location":"kubernetes/#show-logs-for-a-given-pod-since-a-given-date","text":"The --since-time arg takes RFC3339 datetime. EG: 1991-08-03T13:31:46-07:00 . This format requirement is strict, and is incompatible with the GNU date --rfc-3339=seconds output, which uses a space instead of a T to separate the full date from the full time, and +%FT%F%z , which does not include a colon between hours and minutes. kubectl logs <pod_name> --since-time=\"$(date --iso-8601=seconds -d '-5 weeks')\"","title":"Show logs for a given pod since a given date"},{"location":"kubernetes/#output-custom-column-names-for-a-command","text":"$ kubectl get pvc --all-namespaces -o custom-columns=NAME:metadata.name,SIZE:spec.resources.requests.storage NAME SIZE foo-logs 256Gi test-volume-2 1Gi some-awesome-service 5Gi","title":"Output custom column names for a command"},{"location":"kubernetes/#installations","text":"The standard way to install k8s by yourself is to use kubeadm .","title":"Installations"},{"location":"kubernetes/#manually-on-ubuntu-16","text":"# as root swapoff -a # https://github.com/kubernetes/kubernetes/issues/53533 curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add - curl -fsSL https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - echo \"deb [arch=amd64] https://download.docker.com/linux/ubuntu xenial stable\" > /etc/apt/sources.list.d/docker.list echo \"deb http://apt.kubernetes.io/ kubernetes-xenial main\" > /etc/apt/sources.list.d/kubernetes.list apt update apt dist-upgrade -y apt install -y apt-transport-https ca-certificates curl software-properties-common apt install -y docker-ce apt install -y kubelet kubeadm kubectl kubeadm init kubeadm init guide: https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#instructions","title":"Manually on Ubuntu 16"},{"location":"kubernetes/#dns","text":"Kubernetes lets you resolve resources via DNS https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/ https://kubernetes.io/docs/tasks/administer-cluster/dns-debugging-resolution/","title":"DNS"},{"location":"kubernetes/#enable-k8s-dns-logging","text":"kubectl -n kube-system edit configmap coredns # Add 'log' to the 'Corefile' config","title":"Enable k8s dns logging"},{"location":"kubernetes/#dns-entity-map","text":"Kubernetes Service: `<service>.<namespace>.svc.cluster.local (eg: httpbin.default.svc.cluster.local ) kubectl get svc --all-namespaces -o json | jq -r '.items[] | \"\\(.metadata.name).\\(.metadata.namespace).svc.cluster.local\"' Kubernetes service srv records: . .svc.cluster.local (eg: _http._tcp.httpbin.default.svc.cluster.local) Pod by IP address: . .pod.cluster.local (eg: 192-168-1-6.default.pod.cluster.local) Pod by name: ...?","title":"DNS Entity map"},{"location":"kubernetes/#links","text":"http://on-demand.gputechconf.com/gtc/2018/presentation/s8893-the-path-to-gpu-as-a-service-in-kubernetes.pdf http://on-demand.gputechconf.com/gtc/2018/video/S8893/ http://slack.kubernetes.io/ https://blog.hypriot.com/post/setup-kubernetes-raspberry-pi-cluster https://docs.projectcalico.org/latest/introduction/ https://github.com/kelseyhightower/kubernetes-the-hard-way - \"The target audience for this tutorial is someone planning to support a production Kubernetes cluster and wants to understand how everything fits together.\" https://github.com/kinvolk/kubernetes-the-hard-way-vagrant - \"A port of Kelsey Hightower's 'Kubernetes the Hard Way' tutorial to Vagrant.\" https://github.com/kubernetes/dashboard#kubernetes-dashboard https://github.com/kubernetes/kompose - Compose to Kubernetes https://kubernetes.io/docs/concepts/cluster-administration/addons/ https://kubernetes.io/docs/concepts/cluster-administration/logging/ https://kubernetes.io/docs/concepts/services-networking/network-policies/ https://kubernetes.io/docs/concepts/workloads/controllers/deployment/ https://kubernetes.io/docs/concepts/workloads/pods/pod/index.html https://kubernetes.io/docs/getting-started-guides/minikube/ https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm https://www.cncf.io/certification/expert/CKA/ https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#hook-details https://github.com/ClusterHQ/flocker - Flocker is an open-source Container Data Volume Manager for your Dockerized applications. https://cloudplatform.googleblog.com/2018/05/Beyond-CPU-horizontal-pod-autoscaling-comes-to-Google-Kubernetes-Engine.html https://github.com/vapor-ware/ksync - Sync local filesystem with a target container https://metallb.universe.tf - For those of us not running in a cloud, metallb can serve as a k8s native LB.","title":"Links"},{"location":"launchd/","text":"launchd is MacOS X's init system. Example Watch Folder This user LaunchAgent would be placed into $HOME/Library/LaunchAgents/photo_processor.plist . We have to specify /bin/bash as the first ProgramArgument so OS X doesn't complain about DRM or mach-o executable shizz. This effectively limits us to bash 3. <?xml version=\"1.0\" encoding=\"UTF-8\"?> <!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\"> <plist version=\"1.0\"> <dict> <key>Label</key> <string>photo_processor.sh</string> <key>ProgramArguments</key><array> <string>/bin/bash</string> <string>/Users/hoherd/code/dho-bin/photo_processor.sh</string> </array> <key>WatchPaths</key> <array> <string>/Users/hoherd/Dropbox/yp/photo_queue/</string> </array> </dict> </plist> See also https://github.com/jordansissel/pleaserun","title":"launchd"},{"location":"launchd/#example","text":"","title":"Example"},{"location":"launchd/#watch-folder","text":"This user LaunchAgent would be placed into $HOME/Library/LaunchAgents/photo_processor.plist . We have to specify /bin/bash as the first ProgramArgument so OS X doesn't complain about DRM or mach-o executable shizz. This effectively limits us to bash 3. <?xml version=\"1.0\" encoding=\"UTF-8\"?> <!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\"> <plist version=\"1.0\"> <dict> <key>Label</key> <string>photo_processor.sh</string> <key>ProgramArguments</key><array> <string>/bin/bash</string> <string>/Users/hoherd/code/dho-bin/photo_processor.sh</string> </array> <key>WatchPaths</key> <array> <string>/Users/hoherd/Dropbox/yp/photo_queue/</string> </array> </dict> </plist>","title":"Watch Folder"},{"location":"launchd/#see-also","text":"https://github.com/jordansissel/pleaserun","title":"See also"},{"location":"ldap/","text":"Lightweight Directory Access Protocol Acronyms and stuff o=organization c=country dn=distinguished_name dc=domain_component rdn=relative_distinguished_name cn=common_name uid=user_id Tricks Show the whole ldap database From the LDAP server slapcat Show pwdFailureTime count and associated uid sudo slapcat 2>/dev/null | egrep '^(pwdFailureTime|uid:)' | uniq -c -w 14 | grep -B1 pwdFailureTime Show LDAP and local user account info getent passwd maxb Search an LDAP tree ldapsearch -x -b \"dc=example,dc=com\" This can be used when forwarding through ssh -L 3389:127.0.0.1:389 remotehost ldapsearch -x -h 127.0.0.1 -p 3389 -b \"dc=example,dc=com\" Run a search while authenticated This logs in as danielh and searches for a record with uid=robertc ldapsearch -x -b \"dc=example,dc=com\" -D \"uid=danielh,ou=people,dc=example,dc=com\" -W \"uid=danielh\" Refresh LDAP user cache on CentOS 6 nscd -i passwd ; nscd -i group ; /etc/init.d/lldpad restart ; /etc/init.d/nslcd restart ; /etc/init.d/nscd restart ; See Also Apache Directory Studio graphical interface for LDAP: https://directory.apache.org/studio/ A great series of articles on LDAP: http://www.ldapman.org/articles/ Ubuntu Server LDAP integration: https://help.ubuntu.com/community/LDAPClientAuthentication","title":"ldap"},{"location":"ldap/#acronyms-and-stuff","text":"o=organization c=country dn=distinguished_name dc=domain_component rdn=relative_distinguished_name cn=common_name uid=user_id","title":"Acronyms and stuff"},{"location":"ldap/#tricks","text":"","title":"Tricks"},{"location":"ldap/#show-the-whole-ldap-database","text":"From the LDAP server slapcat","title":"Show the whole ldap database"},{"location":"ldap/#show-pwdfailuretime-count-and-associated-uid","text":"sudo slapcat 2>/dev/null | egrep '^(pwdFailureTime|uid:)' | uniq -c -w 14 | grep -B1 pwdFailureTime","title":"Show pwdFailureTime count and associated uid"},{"location":"ldap/#show-ldap-and-local-user-account-info","text":"getent passwd maxb","title":"Show LDAP and local user account info"},{"location":"ldap/#search-an-ldap-tree","text":"ldapsearch -x -b \"dc=example,dc=com\" This can be used when forwarding through ssh -L 3389:127.0.0.1:389 remotehost ldapsearch -x -h 127.0.0.1 -p 3389 -b \"dc=example,dc=com\"","title":"Search an LDAP tree"},{"location":"ldap/#run-a-search-while-authenticated","text":"This logs in as danielh and searches for a record with uid=robertc ldapsearch -x -b \"dc=example,dc=com\" -D \"uid=danielh,ou=people,dc=example,dc=com\" -W \"uid=danielh\"","title":"Run a search while authenticated"},{"location":"ldap/#refresh-ldap-user-cache-on-centos-6","text":"nscd -i passwd ; nscd -i group ; /etc/init.d/lldpad restart ; /etc/init.d/nslcd restart ; /etc/init.d/nscd restart ;","title":"Refresh LDAP user cache on CentOS 6"},{"location":"ldap/#see-also","text":"Apache Directory Studio graphical interface for LDAP: https://directory.apache.org/studio/ A great series of articles on LDAP: http://www.ldapman.org/articles/ Ubuntu Server LDAP integration: https://help.ubuntu.com/community/LDAPClientAuthentication","title":"See Also"},{"location":"lego-mindstorms/","text":"Lego Mindstorms Tips ev3 programming software is available via cask : brew cask install lego-mindstorms-ev3 Links http://www.lego.com/en-us/mindstorms Community Gallery - Robot designs from lego mindstorm community. Robot Operating System for ev3 BUILD A ROBOT - \"These robots give you the full EV3 experience, complete with building instructions, programming missions and the programming tool included in the new free EV3 Programmer App.\" LEGO MINDSTORMS EV3 source code BrickPi - \"This project combines the brains of a Raspberry Pi with the brawn of a LEGO MINDSTORMS NXT\" ev3dev - \"ev3dev is a Debian Linux-based operating system that runs on several LEGO\u00ae MINDSTORMS compatible platforms including the LEGO\u00ae MINDSTORMS EV3 and Raspberry Pi-powered BrickPi.\" http://botbench.com/ http://robotsquare.com/ Videos link description https://youtu.be/Ii4ElBLvp6Q Active differential system for RWD traction control https://youtu.be/t-c6ktfzwW0 Arm robot https://youtu.be/961IdKfa5rI Arm robot - 3 axis, 360 degree motion, lightweight https://youtu.be/yAtmdyyIRbE Arm robot on treads https://youtu.be/P6b60zf1g18 BALANC3R and Gyro Boy, self balancing Segway style bots https://youtu.be/zR4BKgj9WSc Brick2014 EV3 6-Axis Robot Arm https://youtu.be/2zbBCmNoaXs BrickPi walkthrough https://youtu.be/cETV5WGB6kQ Bridge layer https://youtu.be/bFgO-C2TAV8 Clash of Clans player https://youtu.be/np3qWZVyvIU Coin sorter https://youtu.be/oUJ4L4kmbHw Complicated bridge layer https://youtu.be/GOQENxeL4nY Convert rotation into reciprocating motion of a piston https://youtu.be/cPfPGh0yUos Differential gearing demonstrated with lego https://youtu.be/9pjpQoZoW6E EV3 Print3rbot https://youtu.be/dargOslomMA Ghost in the Shell Tachikoma (walking and driving quadruped bot) https://youtu.be/-tXY_Hf3fvU Gopro 3 axis pan/tilt/slide https://youtu.be/qHQZcRPqUkY Great Ball Contraption https://youtu.be/d7jXaffklVg Line Follower https://youtu.be/4xXCmw4Y5iA Plotter https://youtu.be/WG4FyoCjgdk Rubik's Cube solver https://youtu.be/1Ihjh_F7jn0 Spirograph https://youtu.be/59-osMTBQmY Spirograph https://youtu.be/9AI33m26KeM Spirograph https://youtu.be/mtbV47LTuz8 Spirograph https://youtu.be/b5E-VyQOfNM Steampunk walking ship https://youtu.be/Mp8Y2yjV4fU Sudoku solver https://youtu.be/qkhSj2cAKWg Telepresence https://youtu.be/A_mA72r3ZiQ Time Twister - mechanical digital clock https://youtu.be/O9Ha6cM0RjI Time Twister 3 - mechanical digital clock https://youtu.be/staapsj3eRQ World Record Rubik's Cube solver See also Robotics","title":"lego-mindstorms"},{"location":"lego-mindstorms/#tips","text":"ev3 programming software is available via cask : brew cask install lego-mindstorms-ev3","title":"Tips"},{"location":"lego-mindstorms/#links","text":"http://www.lego.com/en-us/mindstorms Community Gallery - Robot designs from lego mindstorm community. Robot Operating System for ev3 BUILD A ROBOT - \"These robots give you the full EV3 experience, complete with building instructions, programming missions and the programming tool included in the new free EV3 Programmer App.\" LEGO MINDSTORMS EV3 source code BrickPi - \"This project combines the brains of a Raspberry Pi with the brawn of a LEGO MINDSTORMS NXT\" ev3dev - \"ev3dev is a Debian Linux-based operating system that runs on several LEGO\u00ae MINDSTORMS compatible platforms including the LEGO\u00ae MINDSTORMS EV3 and Raspberry Pi-powered BrickPi.\" http://botbench.com/ http://robotsquare.com/","title":"Links"},{"location":"lego-mindstorms/#videos","text":"link description https://youtu.be/Ii4ElBLvp6Q Active differential system for RWD traction control https://youtu.be/t-c6ktfzwW0 Arm robot https://youtu.be/961IdKfa5rI Arm robot - 3 axis, 360 degree motion, lightweight https://youtu.be/yAtmdyyIRbE Arm robot on treads https://youtu.be/P6b60zf1g18 BALANC3R and Gyro Boy, self balancing Segway style bots https://youtu.be/zR4BKgj9WSc Brick2014 EV3 6-Axis Robot Arm https://youtu.be/2zbBCmNoaXs BrickPi walkthrough https://youtu.be/cETV5WGB6kQ Bridge layer https://youtu.be/bFgO-C2TAV8 Clash of Clans player https://youtu.be/np3qWZVyvIU Coin sorter https://youtu.be/oUJ4L4kmbHw Complicated bridge layer https://youtu.be/GOQENxeL4nY Convert rotation into reciprocating motion of a piston https://youtu.be/cPfPGh0yUos Differential gearing demonstrated with lego https://youtu.be/9pjpQoZoW6E EV3 Print3rbot https://youtu.be/dargOslomMA Ghost in the Shell Tachikoma (walking and driving quadruped bot) https://youtu.be/-tXY_Hf3fvU Gopro 3 axis pan/tilt/slide https://youtu.be/qHQZcRPqUkY Great Ball Contraption https://youtu.be/d7jXaffklVg Line Follower https://youtu.be/4xXCmw4Y5iA Plotter https://youtu.be/WG4FyoCjgdk Rubik's Cube solver https://youtu.be/1Ihjh_F7jn0 Spirograph https://youtu.be/59-osMTBQmY Spirograph https://youtu.be/9AI33m26KeM Spirograph https://youtu.be/mtbV47LTuz8 Spirograph https://youtu.be/b5E-VyQOfNM Steampunk walking ship https://youtu.be/Mp8Y2yjV4fU Sudoku solver https://youtu.be/qkhSj2cAKWg Telepresence https://youtu.be/A_mA72r3ZiQ Time Twister - mechanical digital clock https://youtu.be/O9Ha6cM0RjI Time Twister 3 - mechanical digital clock https://youtu.be/staapsj3eRQ World Record Rubik's Cube solver","title":"Videos"},{"location":"lego-mindstorms/#see-also","text":"Robotics","title":"See also"},{"location":"linksys/","text":"Linksys makes a variety of networking devices. They are owned by Cisco. ea3500 http://www.linksys.com/us/support-product?pid=01t80000003K7bbAAC 1.1.40.162464 Bug where if 5ghz is enabled, 2.4ghz sometimes will not be accessible. Only workaround is to use only 2.4ghz or 5ghz, not both. Even then sometimes the configs can end up in a funky state where neither will work, and you have to re-configure the wifi settings using wired ethernet.","title":"linksys"},{"location":"linksys/#ea3500","text":"http://www.linksys.com/us/support-product?pid=01t80000003K7bbAAC","title":"ea3500"},{"location":"linksys/#1140162464","text":"Bug where if 5ghz is enabled, 2.4ghz sometimes will not be accessible. Only workaround is to use only 2.4ghz or 5ghz, not both. Even then sometimes the configs can end up in a funky state where neither will work, and you have to re-configure the wifi settings using wired ethernet.","title":"1.1.40.162464"},{"location":"linux-performance-monitoring/","text":"Notes from the Linux Performance Monitoring talk at Velocity 2015 - Part 1 , Part 2 http://www.brendangregg.com/linuxperf.html USE Utilization Saturation Errors Observability Tools atop (atop uses the linux kernel event interface rather than sampling on screen updates, so it is better for viewing systems affected by short-lived processes) htop vmstat -Sm 1 iostat -xmdz 1 mpstat -P ALL 1 free -m sar -n DEV 1 strace -tttT # very disruptive of system performance, slows system significantly tcpdump pidstat -t 1 pidstat -d swapon -s lsof sar -n TCP,ETCP,DEV 1 collectl dstat strace 2>&1 | head -n 100 # since there's no strace -c N ss iptraf slabtop pcstat perf tiptop rdmsr perf-tools/execsnoop Benchmarking tools unixbench imbench sysbench lmbench fio pchar iperf Tuning tools sysctl ulimit chcpu Static tools Tracing ftrace iosnoop iolatency opensnoop tpoint funccount funcgraph kprobe bytehist stap","title":"linux-performance-monitoring"},{"location":"linux-performance-monitoring/#use","text":"Utilization Saturation Errors","title":"USE"},{"location":"linux-performance-monitoring/#observability-tools","text":"atop (atop uses the linux kernel event interface rather than sampling on screen updates, so it is better for viewing systems affected by short-lived processes) htop vmstat -Sm 1 iostat -xmdz 1 mpstat -P ALL 1 free -m sar -n DEV 1 strace -tttT # very disruptive of system performance, slows system significantly tcpdump pidstat -t 1 pidstat -d swapon -s lsof sar -n TCP,ETCP,DEV 1 collectl dstat strace 2>&1 | head -n 100 # since there's no strace -c N ss iptraf slabtop pcstat perf tiptop rdmsr perf-tools/execsnoop","title":"Observability Tools"},{"location":"linux-performance-monitoring/#benchmarking-tools","text":"unixbench imbench sysbench lmbench fio pchar iperf","title":"Benchmarking tools"},{"location":"linux-performance-monitoring/#tuning-tools","text":"sysctl ulimit chcpu","title":"Tuning tools"},{"location":"linux-performance-monitoring/#static-tools","text":"","title":"Static tools"},{"location":"linux-performance-monitoring/#tracing","text":"ftrace iosnoop iolatency opensnoop tpoint funccount funcgraph kprobe bytehist stap","title":"Tracing"},{"location":"linux/","text":"\"Linux is a family of free and open-source software operating systems built around the Linux kernel.\" - https://en.wikipedia.org/wiki/Linux Most linux distros are built on GNU tools, and this article is relevant in distinguishing the importance GNU plays in the linux ecosystem: https://www.gnu.org/gnu/why-gnu-linux.en.html Performance monitoring Linux Load Averages: Solving the Mystery Brendan Gregg's Linux Performance page Notes from the Linux Performance Monitoring talk at Velocity 2015 Tricks Configure a system to reboot on kernel panic These lines should be added to sysctl.conf # Reboot after 10 seconds if kernel panics kernel.panic = 10 # Treat all oopses as panics kernel.panic_on_oops = 1 Force reboot on corrupt system For times that commands like reboot and shutdown are not available. echo 1 > /proc/sys/kernel/sysrq echo b > /proc/sysrq-trigger See also lxc Distros rhel ubuntu Init systems systemd upstart Filesystems Filesystem Heirarchy Standards: http://refspecs.linuxfoundation.org/FHS_3.0/fhs/index.html LVM ZFS","title":"linux"},{"location":"linux/#performance-monitoring","text":"Linux Load Averages: Solving the Mystery Brendan Gregg's Linux Performance page Notes from the Linux Performance Monitoring talk at Velocity 2015","title":"Performance monitoring"},{"location":"linux/#tricks","text":"","title":"Tricks"},{"location":"linux/#configure-a-system-to-reboot-on-kernel-panic","text":"These lines should be added to sysctl.conf # Reboot after 10 seconds if kernel panics kernel.panic = 10 # Treat all oopses as panics kernel.panic_on_oops = 1","title":"Configure a system to reboot on kernel panic"},{"location":"linux/#force-reboot-on-corrupt-system","text":"For times that commands like reboot and shutdown are not available. echo 1 > /proc/sys/kernel/sysrq echo b > /proc/sysrq-trigger","title":"Force reboot on corrupt system"},{"location":"linux/#see-also","text":"lxc","title":"See also"},{"location":"linux/#distros","text":"rhel ubuntu","title":"Distros"},{"location":"linux/#init-systems","text":"systemd upstart","title":"Init systems"},{"location":"linux/#filesystems","text":"Filesystem Heirarchy Standards: http://refspecs.linuxfoundation.org/FHS_3.0/fhs/index.html LVM ZFS","title":"Filesystems"},{"location":"logstash/","text":"\"Logstash is an open source, server-side data processing pipeline that ingests data from a multitude of sources simultaneously, transforms it, and then sends it to your favorite stash.\" - https://www.elastic.co/products/logstash","title":"logstash"},{"location":"lsblk/","text":"\"lsblk lists information about all available or the specified block devices. The lsblk command reads the sysfs filesystem and udev db to gather information.\" - man lsblkq Examples Simple usage Here is the output of lsblk on an Ubuntu 16.04 Vagrant box: $ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 10G 0 disk \u2514\u2500sda1 8:1 0 10G 0 part / sdb 8:16 0 10M 0 disk Show filesystem information $ lsblk -f NAME FSTYPE LABEL UUID MOUNTPOINT sda \u2514\u2500sda1 ext4 cloudimg-rootfs 73ea38ed-7fcd-4871-8afa-17d36f4e4bfc / sdb iso9660 cidata 2017-08-15-16-47-34-00 Show some chosen fields To see a list of available columns, run lsblk -o . $ lsblk -do NAME,SIZE,TYPE,VENDOR,MODEL,SERIAL NAME SIZE TYPE VENDOR MODEL SERIAL sdd 1.8T disk ATA Micron_5100_MTFD 18211C914753 sdb 1.8T disk ATA Micron_5100_MTFD 18211C914736 sde 1.8T disk ATA Micron_5100_MTFD 171216897B54 sdc 1.8T disk ATA Micron_5100_MTFD 171216897B63 sda 223.6G disk ATA INTEL SSDSC2KB24 BTYS815301VG245AGN # Quick way to view ssd vs hdd models and mount points # ROTA: 1=hdd, 0=ssd # As of writing this method does not show zfs mount points. EG: sdb1 is mounted as zfs # $ lsblk -o name,rota,mountpoint,vendor,model,serial NAME ROTA MOUNTPOINT VENDOR MODEL SERIAL sda 0 ATA Samsung SSD 850 S39KNX0JA59421F \u2514\u2500sda1 0 / sdb 1 WD Elements 25A1 575833314435383753393754 \u251c\u2500sdb1 1 \u2514\u2500sdb9 1 sdc 1 Generic External 002210107962 \u251c\u2500sdc1 1 \u2514\u2500sdc2 1 /mnt/sdc sr0 1 MATSHITA DVD+-RW SW830 CN0X85FC3686 Show all available information The output here is really wide, but reformatting it through JSON makes it easier to read. Thankfully lsblk has a JSON output option, unlike many unix CLI tools. $ lsblk -O -J | jq . { \"blockdevices\": [ { \"name\": \"sda\", \"kname\": \"sda\", \"maj:min\": \"8:0\", \"fstype\": null, \"mountpoint\": null, \"label\": null, \"uuid\": null, \"parttype\": null, \"partlabel\": null, \"partuuid\": null, \"partflags\": null, \"ra\": \"128\", \"ro\": \"0\", \"rm\": \"0\", \"hotplug\": \"0\", \"model\": \"HARDDISK \", \"serial\": null, \"size\": \"10G\", \"state\": \"running\", \"owner\": \"root\", \"group\": \"disk\", \"mode\": \"brw-rw----\", \"alignment\": \"0\", \"min-io\": \"512\", \"opt-io\": \"0\", \"phy-sec\": \"512\", \"log-sec\": \"512\", \"rota\": \"1\", \"sched\": \"deadline\", \"rq-size\": \"128\", \"type\": \"disk\", \"disc-aln\": \"0\", \"disc-gran\": \"0B\", \"disc-max\": \"0B\", \"disc-zero\": \"0\", \"wsame\": \"0B\", \"wwn\": null, \"rand\": \"1\", \"pkname\": null, \"hctl\": \"2:0:0:0\", \"tran\": \"spi\", \"subsystems\": \"block:scsi:pci\", \"rev\": \"1.0 \", \"vendor\": \"VBOX \", \"children\": [ { \"name\": \"sda1\", \"kname\": \"sda1\", \"maj:min\": \"8:1\", \"fstype\": \"ext4\", \"mountpoint\": \"/\", \"label\": \"cloudimg-rootfs\", \"uuid\": \"73ea38ed-7fcd-4871-8afa-17d36f4e4bfc\", \"parttype\": \"0x83\", \"partlabel\": null, \"partuuid\": \"8d714561-01\", \"partflags\": \"0x80\", \"ra\": \"128\", \"ro\": \"0\", \"rm\": \"0\", \"hotplug\": \"0\", \"model\": null, \"serial\": null, \"size\": \"10G\", \"state\": null, \"owner\": \"root\", \"group\": \"disk\", \"mode\": \"brw-rw----\", \"alignment\": \"0\", \"min-io\": \"512\", \"opt-io\": \"0\", \"phy-sec\": \"512\", \"log-sec\": \"512\", \"rota\": \"1\", \"sched\": \"deadline\", \"rq-size\": \"128\", \"type\": \"part\", \"disc-aln\": \"0\", \"disc-gran\": \"0B\", \"disc-max\": \"0B\", \"disc-zero\": \"0\", \"wsame\": \"0B\", \"wwn\": null, \"rand\": \"1\", \"pkname\": \"sda\", \"hctl\": null, \"tran\": null, \"subsystems\": \"block:scsi:pci\", \"rev\": null, \"vendor\": null } ] }, { \"name\": \"sdb\", \"kname\": \"sdb\", \"maj:min\": \"8:16\", \"fstype\": \"iso9660\", \"mountpoint\": null, \"label\": \"cidata\", \"uuid\": \"2017-08-15-16-47-34-00\", \"parttype\": null, \"partlabel\": null, \"partuuid\": null, \"partflags\": null, \"ra\": \"128\", \"ro\": \"0\", \"rm\": \"0\", \"hotplug\": \"0\", \"model\": \"HARDDISK \", \"serial\": null, \"size\": \"10M\", \"state\": \"running\", \"owner\": \"root\", \"group\": \"disk\", \"mode\": \"brw-rw----\", \"alignment\": \"0\", \"min-io\": \"512\", \"opt-io\": \"0\", \"phy-sec\": \"512\", \"log-sec\": \"512\", \"rota\": \"1\", \"sched\": \"deadline\", \"rq-size\": \"128\", \"type\": \"disk\", \"disc-aln\": \"0\", \"disc-gran\": \"0B\", \"disc-max\": \"0B\", \"disc-zero\": \"0\", \"wsame\": \"32M\", \"wwn\": null, \"rand\": \"1\", \"pkname\": null, \"hctl\": \"2:0:1:0\", \"tran\": \"spi\", \"subsystems\": \"block:scsi:pci\", \"rev\": \"1.0 \", \"vendor\": \"VBOX \" } ] } See also findmnt","title":"lsblk"},{"location":"lsblk/#examples","text":"","title":"Examples"},{"location":"lsblk/#simple-usage","text":"Here is the output of lsblk on an Ubuntu 16.04 Vagrant box: $ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 10G 0 disk \u2514\u2500sda1 8:1 0 10G 0 part / sdb 8:16 0 10M 0 disk","title":"Simple usage"},{"location":"lsblk/#show-filesystem-information","text":"$ lsblk -f NAME FSTYPE LABEL UUID MOUNTPOINT sda \u2514\u2500sda1 ext4 cloudimg-rootfs 73ea38ed-7fcd-4871-8afa-17d36f4e4bfc / sdb iso9660 cidata 2017-08-15-16-47-34-00","title":"Show filesystem information"},{"location":"lsblk/#show-some-chosen-fields","text":"To see a list of available columns, run lsblk -o . $ lsblk -do NAME,SIZE,TYPE,VENDOR,MODEL,SERIAL NAME SIZE TYPE VENDOR MODEL SERIAL sdd 1.8T disk ATA Micron_5100_MTFD 18211C914753 sdb 1.8T disk ATA Micron_5100_MTFD 18211C914736 sde 1.8T disk ATA Micron_5100_MTFD 171216897B54 sdc 1.8T disk ATA Micron_5100_MTFD 171216897B63 sda 223.6G disk ATA INTEL SSDSC2KB24 BTYS815301VG245AGN # Quick way to view ssd vs hdd models and mount points # ROTA: 1=hdd, 0=ssd # As of writing this method does not show zfs mount points. EG: sdb1 is mounted as zfs # $ lsblk -o name,rota,mountpoint,vendor,model,serial NAME ROTA MOUNTPOINT VENDOR MODEL SERIAL sda 0 ATA Samsung SSD 850 S39KNX0JA59421F \u2514\u2500sda1 0 / sdb 1 WD Elements 25A1 575833314435383753393754 \u251c\u2500sdb1 1 \u2514\u2500sdb9 1 sdc 1 Generic External 002210107962 \u251c\u2500sdc1 1 \u2514\u2500sdc2 1 /mnt/sdc sr0 1 MATSHITA DVD+-RW SW830 CN0X85FC3686","title":"Show some chosen fields"},{"location":"lsblk/#show-all-available-information","text":"The output here is really wide, but reformatting it through JSON makes it easier to read. Thankfully lsblk has a JSON output option, unlike many unix CLI tools. $ lsblk -O -J | jq . { \"blockdevices\": [ { \"name\": \"sda\", \"kname\": \"sda\", \"maj:min\": \"8:0\", \"fstype\": null, \"mountpoint\": null, \"label\": null, \"uuid\": null, \"parttype\": null, \"partlabel\": null, \"partuuid\": null, \"partflags\": null, \"ra\": \"128\", \"ro\": \"0\", \"rm\": \"0\", \"hotplug\": \"0\", \"model\": \"HARDDISK \", \"serial\": null, \"size\": \"10G\", \"state\": \"running\", \"owner\": \"root\", \"group\": \"disk\", \"mode\": \"brw-rw----\", \"alignment\": \"0\", \"min-io\": \"512\", \"opt-io\": \"0\", \"phy-sec\": \"512\", \"log-sec\": \"512\", \"rota\": \"1\", \"sched\": \"deadline\", \"rq-size\": \"128\", \"type\": \"disk\", \"disc-aln\": \"0\", \"disc-gran\": \"0B\", \"disc-max\": \"0B\", \"disc-zero\": \"0\", \"wsame\": \"0B\", \"wwn\": null, \"rand\": \"1\", \"pkname\": null, \"hctl\": \"2:0:0:0\", \"tran\": \"spi\", \"subsystems\": \"block:scsi:pci\", \"rev\": \"1.0 \", \"vendor\": \"VBOX \", \"children\": [ { \"name\": \"sda1\", \"kname\": \"sda1\", \"maj:min\": \"8:1\", \"fstype\": \"ext4\", \"mountpoint\": \"/\", \"label\": \"cloudimg-rootfs\", \"uuid\": \"73ea38ed-7fcd-4871-8afa-17d36f4e4bfc\", \"parttype\": \"0x83\", \"partlabel\": null, \"partuuid\": \"8d714561-01\", \"partflags\": \"0x80\", \"ra\": \"128\", \"ro\": \"0\", \"rm\": \"0\", \"hotplug\": \"0\", \"model\": null, \"serial\": null, \"size\": \"10G\", \"state\": null, \"owner\": \"root\", \"group\": \"disk\", \"mode\": \"brw-rw----\", \"alignment\": \"0\", \"min-io\": \"512\", \"opt-io\": \"0\", \"phy-sec\": \"512\", \"log-sec\": \"512\", \"rota\": \"1\", \"sched\": \"deadline\", \"rq-size\": \"128\", \"type\": \"part\", \"disc-aln\": \"0\", \"disc-gran\": \"0B\", \"disc-max\": \"0B\", \"disc-zero\": \"0\", \"wsame\": \"0B\", \"wwn\": null, \"rand\": \"1\", \"pkname\": \"sda\", \"hctl\": null, \"tran\": null, \"subsystems\": \"block:scsi:pci\", \"rev\": null, \"vendor\": null } ] }, { \"name\": \"sdb\", \"kname\": \"sdb\", \"maj:min\": \"8:16\", \"fstype\": \"iso9660\", \"mountpoint\": null, \"label\": \"cidata\", \"uuid\": \"2017-08-15-16-47-34-00\", \"parttype\": null, \"partlabel\": null, \"partuuid\": null, \"partflags\": null, \"ra\": \"128\", \"ro\": \"0\", \"rm\": \"0\", \"hotplug\": \"0\", \"model\": \"HARDDISK \", \"serial\": null, \"size\": \"10M\", \"state\": \"running\", \"owner\": \"root\", \"group\": \"disk\", \"mode\": \"brw-rw----\", \"alignment\": \"0\", \"min-io\": \"512\", \"opt-io\": \"0\", \"phy-sec\": \"512\", \"log-sec\": \"512\", \"rota\": \"1\", \"sched\": \"deadline\", \"rq-size\": \"128\", \"type\": \"disk\", \"disc-aln\": \"0\", \"disc-gran\": \"0B\", \"disc-max\": \"0B\", \"disc-zero\": \"0\", \"wsame\": \"32M\", \"wwn\": null, \"rand\": \"1\", \"pkname\": null, \"hctl\": \"2:0:1:0\", \"tran\": \"spi\", \"subsystems\": \"block:scsi:pci\", \"rev\": \"1.0 \", \"vendor\": \"VBOX \" } ] }","title":"Show all available information"},{"location":"lsblk/#see-also","text":"findmnt","title":"See also"},{"location":"lshw/","text":"lshw is a cli tool on linux to show you information about your hardware. It outputs in several formats that are human and computer friendly. Examples lshw --help $ lshw --help Hardware Lister (lshw) - B.02.18 usage: lshw [-format] [-options ...] lshw -version -version print program version (B.02.18) format can be -html output hardware tree as HTML -xml output hardware tree as XML -json output hardware tree as a JSON object -short output hardware paths -businfo output bus information options can be -class CLASS only show a certain class of hardware -C CLASS same as '-class CLASS' -c CLASS same as '-class CLASS' -disable TEST disable a test (like pci, isapnp, cpuid, etc. ) -enable TEST enable a test (like pci, isapnp, cpuid, etc. ) -quiet don't display status -sanitize sanitize output (remove sensitive information like serial numbers, etc.) -numeric output numeric IDs (for PCI, USB, etc.) -notime exclude volatile attributes (timestamps) from output Example of -short output It's best to use sudo, otherwise you will not see all hardware. $ sudo lshw -short H/W path Device Class Description ================================================================= system OptiPlex 7010 (OptiPlex 7010) /0 bus 0YXT71 /0/0 memory 64KiB BIOS /0/5e processor Intel(R) Core(TM) i7-3770 CPU @ 3.40GHz /0/5e/3a memory 256KiB L1 cache /0/5e/3b memory 1MiB L2 cache /0/5e/3c memory 8MiB L3 cache /0/3d memory 16GiB System Memory /0/3d/0 memory 4GiB DIMM DDR3 Synchronous 1600 MHz (0.6 ns) /0/3d/1 memory 4GiB DIMM DDR3 Synchronous 1600 MHz (0.6 ns) /0/3d/2 memory 4GiB DIMM DDR3 Synchronous 1600 MHz (0.6 ns) /0/3d/3 memory 4GiB DIMM DDR3 Synchronous 1600 MHz (0.6 ns) /0/100 bridge Xeon E3-1200 v2/3rd Gen Core processor DRAM Controller /0/100/2 display Xeon E3-1200 v2/3rd Gen Core processor Graphics Controller /0/100/14 bus 7 Series/C210 Series Chipset Family USB xHCI Host Controller /0/100/14/0 usb3 bus xHCI Host Controller /0/100/14/1 usb4 bus xHCI Host Controller /0/100/14/1/4 scsi6 storage Elements 25A1 /0/100/14/1/4/0.0.0 /dev/sdb disk 4TB Elements 25A1 /0/100/14/1/4/0.0.0/1 /dev/sdb1 volume 3725GiB OS X ZFS partition or Solaris /usr partition /0/100/14/1/4/0.0.0/9 /dev/sdb9 volume 8191KiB reserved partition /0/100/16 communication 7 Series/C216 Chipset Family MEI Controller #1 /0/100/16.3 communication 7 Series/C210 Series Chipset Family KT Controller /0/100/19 eno1 network 82579LM Gigabit Network Connection /0/100/1a bus 7 Series/C216 Chipset Family USB Enhanced Host Controller #2 /0/100/1a/1 usb1 bus EHCI Host Controller /0/100/1a/1/1 bus Integrated Rate Matching Hub /0/100/1a/1/1/2 generic BCM20702A0 /0/100/1b multimedia 7 Series/C216 Chipset Family High Definition Audio Controller /0/100/1d bus 7 Series/C216 Chipset Family USB Enhanced Host Controller #1 /0/100/1d/1 usb2 bus EHCI Host Controller /0/100/1d/1/1 bus Integrated Rate Matching Hub /0/100/1d/1/1/5 bus USB2.0 Hub /0/100/1d/1/1/5/2 scsi7 storage External /0/100/1d/1/1/5/2/0.0.0 /dev/sdc disk 256GB External /0/100/1d/1/1/5/2/0.0.0/1 volume 512MiB EFI GPT partition /0/100/1d/1/1/5/2/0.0.0/2 /dev/sdc2 volume 221GiB EXT4 volume /0/100/1e bridge 82801 PCI Bridge /0/100/1f bridge Q77 Express Chipset LPC Controller /0/100/1f.2 storage 7 Series/C210 Series Chipset Family 6-port SATA Controller [AHCI mode] /0/100/1f.3 bus 7 Series/C216 Chipset Family SMBus Controller /0/1 scsi0 storage /0/1/0.0.0 /dev/sda disk 256GB Samsung SSD 850 /0/1/0.0.0/1 /dev/sda1 volume 238GiB EXT4 volume /0/2 scsi1 storage /0/2/0.0.0 /dev/cdrom disk DVD+-RW SW830 /1 docker0 network Ethernet interface See also lsblk lscpu lslogins lsmod lsof lspci lsusb","title":"lshw"},{"location":"lshw/#examples","text":"","title":"Examples"},{"location":"lshw/#lshw-help","text":"$ lshw --help Hardware Lister (lshw) - B.02.18 usage: lshw [-format] [-options ...] lshw -version -version print program version (B.02.18) format can be -html output hardware tree as HTML -xml output hardware tree as XML -json output hardware tree as a JSON object -short output hardware paths -businfo output bus information options can be -class CLASS only show a certain class of hardware -C CLASS same as '-class CLASS' -c CLASS same as '-class CLASS' -disable TEST disable a test (like pci, isapnp, cpuid, etc. ) -enable TEST enable a test (like pci, isapnp, cpuid, etc. ) -quiet don't display status -sanitize sanitize output (remove sensitive information like serial numbers, etc.) -numeric output numeric IDs (for PCI, USB, etc.) -notime exclude volatile attributes (timestamps) from output","title":"lshw --help"},{"location":"lshw/#example-of-short-output","text":"It's best to use sudo, otherwise you will not see all hardware. $ sudo lshw -short H/W path Device Class Description ================================================================= system OptiPlex 7010 (OptiPlex 7010) /0 bus 0YXT71 /0/0 memory 64KiB BIOS /0/5e processor Intel(R) Core(TM) i7-3770 CPU @ 3.40GHz /0/5e/3a memory 256KiB L1 cache /0/5e/3b memory 1MiB L2 cache /0/5e/3c memory 8MiB L3 cache /0/3d memory 16GiB System Memory /0/3d/0 memory 4GiB DIMM DDR3 Synchronous 1600 MHz (0.6 ns) /0/3d/1 memory 4GiB DIMM DDR3 Synchronous 1600 MHz (0.6 ns) /0/3d/2 memory 4GiB DIMM DDR3 Synchronous 1600 MHz (0.6 ns) /0/3d/3 memory 4GiB DIMM DDR3 Synchronous 1600 MHz (0.6 ns) /0/100 bridge Xeon E3-1200 v2/3rd Gen Core processor DRAM Controller /0/100/2 display Xeon E3-1200 v2/3rd Gen Core processor Graphics Controller /0/100/14 bus 7 Series/C210 Series Chipset Family USB xHCI Host Controller /0/100/14/0 usb3 bus xHCI Host Controller /0/100/14/1 usb4 bus xHCI Host Controller /0/100/14/1/4 scsi6 storage Elements 25A1 /0/100/14/1/4/0.0.0 /dev/sdb disk 4TB Elements 25A1 /0/100/14/1/4/0.0.0/1 /dev/sdb1 volume 3725GiB OS X ZFS partition or Solaris /usr partition /0/100/14/1/4/0.0.0/9 /dev/sdb9 volume 8191KiB reserved partition /0/100/16 communication 7 Series/C216 Chipset Family MEI Controller #1 /0/100/16.3 communication 7 Series/C210 Series Chipset Family KT Controller /0/100/19 eno1 network 82579LM Gigabit Network Connection /0/100/1a bus 7 Series/C216 Chipset Family USB Enhanced Host Controller #2 /0/100/1a/1 usb1 bus EHCI Host Controller /0/100/1a/1/1 bus Integrated Rate Matching Hub /0/100/1a/1/1/2 generic BCM20702A0 /0/100/1b multimedia 7 Series/C216 Chipset Family High Definition Audio Controller /0/100/1d bus 7 Series/C216 Chipset Family USB Enhanced Host Controller #1 /0/100/1d/1 usb2 bus EHCI Host Controller /0/100/1d/1/1 bus Integrated Rate Matching Hub /0/100/1d/1/1/5 bus USB2.0 Hub /0/100/1d/1/1/5/2 scsi7 storage External /0/100/1d/1/1/5/2/0.0.0 /dev/sdc disk 256GB External /0/100/1d/1/1/5/2/0.0.0/1 volume 512MiB EFI GPT partition /0/100/1d/1/1/5/2/0.0.0/2 /dev/sdc2 volume 221GiB EXT4 volume /0/100/1e bridge 82801 PCI Bridge /0/100/1f bridge Q77 Express Chipset LPC Controller /0/100/1f.2 storage 7 Series/C210 Series Chipset Family 6-port SATA Controller [AHCI mode] /0/100/1f.3 bus 7 Series/C216 Chipset Family SMBus Controller /0/1 scsi0 storage /0/1/0.0.0 /dev/sda disk 256GB Samsung SSD 850 /0/1/0.0.0/1 /dev/sda1 volume 238GiB EXT4 volume /0/2 scsi1 storage /0/2/0.0.0 /dev/cdrom disk DVD+-RW SW830 /1 docker0 network Ethernet interface","title":"Example of -short output"},{"location":"lshw/#see-also","text":"lsblk lscpu lslogins lsmod lsof lspci lsusb","title":"See also"},{"location":"lsof/","text":"lsof lists open files. This CLI tool is available on most *nix OSes. Examples List files open by a given user lsof -u username show listening TCP sockets Since everything in unix is a file, including network sockets, you can list open sockets and the programs that have them open. However, this is notably unreliable in Docker, so don't trust this completely. When in doubt, double check against ss -l . lsof -iTCP -sTCP:LISTEN Show a sorted list of processes by listening port lsof -iTCP -sTCP:LISTEN -P | sort -k2 -t: -n show what process is using port 80 or 443 with port numbers # -n makes lsof not resolve hostnames from ip addresses # -P makes lsof not resolve service names for port numbers. # -iTCP shows IP TCP sockets. lsof -nP -iTCP:80,443 show the selinux context for sockets lsof -i -Z See man page for extended syntax around Z See Also Process Explorer - LSOF type functionality for windows.","title":"lsof"},{"location":"lsof/#examples","text":"","title":"Examples"},{"location":"lsof/#list-files-open-by-a-given-user","text":"lsof -u username","title":"List files open by a given user"},{"location":"lsof/#show-listening-tcp-sockets","text":"Since everything in unix is a file, including network sockets, you can list open sockets and the programs that have them open. However, this is notably unreliable in Docker, so don't trust this completely. When in doubt, double check against ss -l . lsof -iTCP -sTCP:LISTEN","title":"show listening TCP sockets"},{"location":"lsof/#show-a-sorted-list-of-processes-by-listening-port","text":"lsof -iTCP -sTCP:LISTEN -P | sort -k2 -t: -n","title":"Show a sorted list of processes by listening port"},{"location":"lsof/#show-what-process-is-using-port-80-or-443-with-port-numbers","text":"# -n makes lsof not resolve hostnames from ip addresses # -P makes lsof not resolve service names for port numbers. # -iTCP shows IP TCP sockets. lsof -nP -iTCP:80,443","title":"show what process is using port 80 or 443 with port numbers"},{"location":"lsof/#show-the-selinux-context-for-sockets","text":"lsof -i -Z See man page for extended syntax around Z","title":"show the selinux context for sockets"},{"location":"lsof/#see-also","text":"Process Explorer - LSOF type functionality for windows.","title":"See Also"},{"location":"lvm/","text":"Linux Logical Volume Manager. Tutorial Overview Managing General flow Physical volumes (pv) are grouped into volume groups (vg). Volume groups are sliced up into logical volumes (lv). Because of that, the general flow is something like: # Partitioning is not necessary, so no need for fdisk or sgdisk pvcreate /dev/sd{x..z} vgcreate vg_scratch /dev/sd{x..z} lvcreate -l 95%FREE -n lv_scratch vg_scratch mkfs.ext4 /dev/vg_scratch/lv_scratch Examples Show a bunch of info pvdisplay -v pvs -v pvs -a pvs --segments vgdisplay -v vgs -v vgs -a -o +devices Show system disks and if they are in an LVM lvmdiskscan Show all logical volumes lvs Activate all volume groups vgchange -a y Create a physical volume physical volumes are groups of physical disks that can be used to create logical volumes pvcreate pv_name /dev/sdb2 /dev/sdc2 Create a logical volume This creates a specifically named logical volume on a volume group named vg_data lvcreate -L 10G -n lv_name vg_data Show how each logical volume is set up lvdisplay Show free extents vgs -o vg_free_count Extend a volume group to 1TB lvextend -L 1T /dev/vgroot/lv_srv && \\ resize2fs /dev/mapper/vgroot-lv_srv && \\ df -h /srv Extend a volume group to its max lvextend -l +100%FREE /dev/vgroot/lv_srv && \\ resize2fs /dev/mapper/vgroot-lv_srv && \\ df -h /srv","title":"lvm"},{"location":"lvm/#general-flow","text":"Physical volumes (pv) are grouped into volume groups (vg). Volume groups are sliced up into logical volumes (lv). Because of that, the general flow is something like: # Partitioning is not necessary, so no need for fdisk or sgdisk pvcreate /dev/sd{x..z} vgcreate vg_scratch /dev/sd{x..z} lvcreate -l 95%FREE -n lv_scratch vg_scratch mkfs.ext4 /dev/vg_scratch/lv_scratch","title":"General flow"},{"location":"lvm/#examples","text":"","title":"Examples"},{"location":"lvm/#show-a-bunch-of-info","text":"pvdisplay -v pvs -v pvs -a pvs --segments vgdisplay -v vgs -v vgs -a -o +devices","title":"Show a bunch of info"},{"location":"lvm/#show-system-disks-and-if-they-are-in-an-lvm","text":"lvmdiskscan","title":"Show system disks and if they are in an LVM"},{"location":"lvm/#show-all-logical-volumes","text":"lvs","title":"Show all logical volumes"},{"location":"lvm/#activate-all-volume-groups","text":"vgchange -a y","title":"Activate all volume groups"},{"location":"lvm/#create-a-physical-volume","text":"physical volumes are groups of physical disks that can be used to create logical volumes pvcreate pv_name /dev/sdb2 /dev/sdc2","title":"Create a physical volume"},{"location":"lvm/#create-a-logical-volume","text":"This creates a specifically named logical volume on a volume group named vg_data lvcreate -L 10G -n lv_name vg_data","title":"Create a logical volume"},{"location":"lvm/#show-how-each-logical-volume-is-set-up","text":"lvdisplay","title":"Show how each logical volume is set up"},{"location":"lvm/#show-free-extents","text":"vgs -o vg_free_count","title":"Show free extents"},{"location":"lvm/#extend-a-volume-group-to-1tb","text":"lvextend -L 1T /dev/vgroot/lv_srv && \\ resize2fs /dev/mapper/vgroot-lv_srv && \\ df -h /srv","title":"Extend a volume group to 1TB"},{"location":"lvm/#extend-a-volume-group-to-its-max","text":"lvextend -l +100%FREE /dev/vgroot/lv_srv && \\ resize2fs /dev/mapper/vgroot-lv_srv && \\ df -h /srv","title":"Extend a volume group to its max"},{"location":"lxc/","text":"\"LXC is a userspace interface for the Linux kernel containment features. Through a powerful API and simple tools, it lets Linux users easily create and manage system or application containers.\" - https://linuxcontainers.org See Also Docker","title":"LXC"},{"location":"lxc/#see-also","text":"Docker","title":"See Also"},{"location":"machine-learning/","text":"Machine learning is the subfield of computer science that, according to Arthur Samuel in 1959, gives \"computers the ability to learn without being explicitly programmed.\" - https://en.wikipedia.org/wiki/Machine_learning Terminology and concepts Supervised machine learning: The program is \"trained\" on a pre-defined set of \"training examples\", which then facilitate its ability to reach an accurate conclusion when given new data. Unsupervised machine learning: The program is given a bunch of data and must find patterns and relationships therein. \"The goal of ML is never to make 'perfect' guesses, because ML deals in domains where there is no such thing. The goal is to make guesses that are good enough to be useful.\" Machine learning builds heavily on statistics. Prerequisites Statistics Linear Algebra Calculus Resources Reddit /r/machinelearning wiki Data Science From Scratch book Andrew Ng's Coursera course on ML Machine Learning with Python / Practical Machine Learning Tutorial with Python Introduction Your First Machine Learning Project in Python Step-By-Step Example Machine Learning IPython Notebook FastML: Machine Learning Made Easy Tensorflow My Neural Network isn't working! What should I do? Machine Learning Recipes with Josh Gordon - Google Developers See Also Life 3.0: Being Human in the Age of Artificial Intelligence: https://www.amazon.com/Life-3-0-Being-Artificial-Intelligence/dp/1101946598 DeepMind and Blizzard open StarCraft II as an AI research environment","title":"Machine Learning"},{"location":"machine-learning/#terminology-and-concepts","text":"Supervised machine learning: The program is \"trained\" on a pre-defined set of \"training examples\", which then facilitate its ability to reach an accurate conclusion when given new data. Unsupervised machine learning: The program is given a bunch of data and must find patterns and relationships therein. \"The goal of ML is never to make 'perfect' guesses, because ML deals in domains where there is no such thing. The goal is to make guesses that are good enough to be useful.\" Machine learning builds heavily on statistics.","title":"Terminology and concepts"},{"location":"machine-learning/#prerequisites","text":"Statistics Linear Algebra Calculus","title":"Prerequisites"},{"location":"machine-learning/#resources","text":"Reddit /r/machinelearning wiki Data Science From Scratch book Andrew Ng's Coursera course on ML Machine Learning with Python / Practical Machine Learning Tutorial with Python Introduction Your First Machine Learning Project in Python Step-By-Step Example Machine Learning IPython Notebook FastML: Machine Learning Made Easy Tensorflow My Neural Network isn't working! What should I do? Machine Learning Recipes with Josh Gordon - Google Developers","title":"Resources"},{"location":"machine-learning/#see-also","text":"Life 3.0: Being Human in the Age of Artificial Intelligence: https://www.amazon.com/Life-3-0-Being-Artificial-Intelligence/dp/1101946598 DeepMind and Blizzard open StarCraft II as an AI research environment","title":"See Also"},{"location":"make/","text":"GNU make Automatic variables $ cat Makefile all: foo_one foo_two foo_%: bar_a bar_b @echo $*: this is $@ and it requires $^ bar_%: baz @echo $*: this is $@ and it requires $^ baz: @echo this is baz $ make this is baz a: this is bar_a and it requires baz b: this is bar_b and it requires baz one: this is foo_one and it requires bar_a bar_b two: this is foo_two and it requires bar_a bar_b Links https://www.gnu.org/software/make/manual/html_node/Automatic-Variables.html https://www.gnu.org/software/make/manual/html_node/Standard-Targets.html","title":"GNU make"},{"location":"make/#automatic-variables","text":"$ cat Makefile all: foo_one foo_two foo_%: bar_a bar_b @echo $*: this is $@ and it requires $^ bar_%: baz @echo $*: this is $@ and it requires $^ baz: @echo this is baz $ make this is baz a: this is bar_a and it requires baz b: this is bar_b and it requires baz one: this is foo_one and it requires bar_a bar_b two: this is foo_two and it requires bar_a bar_b","title":"Automatic variables"},{"location":"make/#links","text":"https://www.gnu.org/software/make/manual/html_node/Automatic-Variables.html https://www.gnu.org/software/make/manual/html_node/Standard-Targets.html","title":"Links"},{"location":"math/","text":"Links https://en.wikipedia.org/wiki/Category:Probability_theory_paradoxes","title":"math"},{"location":"math/#links","text":"https://en.wikipedia.org/wiki/Category:Probability_theory_paradoxes","title":"Links"},{"location":"mdraid/","text":"Linux software raid. Examples Show details of an array mdadm --detail /dev/md0 Links http://poweredgec.dell.com/ - the Dell ldstate command is a good view into software raid and hardware raid (eg: megaraid, sas2) under one command. https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/5/html/Deployment_Guide/s2-raid-manage-removing.html http://tldp.org/HOWTO/Software-RAID-HOWTO.html https://raid.wiki.kernel.org/index.php/Linux_Raid https://raid.wiki.kernel.org/index.php/RAID_setup","title":"mdraid"},{"location":"mdraid/#examples","text":"","title":"Examples"},{"location":"mdraid/#show-details-of-an-array","text":"mdadm --detail /dev/md0","title":"Show details of an array"},{"location":"mdraid/#links","text":"http://poweredgec.dell.com/ - the Dell ldstate command is a good view into software raid and hardware raid (eg: megaraid, sas2) under one command. https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/5/html/Deployment_Guide/s2-raid-manage-removing.html http://tldp.org/HOWTO/Software-RAID-HOWTO.html https://raid.wiki.kernel.org/index.php/Linux_Raid https://raid.wiki.kernel.org/index.php/RAID_setup","title":"Links"},{"location":"metallb/","text":"\"MetalLB is a load-balancer implementation for bare metal Kubernetes clusters, using standard routing protocols.\" - https://metallb.universe.tf/ \"This is not an official Google project, it is just code that happens to be owned by Google.\" - https://github.com/google/metallb Links https://github.com/google/metallb https://metallb.universe.tf/installation https://metallb.universe.tf/configuration/#layer-2-configuration See Also kubernetes minikube","title":"MetalLB"},{"location":"metallb/#links","text":"https://github.com/google/metallb https://metallb.universe.tf/installation https://metallb.universe.tf/configuration/#layer-2-configuration","title":"Links"},{"location":"metallb/#see-also","text":"kubernetes minikube","title":"See Also"},{"location":"micropython/","text":"\"MicroPython is a lean and efficient implementation of the Python 3 programming language that includes a small subset of the Python standard library and is optimised to run on microcontrollers and in constrained environments.\"- https://micropython.org https://docs.micropython.org Hardware ESP8266 pyboard Arduino Due microbit See also TalkPython.fm Episode #108: MicroPython and Open Source Hardware at Adafruit","title":"MicroPython"},{"location":"micropython/#hardware","text":"ESP8266 pyboard Arduino Due microbit","title":"Hardware"},{"location":"micropython/#see-also","text":"TalkPython.fm Episode #108: MicroPython and Open Source Hardware at Adafruit","title":"See also"},{"location":"minikube/","text":"\"Minikube is a tool that makes it easy to run Kubernetes locally. Minikube runs a single-node Kubernetes cluster inside a VM on your laptop for users looking to try out Kubernetes or develop with it day-to-day.\" - https://github.com/kubernetes/minikube Examples List available addons $ minikube addons list - addon-manager: enabled - dashboard: disabled - default-storageclass: enabled - efk: disabled - freshpod: disabled - gvisor: disabled - heapster: disabled - ingress: enabled - kube-dns: disabled - metrics-server: disabled - nvidia-driver-installer: disabled - nvidia-gpu-device-plugin: disabled - registry: enabled - registry-creds: disabled - storage-provisioner: enabled - storage-provisioner-gluster: disabled Enable the registry addon minikube addons enable registry Links https://github.com/kubernetes/minikube https://metallb.universe.tf/community/#testing-in-minikube","title":"minikube"},{"location":"minikube/#examples","text":"","title":"Examples"},{"location":"minikube/#list-available-addons","text":"$ minikube addons list - addon-manager: enabled - dashboard: disabled - default-storageclass: enabled - efk: disabled - freshpod: disabled - gvisor: disabled - heapster: disabled - ingress: enabled - kube-dns: disabled - metrics-server: disabled - nvidia-driver-installer: disabled - nvidia-gpu-device-plugin: disabled - registry: enabled - registry-creds: disabled - storage-provisioner: enabled - storage-provisioner-gluster: disabled","title":"List available addons"},{"location":"minikube/#enable-the-registry-addon","text":"minikube addons enable registry","title":"Enable the registry addon"},{"location":"minikube/#links","text":"https://github.com/kubernetes/minikube https://metallb.universe.tf/community/#testing-in-minikube","title":"Links"},{"location":"mkdocs/","text":"\"MkDocs is a fast, simple and downright gorgeous static site generator that's geared towards building project documentation. Documentation source files are written in Markdown, and configured with a single YAML configuration file.\" - http://www.mkdocs.org/ Links https://github.com/mkdocs/mkdocs/blob/master/docs/user-guide/writing-your-docs.md Ironically this documentation is more complete than what is on http://www.mkdocs.org/user-guide","title":"MkDocs"},{"location":"mkdocs/#links","text":"https://github.com/mkdocs/mkdocs/blob/master/docs/user-guide/writing-your-docs.md Ironically this documentation is more complete than what is on http://www.mkdocs.org/user-guide","title":"Links"},{"location":"mqtt/","text":"'MQTT is a machine-to-machine (M2M)/\"Internet of Things\" connectivity protocol.' - http://mqtt.org/","title":"mqtt"},{"location":"mutt/","text":"CLI e-mail client Usage http://www.mutt.org/doc/manual/manual-2.html j or Down next-entry move to the next entry k or Up previous-entry move to the previous entry z or PageDn page-down go to the next page Z or PageUp page-up go to the previous page = or Home first-entry jump to the first entry * or End last-entry jump to the last entry q quit exit the current menu ? help list all keybindings for the current menu Message Deletion http://www.sendmail.org/~ca/email/mutt/manual-4.html Delete e-mails older than 2012-12-01: [shift-d] ~d 1/12/12-1/1/1 # D/M/Y. this will only delete back to 2001-01-01 Delete messages where the subject matches a search: [shift-d] ~b search\\ pattern Delete messages where the subject matches a search: [shift-d] ~s search\\ pattern Delete messages older than one month: [shift-d] ~d >1m Delete messages older than 14 days: [shift-d] ~d > 14d","title":"mutt"},{"location":"mutt/#usage","text":"http://www.mutt.org/doc/manual/manual-2.html j or Down next-entry move to the next entry k or Up previous-entry move to the previous entry z or PageDn page-down go to the next page Z or PageUp page-up go to the previous page = or Home first-entry jump to the first entry * or End last-entry jump to the last entry q quit exit the current menu ? help list all keybindings for the current menu","title":"Usage"},{"location":"mutt/#message-deletion","text":"http://www.sendmail.org/~ca/email/mutt/manual-4.html Delete e-mails older than 2012-12-01: [shift-d] ~d 1/12/12-1/1/1 # D/M/Y. this will only delete back to 2001-01-01 Delete messages where the subject matches a search: [shift-d] ~b search\\ pattern Delete messages where the subject matches a search: [shift-d] ~s search\\ pattern Delete messages older than one month: [shift-d] ~d >1m Delete messages older than 14 days: [shift-d] ~d > 14d","title":"Message Deletion"},{"location":"myrepos/","text":"\"You have a lot of version control repositories. Sometimes you want to update them all at once. Or push out all your local changes. You use special command lines in some repositories to implement specific workflows. Myrepos provides a mr command, which is a tool to manage all your version control repositories.\" -- http://myrepos.branchable.com/ Usage Examples Register a bunch of repos for repo in ~/code/* ; do mr register \"$repo\" done Update all of your registered repos mr up","title":"myrepos"},{"location":"myrepos/#usage-examples","text":"","title":"Usage Examples"},{"location":"myrepos/#register-a-bunch-of-repos","text":"for repo in ~/code/* ; do mr register \"$repo\" done","title":"Register a bunch of repos"},{"location":"myrepos/#update-all-of-your-registered-repos","text":"mr up","title":"Update all of your registered repos"},{"location":"mysql/","text":"\"MySQL is an open-source relational database management system. Its name is a combination of \"My\", the name of co-founder Michael Widenius's daughter, and \"SQL\", the abbreviation for Structured Query Language. The MySQL development project has made its source code available under the terms of the GNU General Public License, as well as under a variety of proprietary agreements. MySQL was owned and sponsored by a single for-profit firm, the Swedish company MySQL AB, and is now owned by Oracle Corporation.\" - https://en.wikipedia.org/wiki/MySQL Examples Show variables of the running server mysqladmin variables Enable bin logging Edit /etc/my.cnf: log-bin=/var/lib/mysql/mysql-bin Show how a table was created SHOW CREATE TABLE table_name \\G Create a table CREATE TABLE photo_sizes ( `photo_id` char(32) NOT NULL, `format` mediumtext, `width` mediumtext, `height` mediumtext, `source` mediumtext, `url` mediumtext, PRIMARY KEY(`photo_id`) ) ; Create a table with multiple columns as the primary key CREATE TABLE `photo_sizes` ( `photo_id` char(32) NOT NULL, `format` char(32) NOT NULL DEFAULT '', `width` mediumtext, `height` mediumtext, `source` mediumtext, `url` mediumtext, PRIMARY KEY (`photo_id`,`format`) ) ENGINE=MyISAM DEFAULT CHARSET=latin1 Show what processes are running show processlist; Dump databases to sql files All databases mysqldump -u root -phunter2 --all-databases | gzip -9 > ~/$(date +%F-%H%m).sql.gz Or just a single database mysqldump -u root -phunter2 my_favorite_db | gzip -9 > ~/my_favorite_db-$(date +%F-%H%m).sql.gz Duplicate a database sudo mysqldump -v mogilefs | sudo mysql -D mogilefs_sjc Dump the schema of a database with no actual data sudo mysqldump --no-data dbname > schema.sql Show privileges show GRANTS ; Create a new user CREATE USER 'a_new_user'@'10.0.5.%' IDENTIFIED BY 'the_user_password'; GRANT ALL PRIVILEGES ON some_database.* TO 'a_new_user'@'10.0.5.%' WITH GRANT OPTION; Delete a user DELETE from mysql.user where user = 'user_name'; Grant Privileges GRANT ALL ON database.* TO 'newuser'@'localhost'; Change root password /usr/bin/mysqladmin -u root password 'new-password' /usr/bin/mysqladmin -u root -h hostname password 'new-password' or... UPDATE mysql.user SET Password=PASSWORD('hunter2') WHERE User='leroy_jenkins' AND Host='localhost' ; Create statements Create an index on table images for column rating_count create index rating_count on images (rating_count) ; Drop an index from a table drop index rating_count on images ; Table Alters Add a column alter table flixplor add o_width char(12); Drop a column alter table flixplor drop column o_width; Change the type of a column alter table flixplor modify o_height mediumint ; Add a current timestamp column alter table images add last_updated timestamp not null default current_timestamp on update current_timestamp; Change the table engine to innodb ALTER TABLE images ENGINE=INNODB; Change a table's encoding alter table raw_flickr_data CONVERT TO CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci ; Table Inserts Add a record Table Updates Update if key exists For unique fields, you cannot insert, so you need to have an on duplicate key section in your statement. INSERT INTO photo_sizes ( photo_id, format, height, width, url, source ) values ( '32704962848', 'Medium 640', '427', '640', 'https://www.flickr.com/photos/warzauwynn/32704962848/sizes/z/', 'https://farm5.staticflickr.com/4855/32704962848_3a96b4c635_z.jpg' ) ON DUPLICATE KEY UPDATE height='427', width='640', url='https://www.flickr.com/photos/warzauwynn/32704962848/sizes/z/', source='https://farm5.staticflickr.com/4855/32704962848_3a96b4c635_z.jpg' http://dev.mysql.com/doc/refman/5.0/en/insert-on-duplicate.html Update a datetime column with a random datetime UPDATE flixplor SET last_retrieval = subtime( concat_ws(' ','2019-01-01' - interval rand() * 10000 day ,'00:00:00'), -- create a random YYYY-MM-DD within 10k days of given datetime sec_to_time(floor(0 + (rand() * 86401))) -- create a random HH:MM:SS within a 24 hour period ) WHERE last_retrieval < '2019-01-01 00:00:00'; Table Selects Select values and don't show duplicates SELECT col from servers group by col ; Select photo_id and discard duplicates (uniq) SELECT photo_id from photo_sizes group by photo_id ; Select and count unique pairs of columns SELECT model, unit, count(*) as n from servers group by model, unit having n > 1 order by model asc ; Select the count of rows in a table SELECT count(*) from flixplor where o_height > 100 ; Do some math to create a new column during a select SELECT photo_id,last_retrieval,o_height,o_width,(o_height * o_width) as pixels from flixplor where last_reposted < from_unixtime('1384268667') or last_reposted is NULL order by (o_height * o_width) limit 10 ; Transform datetime into a date diff This selects the number of hours since the given datestamp instead of the datestamp itself. SELECT TIMESTAMPDIFF(HOUR, date_taken, NOW()) from photos ; See also DATEDIFF. Statement explanations The EXPLAIN statement can give you additional info about how complex your statement is. Explain select mysql> explain SELECT *,(rating_sum / rating_count) as average from images where (rating_sum / rating_count) > 20 or rating_count=0 ORDER BY RAND() LIMIT 1 ; +----+-------------+--------+------+---------------+------+---------+------+--------+----------------------------------------------+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +----+-------------+--------+------+---------------+------+---------+------+--------+----------------------------------------------+ | 1 | SIMPLE | images | ALL | rating_count | NULL | NULL | NULL | 301937 | Using where; Using temporary; Using filesort | +----+-------------+--------+------+---------------+------+---------+------+--------+----------------------------------------------+ 1 row in set (0.00 sec) Misc Complete statement with \\G for different output format ERROR 1045 (28000) may be caused by invalid hostname in connect command. Replace the --host token with the full hostname of the db server. Or, restart mysql and try again. Recommended reading MySQL (5th Edition) (Developer's Library) High Performance MySQL: Optimization, Backups, Replication, and More See Also http://www.sqlalchemy.org/ - ORM, better for abstracting database in code http://www.mycli.net/ - A better CLI for MySQL https://modern-sql.com/blog/2018-04/mysql-8.0 - Good comparison of modern (as of 2018) SQL options","title":"MySQL"},{"location":"mysql/#examples","text":"","title":"Examples"},{"location":"mysql/#show-variables-of-the-running-server","text":"mysqladmin variables","title":"Show variables of the running server"},{"location":"mysql/#enable-bin-logging","text":"Edit /etc/my.cnf: log-bin=/var/lib/mysql/mysql-bin","title":"Enable bin logging"},{"location":"mysql/#show-how-a-table-was-created","text":"SHOW CREATE TABLE table_name \\G","title":"Show how a table was created"},{"location":"mysql/#create-a-table","text":"CREATE TABLE photo_sizes ( `photo_id` char(32) NOT NULL, `format` mediumtext, `width` mediumtext, `height` mediumtext, `source` mediumtext, `url` mediumtext, PRIMARY KEY(`photo_id`) ) ;","title":"Create a table"},{"location":"mysql/#create-a-table-with-multiple-columns-as-the-primary-key","text":"CREATE TABLE `photo_sizes` ( `photo_id` char(32) NOT NULL, `format` char(32) NOT NULL DEFAULT '', `width` mediumtext, `height` mediumtext, `source` mediumtext, `url` mediumtext, PRIMARY KEY (`photo_id`,`format`) ) ENGINE=MyISAM DEFAULT CHARSET=latin1","title":"Create a table with multiple columns as the primary key"},{"location":"mysql/#show-what-processes-are-running","text":"show processlist;","title":"Show what processes are running"},{"location":"mysql/#dump-databases-to-sql-files","text":"All databases mysqldump -u root -phunter2 --all-databases | gzip -9 > ~/$(date +%F-%H%m).sql.gz Or just a single database mysqldump -u root -phunter2 my_favorite_db | gzip -9 > ~/my_favorite_db-$(date +%F-%H%m).sql.gz","title":"Dump databases to sql files"},{"location":"mysql/#duplicate-a-database","text":"sudo mysqldump -v mogilefs | sudo mysql -D mogilefs_sjc","title":"Duplicate a database"},{"location":"mysql/#dump-the-schema-of-a-database-with-no-actual-data","text":"sudo mysqldump --no-data dbname > schema.sql","title":"Dump the schema of a database with no actual data"},{"location":"mysql/#show-privileges","text":"show GRANTS ;","title":"Show privileges"},{"location":"mysql/#create-a-new-user","text":"CREATE USER 'a_new_user'@'10.0.5.%' IDENTIFIED BY 'the_user_password'; GRANT ALL PRIVILEGES ON some_database.* TO 'a_new_user'@'10.0.5.%' WITH GRANT OPTION;","title":"Create a new user"},{"location":"mysql/#delete-a-user","text":"DELETE from mysql.user where user = 'user_name';","title":"Delete a user"},{"location":"mysql/#grant-privileges","text":"GRANT ALL ON database.* TO 'newuser'@'localhost';","title":"Grant Privileges"},{"location":"mysql/#change-root-password","text":"/usr/bin/mysqladmin -u root password 'new-password' /usr/bin/mysqladmin -u root -h hostname password 'new-password' or... UPDATE mysql.user SET Password=PASSWORD('hunter2') WHERE User='leroy_jenkins' AND Host='localhost' ;","title":"Change root password"},{"location":"mysql/#create-statements","text":"","title":"Create statements"},{"location":"mysql/#create-an-index-on-table-images-for-column-rating_count","text":"create index rating_count on images (rating_count) ;","title":"Create an index on table images for column rating_count"},{"location":"mysql/#drop-an-index-from-a-table","text":"drop index rating_count on images ;","title":"Drop an index from a table"},{"location":"mysql/#table-alters","text":"","title":"Table Alters"},{"location":"mysql/#add-a-column","text":"alter table flixplor add o_width char(12);","title":"Add a column"},{"location":"mysql/#drop-a-column","text":"alter table flixplor drop column o_width;","title":"Drop a column"},{"location":"mysql/#change-the-type-of-a-column","text":"alter table flixplor modify o_height mediumint ;","title":"Change the type of a column"},{"location":"mysql/#add-a-current-timestamp-column","text":"alter table images add last_updated timestamp not null default current_timestamp on update current_timestamp;","title":"Add a current timestamp column"},{"location":"mysql/#change-the-table-engine-to-innodb","text":"ALTER TABLE images ENGINE=INNODB;","title":"Change the table engine to innodb"},{"location":"mysql/#change-a-tables-encoding","text":"alter table raw_flickr_data CONVERT TO CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci ;","title":"Change a table's encoding"},{"location":"mysql/#table-inserts","text":"","title":"Table Inserts"},{"location":"mysql/#add-a-record","text":"","title":"Add a record"},{"location":"mysql/#table-updates","text":"","title":"Table Updates"},{"location":"mysql/#update-if-key-exists","text":"For unique fields, you cannot insert, so you need to have an on duplicate key section in your statement. INSERT INTO photo_sizes ( photo_id, format, height, width, url, source ) values ( '32704962848', 'Medium 640', '427', '640', 'https://www.flickr.com/photos/warzauwynn/32704962848/sizes/z/', 'https://farm5.staticflickr.com/4855/32704962848_3a96b4c635_z.jpg' ) ON DUPLICATE KEY UPDATE height='427', width='640', url='https://www.flickr.com/photos/warzauwynn/32704962848/sizes/z/', source='https://farm5.staticflickr.com/4855/32704962848_3a96b4c635_z.jpg' http://dev.mysql.com/doc/refman/5.0/en/insert-on-duplicate.html","title":"Update if key exists"},{"location":"mysql/#update-a-datetime-column-with-a-random-datetime","text":"UPDATE flixplor SET last_retrieval = subtime( concat_ws(' ','2019-01-01' - interval rand() * 10000 day ,'00:00:00'), -- create a random YYYY-MM-DD within 10k days of given datetime sec_to_time(floor(0 + (rand() * 86401))) -- create a random HH:MM:SS within a 24 hour period ) WHERE last_retrieval < '2019-01-01 00:00:00';","title":"Update a datetime column with a random datetime"},{"location":"mysql/#table-selects","text":"","title":"Table Selects"},{"location":"mysql/#select-values-and-dont-show-duplicates","text":"SELECT col from servers group by col ;","title":"Select values and don't show duplicates"},{"location":"mysql/#select-photo_id-and-discard-duplicates-uniq","text":"SELECT photo_id from photo_sizes group by photo_id ;","title":"Select photo_id and discard duplicates (uniq)"},{"location":"mysql/#select-and-count-unique-pairs-of-columns","text":"SELECT model, unit, count(*) as n from servers group by model, unit having n > 1 order by model asc ;","title":"Select and count unique pairs of columns"},{"location":"mysql/#select-the-count-of-rows-in-a-table","text":"SELECT count(*) from flixplor where o_height > 100 ;","title":"Select the count of rows in a table"},{"location":"mysql/#do-some-math-to-create-a-new-column-during-a-select","text":"SELECT photo_id,last_retrieval,o_height,o_width,(o_height * o_width) as pixels from flixplor where last_reposted < from_unixtime('1384268667') or last_reposted is NULL order by (o_height * o_width) limit 10 ;","title":"Do some math to create a new column during a select"},{"location":"mysql/#transform-datetime-into-a-date-diff","text":"This selects the number of hours since the given datestamp instead of the datestamp itself. SELECT TIMESTAMPDIFF(HOUR, date_taken, NOW()) from photos ; See also DATEDIFF.","title":"Transform datetime into a date diff"},{"location":"mysql/#statement-explanations","text":"The EXPLAIN statement can give you additional info about how complex your statement is.","title":"Statement explanations"},{"location":"mysql/#explain-select","text":"mysql> explain SELECT *,(rating_sum / rating_count) as average from images where (rating_sum / rating_count) > 20 or rating_count=0 ORDER BY RAND() LIMIT 1 ; +----+-------------+--------+------+---------------+------+---------+------+--------+----------------------------------------------+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +----+-------------+--------+------+---------------+------+---------+------+--------+----------------------------------------------+ | 1 | SIMPLE | images | ALL | rating_count | NULL | NULL | NULL | 301937 | Using where; Using temporary; Using filesort | +----+-------------+--------+------+---------------+------+---------+------+--------+----------------------------------------------+ 1 row in set (0.00 sec)","title":"Explain select"},{"location":"mysql/#misc","text":"Complete statement with \\G for different output format ERROR 1045 (28000) may be caused by invalid hostname in connect command. Replace the --host token with the full hostname of the db server. Or, restart mysql and try again.","title":"Misc"},{"location":"mysql/#recommended-reading","text":"MySQL (5th Edition) (Developer's Library) High Performance MySQL: Optimization, Backups, Replication, and More","title":"Recommended reading"},{"location":"mysql/#see-also","text":"http://www.sqlalchemy.org/ - ORM, better for abstracting database in code http://www.mycli.net/ - A better CLI for MySQL https://modern-sql.com/blog/2018-04/mysql-8.0 - Good comparison of modern (as of 2018) SQL options","title":"See Also"},{"location":"namei/","text":"\"follow a pathname until a terminal point is found\" - man namei Examples Simple usage # namei /etc/systemd/system/multi-user.target.wants/ssh.service f: /etc/systemd/system/multi-user.target.wants/ssh.service d / d etc d systemd d system d multi-user.target.wants l ssh.service -> /lib/systemd/system/ssh.service d / d lib d systemd d system - ssh.service Show permissions of all entries # namei -l /etc/systemd/system/multi-user.target.wants/ssh.service f: /etc/systemd/system/multi-user.target.wants/ssh.service drwxr-xr-x root root / drwxr-xr-x root root etc drwxr-xr-x root root systemd drwxr-xr-x root root system drwxr-xr-x root root multi-user.target.wants lrwxrwxrwx root root ssh.service -> /lib/systemd/system/ssh.service drwxr-xr-x root root / drwxr-xr-x root root lib drwxr-xr-x root root systemd drwxr-xr-x root root system -rw-r--r-- root root ssh.service","title":"namei"},{"location":"namei/#examples","text":"","title":"Examples"},{"location":"namei/#simple-usage","text":"# namei /etc/systemd/system/multi-user.target.wants/ssh.service f: /etc/systemd/system/multi-user.target.wants/ssh.service d / d etc d systemd d system d multi-user.target.wants l ssh.service -> /lib/systemd/system/ssh.service d / d lib d systemd d system - ssh.service","title":"Simple usage"},{"location":"namei/#show-permissions-of-all-entries","text":"# namei -l /etc/systemd/system/multi-user.target.wants/ssh.service f: /etc/systemd/system/multi-user.target.wants/ssh.service drwxr-xr-x root root / drwxr-xr-x root root etc drwxr-xr-x root root systemd drwxr-xr-x root root system drwxr-xr-x root root multi-user.target.wants lrwxrwxrwx root root ssh.service -> /lib/systemd/system/ssh.service drwxr-xr-x root root / drwxr-xr-x root root lib drwxr-xr-x root root systemd drwxr-xr-x root root system -rw-r--r-- root root ssh.service","title":"Show permissions of all entries"},{"location":"ncftp/","text":"\"NcFTP Client is a set of FREE application programs implementing the File Transfer Protocol. ... The program has been in service on UNIX systems since 1991 and is a popular alternative to the standard FTP program, /usr/bin/ftp.\" - https://www.ncftp.com/ncftp/ Examples Connect to a non-standard port ncftp -P 5000 10.8.5.103 Recursively put a directory put -R localdir","title":"ncftp"},{"location":"ncftp/#examples","text":"","title":"Examples"},{"location":"ncftp/#connect-to-a-non-standard-port","text":"ncftp -P 5000 10.8.5.103","title":"Connect to a non-standard port"},{"location":"ncftp/#recursively-put-a-directory","text":"put -R localdir","title":"Recursively put a directory"},{"location":"netgear/","text":"Netgear R7000 DD-WRT https://www.myopenrouter.com/downloads/dd-wrt-r7000 http://www.desipro.de/ddwrt/K3-AC-Arm/ See Also: dd-wrt Netgear GSS116E https://www.netgear.com/support/product/GSS116E Netgear M4300-8X8F \"Stackable Managed Switch with 16x10G including 8x10GBASE-T and 8xSFP+ Layer 3\" https://www.netgear.com/support/product/M4300-8X8F Arlo Pros Completely wireless solution available but not required. Cons Requires internet access to interact with, even for cameras that are accessible on the same LAN.","title":"NETGEAR"},{"location":"netgear/#netgear-r7000","text":"","title":"Netgear R7000"},{"location":"netgear/#dd-wrt","text":"https://www.myopenrouter.com/downloads/dd-wrt-r7000 http://www.desipro.de/ddwrt/K3-AC-Arm/ See Also: dd-wrt","title":"DD-WRT"},{"location":"netgear/#netgear-gss116e","text":"https://www.netgear.com/support/product/GSS116E","title":"Netgear GSS116E"},{"location":"netgear/#netgear-m4300-8x8f","text":"\"Stackable Managed Switch with 16x10G including 8x10GBASE-T and 8xSFP+ Layer 3\" https://www.netgear.com/support/product/M4300-8X8F","title":"Netgear M4300-8X8F"},{"location":"netgear/#arlo","text":"","title":"Arlo"},{"location":"netgear/#pros","text":"Completely wireless solution available but not required.","title":"Pros"},{"location":"netgear/#cons","text":"Requires internet access to interact with, even for cameras that are accessible on the same LAN.","title":"Cons"},{"location":"nethogs/","text":"\"Linux 'net top' tool\" - https://github.com/raboof/nethogs Nethogs shows you which PIDs used or are using how much bandwidth. See also Top variant list","title":"nethogs"},{"location":"nethogs/#see-also","text":"Top variant list","title":"See also"},{"location":"networking/","text":"Links https://en.wikipedia.org/wiki/Reserved_IP_addresses https://wiki.debian.org/BridgeNetworkConnectionsProxyArp See also bind - DNS server iftop - interface top ip command for linux iperf - network performance testing iptables - linux firewall linksys - soho network hardware vendor netgear - network hardware vendor networksetup - Mac OSX configuration tool for network settings in System Preferences ntop - network top OSI model - The Open Systems Interconnection model (OSI model) is a conceptual model that characterizes and standardizes the communication functions of a telecommunication or computing system without regard to its underlying internal structure and technology. pac - dynamic proxy configuration procurve - HP managed networking","title":"networking"},{"location":"networking/#links","text":"https://en.wikipedia.org/wiki/Reserved_IP_addresses https://wiki.debian.org/BridgeNetworkConnectionsProxyArp","title":"Links"},{"location":"networking/#see-also","text":"bind - DNS server iftop - interface top ip command for linux iperf - network performance testing iptables - linux firewall linksys - soho network hardware vendor netgear - network hardware vendor networksetup - Mac OSX configuration tool for network settings in System Preferences ntop - network top OSI model - The Open Systems Interconnection model (OSI model) is a conceptual model that characterizes and standardizes the communication functions of a telecommunication or computing system without regard to its underlying internal structure and technology. pac - dynamic proxy configuration procurve - HP managed networking","title":"See also"},{"location":"networksetup/","text":"\"networksetup -- configuration tool for network settings in System Preferences.\" - man networksetup networksetup is a standard tool on osx Examples shows the relevant info for the device named Wi-Fi networksetup -getinfo \"Wi-Fi\" shows all connected hardware ports networksetup -listallhardwareports Show all search domains networksetup -listallnetworkservices | tail -n +2 | xargs -I :: networksetup -getsearchdomains \"::\" Create a bunch of VLAN interfaces for X in {1..32} ; do sudo networksetup -createVLAN \"vlan${X}\" en3 \"${X}\" ; done ; Delete a bunch of VLAN interfaces for X in {1..32} ; do sudo networksetup -deleteVLAN \"vlan${X}\" en3 \"${X}\" ; done ;","title":"networksetup"},{"location":"networksetup/#examples","text":"","title":"Examples"},{"location":"networksetup/#shows-the-relevant-info-for-the-device-named-wi-fi","text":"networksetup -getinfo \"Wi-Fi\"","title":"shows the relevant info for the device named Wi-Fi"},{"location":"networksetup/#shows-all-connected-hardware-ports","text":"networksetup -listallhardwareports","title":"shows all connected hardware ports"},{"location":"networksetup/#show-all-search-domains","text":"networksetup -listallnetworkservices | tail -n +2 | xargs -I :: networksetup -getsearchdomains \"::\"","title":"Show all search domains"},{"location":"networksetup/#create-a-bunch-of-vlan-interfaces","text":"for X in {1..32} ; do sudo networksetup -createVLAN \"vlan${X}\" en3 \"${X}\" ; done ;","title":"Create a bunch of VLAN interfaces"},{"location":"networksetup/#delete-a-bunch-of-vlan-interfaces","text":"for X in {1..32} ; do sudo networksetup -deleteVLAN \"vlan${X}\" en3 \"${X}\" ; done ;","title":"Delete a bunch of VLAN interfaces"},{"location":"neverware/","text":"\"Cutting-edge hardware? Not as important as a cutting-edge OS.\" - https://www.neverware.com/ \"CloudReady is based on Google\u2019s Chromium OS, the same open-source architecture as Chrome OS.\" - https://www.neverware.com/ Home Edition: https://www.neverware.com/freedownload","title":"neverware"},{"location":"nfs/","text":"nfs is the Network File System. - Configured in linux at /etc/exports - Great info here: http://nfs.sourceforge.net/ Tips and Tricks Regarding mount points within shares If you have a mount point within an NFS share, you must have a separate entry in your exports file that sets the permissions of this mount point. Currently OS X has a problem with this, but officially this is the way to do it. Show hosts that are connected to this NFS server showmount Show what hosts are using what exports showmount -a Show exported directories showmount -e Show directories in use by NFS showmount -d Add an NFS mount to fstab opal:/z4 /mnt/z4 nfs rsize=8192,wsize=8192,timeo=14,intr Linux Tips and Tricks Show which versions of NFS your NFS server supports rpcinfo -p Allow an OS X client to mount nfs4 nested zfs data sets OS X has problems with the privileged port default requirement in nfs4, so the insecure option is required. The nohide option allows you to mount nested zfs datasets, instead of requiring a separate export for each dataset. /z4 *.local(rw,async,no_subtree_check,insecure,nohide) OS X Tips and Tricks Create persistent NFS mount in OS X 10.8 This is not bulletproof. Modern OS X 10.9+ version are switching away from NFS to CIFS. The NFS client on OS X is pretty weak. For instance it might crash your machine if the share has 0 bytes free but is mounted RW. Use at your own risk. sudo mkdir /mnt # OS X doesn't like you playing with /Volumes, it may delete your dirs sudo dscl . -create /Mounts/z4 sudo dscl . -create /Mounts/z4 VFSLinkDir /mnt/z4 sudo dscl . -create /Mounts/z4 VFSOpts resvport rw nosuid sudo dscl . -create /Mounts/z4 VFSType nfs sudo dscl . -create /Mounts/z4 RecordName opal:/z4 sudo dscl . -create /Mounts/iTunes sudo dscl . -create /Mounts/iTunes VFSLinkDir /mnt/z4/iTunes sudo dscl . -create /Mounts/iTunes VFSOpts resvport rw nosuid sudo dscl . -create /Mounts/iTunes VFSType nfs sudo dscl . -create /Mounts/iTunes RecordName opal:/z4/iTunes sudo dscl . -read /Mounts/opal:/z4 sudo dscl . -read /Mounts/opal:/z4/iTunes sudo dscl . -list /Mounts sudo dscl . -delete /Mounts opal:/z4/iTunes","title":"nfs"},{"location":"nfs/#tips-and-tricks","text":"","title":"Tips and Tricks"},{"location":"nfs/#regarding-mount-points-within-shares","text":"If you have a mount point within an NFS share, you must have a separate entry in your exports file that sets the permissions of this mount point. Currently OS X has a problem with this, but officially this is the way to do it.","title":"Regarding mount points within shares"},{"location":"nfs/#show-hosts-that-are-connected-to-this-nfs-server","text":"showmount","title":"Show hosts that are connected to this NFS server"},{"location":"nfs/#show-what-hosts-are-using-what-exports","text":"showmount -a","title":"Show what hosts are using what exports"},{"location":"nfs/#show-exported-directories","text":"showmount -e","title":"Show exported directories"},{"location":"nfs/#show-directories-in-use-by-nfs","text":"showmount -d","title":"Show directories in use by NFS"},{"location":"nfs/#add-an-nfs-mount-to-fstab","text":"opal:/z4 /mnt/z4 nfs rsize=8192,wsize=8192,timeo=14,intr","title":"Add an NFS mount to fstab"},{"location":"nfs/#linux-tips-and-tricks","text":"","title":"Linux Tips and Tricks"},{"location":"nfs/#show-which-versions-of-nfs-your-nfs-server-supports","text":"rpcinfo -p","title":"Show which versions of NFS your NFS server supports"},{"location":"nfs/#allow-an-os-x-client-to-mount-nfs4-nested-zfs-data-sets","text":"OS X has problems with the privileged port default requirement in nfs4, so the insecure option is required. The nohide option allows you to mount nested zfs datasets, instead of requiring a separate export for each dataset. /z4 *.local(rw,async,no_subtree_check,insecure,nohide)","title":"Allow an OS X client to mount nfs4 nested zfs data sets"},{"location":"nfs/#os-x-tips-and-tricks","text":"","title":"OS X Tips and Tricks"},{"location":"nfs/#create-persistent-nfs-mount-in-os-x-108","text":"This is not bulletproof. Modern OS X 10.9+ version are switching away from NFS to CIFS. The NFS client on OS X is pretty weak. For instance it might crash your machine if the share has 0 bytes free but is mounted RW. Use at your own risk. sudo mkdir /mnt # OS X doesn't like you playing with /Volumes, it may delete your dirs sudo dscl . -create /Mounts/z4 sudo dscl . -create /Mounts/z4 VFSLinkDir /mnt/z4 sudo dscl . -create /Mounts/z4 VFSOpts resvport rw nosuid sudo dscl . -create /Mounts/z4 VFSType nfs sudo dscl . -create /Mounts/z4 RecordName opal:/z4 sudo dscl . -create /Mounts/iTunes sudo dscl . -create /Mounts/iTunes VFSLinkDir /mnt/z4/iTunes sudo dscl . -create /Mounts/iTunes VFSOpts resvport rw nosuid sudo dscl . -create /Mounts/iTunes VFSType nfs sudo dscl . -create /Mounts/iTunes RecordName opal:/z4/iTunes sudo dscl . -read /Mounts/opal:/z4 sudo dscl . -read /Mounts/opal:/z4/iTunes sudo dscl . -list /Mounts sudo dscl . -delete /Mounts opal:/z4/iTunes","title":"Create persistent NFS mount in OS X 10.8"},{"location":"nintendo-3ds/","text":"Lego + 3DS notes 3DSXL screen is ~ 9x11 Lego units 3DSXL outside is 20x12 Lego units Emulation Citra 3DS emulator Dumping cartridges Hacking and CFW https://3ds.hacks.guide/ https://www.3dbrew.org/wiki/Homebrew_Applications https://zoogie.github.io/web/34%E2%85%95c3/#/ https://www.reddit.com/r/3dshacks/comments/6iclr8/a_technical_overview_of_the_3ds_operating_system/ https://youtu.be/ImR-TdDAIJE - How a Terrible Game Cracked the 3DS's Security - Early Days of 3DS Hacking https://www.youtube.com/watch?v=bZczf57HSag - (2015 32c3 talk) 3DS console hacking deep dive https://www.youtube.com/watch?v=8C5cn_Qj0G8 - Nintendo Hacking 2016 33c3","title":"Nintendo 3DS"},{"location":"nintendo-3ds/#lego-3ds-notes","text":"3DSXL screen is ~ 9x11 Lego units 3DSXL outside is 20x12 Lego units","title":"Lego + 3DS notes"},{"location":"nintendo-3ds/#emulation","text":"Citra 3DS emulator Dumping cartridges","title":"Emulation"},{"location":"nintendo-3ds/#hacking-and-cfw","text":"https://3ds.hacks.guide/ https://www.3dbrew.org/wiki/Homebrew_Applications https://zoogie.github.io/web/34%E2%85%95c3/#/ https://www.reddit.com/r/3dshacks/comments/6iclr8/a_technical_overview_of_the_3ds_operating_system/ https://youtu.be/ImR-TdDAIJE - How a Terrible Game Cracked the 3DS's Security - Early Days of 3DS Hacking https://www.youtube.com/watch?v=bZczf57HSag - (2015 32c3 talk) 3DS console hacking deep dive https://www.youtube.com/watch?v=8C5cn_Qj0G8 - Nintendo Hacking 2016 33c3","title":"Hacking and CFW"},{"location":"nintendo-amiibo/","text":"Amiibo are NFC figurines that enable in-game features on Nintendo 3DS, Wii U and Switch platforms. Info Uses NTAG215 RFID chips. Links How to Mimic Any Amiibo https://www.codejunkies.com/powersaves-for-amiibo/ http://amiibo.wikia.com/wiki/Amiibo_Wiki http://nintendo.wikia.com/wiki/List_of_Amiibo_compatible_Games https://www.nintendo.com/amiibo/games https://nfc-bank.com https://github.com/HiddenRamblings/TagMo - TagMo is an Android app which allows for cloning Amiibos using blank NTAG215 NFC tags.","title":"Nintendo Amiibo"},{"location":"nintendo-amiibo/#info","text":"Uses NTAG215 RFID chips.","title":"Info"},{"location":"nintendo-amiibo/#links","text":"How to Mimic Any Amiibo https://www.codejunkies.com/powersaves-for-amiibo/ http://amiibo.wikia.com/wiki/Amiibo_Wiki http://nintendo.wikia.com/wiki/List_of_Amiibo_compatible_Games https://www.nintendo.com/amiibo/games https://nfc-bank.com https://github.com/HiddenRamblings/TagMo - TagMo is an Android app which allows for cloning Amiibos using blank NTAG215 NFC tags.","title":"Links"},{"location":"nintendo-switch/","text":"Homebrew Homebrew Glossary Most of these were taken from https://github.com/XorTroll/Goldleaf NSP (Nintendo Submission Package): It's the official format used by Nintendo to provide installable content from their CDN servers. NRO (Nintendo Relocatable Object): Officially is treated as a dynamic library object, similar to DLLs on Windows. These are loaded dynamically by applications at runtime. NCA (Nintendo Content Archive): This format is the base format used by Horizon OS to store content. NACP (Nintendo Application Control Property): This is the format used by Nintendo to store several properties of installed titles, like the title's name, version, author name and other information, like if the title supports screnshots or video captures. XCI: Cartridge dump file Homebrew links https://nh-server.github.io/switch-guide/ https://github.com/AtlasNX/Kosmos/ https://github.com/XorTroll/Goldleaf https://github.com/DarkMatterCore/gcdumptool https://switchtools.sshnuke.net/ https://switchbrew.github.io/nx-hbl/ https://www.reddit.com/r/SwitchHaxing/top/?sort=top&t=month https://youtu.be/3-UeB_enPrM - How to install HomeBrew Menu on Nintendo Switch with 5.0.2 or 4.X firmware RCM","title":"Nintendo Switch"},{"location":"nintendo-switch/#homebrew","text":"","title":"Homebrew"},{"location":"nintendo-switch/#homebrew-glossary","text":"Most of these were taken from https://github.com/XorTroll/Goldleaf NSP (Nintendo Submission Package): It's the official format used by Nintendo to provide installable content from their CDN servers. NRO (Nintendo Relocatable Object): Officially is treated as a dynamic library object, similar to DLLs on Windows. These are loaded dynamically by applications at runtime. NCA (Nintendo Content Archive): This format is the base format used by Horizon OS to store content. NACP (Nintendo Application Control Property): This is the format used by Nintendo to store several properties of installed titles, like the title's name, version, author name and other information, like if the title supports screnshots or video captures. XCI: Cartridge dump file","title":"Homebrew Glossary"},{"location":"nintendo-switch/#homebrew-links","text":"https://nh-server.github.io/switch-guide/ https://github.com/AtlasNX/Kosmos/ https://github.com/XorTroll/Goldleaf https://github.com/DarkMatterCore/gcdumptool https://switchtools.sshnuke.net/ https://switchbrew.github.io/nx-hbl/ https://www.reddit.com/r/SwitchHaxing/top/?sort=top&t=month https://youtu.be/3-UeB_enPrM - How to install HomeBrew Menu on Nintendo Switch with 5.0.2 or 4.X firmware RCM","title":"Homebrew links"},{"location":"nintendo-wii/","text":"A gaming system by Nintendo. See also the Dolphin emulator. Tips In Boot-Mii you can use the power and reset buttons to navigate if you don't have a GC controller. Wii Remote Sync Standard Mode: Press the Power button on the Wii console to turn it on. Remove the battery cover on the back of the Wii Remote to be synced. Open the SD Card Slot cover on the front of the Wii console. Press and release the SYNC button just below the batteries on the Wii Remote; the Player LED on the front of the Wii Remote will blink. While the lights are still blinking, quickly press and release the red SYNC button on the front of the Wii console. When the Player LED blinking stops and stays lit, the syncing is complete. The LED that is illuminated indicates the player number (1 through 4). Hack notes IOSes Explained - http://gwht.wikidot.com/ioses-explained Backup Launcher runs ISO - http://wiihacked.com/backup-launcher-v30gamma-download-here-and-how-to-install Wiibrew is the channel for loading home-brew software. - http://wiibrew.org Wii Backup Manager is an app to help manage ISOs and filesystems - http://www.wiibackupmanager.co.uk/ Wiimms ISO Tools will modify ISOs and WBFS - http://wit.wiimm.de/ A list of WBFS managers - http://wiki.gbatemp.net/wiki/WBFS_Managers Anti-brick and general hacking tutorial - http://www.howtogeek.com/howto/38041/set-up-anti-brick-protection-to-safeguard-and-supercharge-your-wii/ https://sites.google.com/site/completesg/backup-launchers/installation - Successfully installed Trucha patched IOS to enable the USB loader https://sites.google.com/site/completesg/cios/hermes-cios - More software that needs to be installed to get USB loaders to work nicely http://www.howtogeek.com/howto/40349/install-a-wii-game-loader-for-easy-backups-and-fast-load-times/ - How to USB Load ISOs on the Wii https://code.google.com/archive/p/nusdownloader/ - NUS Downloader lets you download Various official Wii software","title":"Nintendo Wii"},{"location":"nintendo-wii/#tips","text":"In Boot-Mii you can use the power and reset buttons to navigate if you don't have a GC controller.","title":"Tips"},{"location":"nintendo-wii/#wii-remote-sync","text":"Standard Mode: Press the Power button on the Wii console to turn it on. Remove the battery cover on the back of the Wii Remote to be synced. Open the SD Card Slot cover on the front of the Wii console. Press and release the SYNC button just below the batteries on the Wii Remote; the Player LED on the front of the Wii Remote will blink. While the lights are still blinking, quickly press and release the red SYNC button on the front of the Wii console. When the Player LED blinking stops and stays lit, the syncing is complete. The LED that is illuminated indicates the player number (1 through 4).","title":"Wii Remote Sync"},{"location":"nintendo-wii/#hack-notes","text":"IOSes Explained - http://gwht.wikidot.com/ioses-explained Backup Launcher runs ISO - http://wiihacked.com/backup-launcher-v30gamma-download-here-and-how-to-install Wiibrew is the channel for loading home-brew software. - http://wiibrew.org Wii Backup Manager is an app to help manage ISOs and filesystems - http://www.wiibackupmanager.co.uk/ Wiimms ISO Tools will modify ISOs and WBFS - http://wit.wiimm.de/ A list of WBFS managers - http://wiki.gbatemp.net/wiki/WBFS_Managers Anti-brick and general hacking tutorial - http://www.howtogeek.com/howto/38041/set-up-anti-brick-protection-to-safeguard-and-supercharge-your-wii/ https://sites.google.com/site/completesg/backup-launchers/installation - Successfully installed Trucha patched IOS to enable the USB loader https://sites.google.com/site/completesg/cios/hermes-cios - More software that needs to be installed to get USB loaders to work nicely http://www.howtogeek.com/howto/40349/install-a-wii-game-loader-for-easy-backups-and-fast-load-times/ - How to USB Load ISOs on the Wii https://code.google.com/archive/p/nusdownloader/ - NUS Downloader lets you download Various official Wii software","title":"Hack notes"},{"location":"nintendo-wiiu/","text":"https://wiiu.hacks.guide/haxchi.html","title":"Nintendo Wii U"},{"location":"ntop/","text":"\"High-speed web-based traffic analysis.\" - https://www.ntop.org/ This isn't a traditional top style tool since it has a web interface. For network top in a TUI, see iftop See also Top variant list","title":"ntop"},{"location":"ntop/#see-also","text":"Top variant list","title":"See also"},{"location":"ntp/","text":"\"Network Time Protocol (NTP) is a networking protocol for clock synchronization between computer systems over packet-switched, variable-latency data networks. In operation since before 1985, NTP is one of the oldest Internet protocols in current use.\" - https://en.wikipedia.org/wiki/Network_Time_Protocol Links RFC 5905: Network Time Protocol Version 4: Protocol and Algorithms Specification Understanding and mitigating NTP-based DDoS attacks Google Public NTP: Leap Smear Five different ways to handle leap seconds with NTP The Unix leap second mess ntp.org FAQ: What happens during a Leap Second? The Raspberry Pi as a Stratum-1 NTP Server NTP vs PTP: Network Timing Smackdown! See Also time - Notes on time technologies ptp - Precision Time Protocol","title":"NTP"},{"location":"ntp/#links","text":"RFC 5905: Network Time Protocol Version 4: Protocol and Algorithms Specification Understanding and mitigating NTP-based DDoS attacks Google Public NTP: Leap Smear Five different ways to handle leap seconds with NTP The Unix leap second mess ntp.org FAQ: What happens during a Leap Second? The Raspberry Pi as a Stratum-1 NTP Server NTP vs PTP: Network Timing Smackdown!","title":"Links"},{"location":"ntp/#see-also","text":"time - Notes on time technologies ptp - Precision Time Protocol","title":"See Also"},{"location":"nvidia/","text":"Verify linux nvidia drivers are installed: nvidia-smi Verify linux nvidia drivers work within docker: docker run --runtime=nvidia --rm nvidia/cuda:9.0-base nvidia-smi Show nvidia container info: nvidia-container-cli -k -d /dev/tty info","title":"nvidia"},{"location":"openvpn/","text":"\"Your private path to access network resources and services securely\" - https://openvpn.net/","title":"openvpn"},{"location":"osquery/","text":"\"SQL powered operating system instrumentation, monitoring, and analytics.\" oquery runs locally and allows you to inspect your host using sql queries. Tables exist for a variety of useful data, such as file hashes, process list, last user login, etc.. Links https://github.com/facebook/osquery https://osquery.readthedocs.io https://osquery.io/schema/ https://github.com/UtahDave/salt-vagrant-demo https://www.digitalocean.com/community/tutorials/how-to-monitor-your-system-security-with-osquery-on-ubuntu-16-04 https://kolide.com/fleet - osquery fleet/cluster system","title":"osquery"},{"location":"osquery/#links","text":"https://github.com/facebook/osquery https://osquery.readthedocs.io https://osquery.io/schema/ https://github.com/UtahDave/salt-vagrant-demo https://www.digitalocean.com/community/tutorials/how-to-monitor-your-system-security-with-osquery-on-ubuntu-16-04 https://kolide.com/fleet - osquery fleet/cluster system","title":"Links"},{"location":"ostinato/","text":"\"Ostinato is a packet crafter, network traffic generator and analyzer with a friendly GUI. Also a powerful Python API for network test automation. Craft and send packets of several streams with different protocols at different rates. Think of it as ' in Reverse'.\" - http://ostinato.org/","title":"ostinato"},{"location":"osx/","text":"Apple's Unix desktop operating system. Nifty Commands serverinfo caffeinate sharing tccutil scutil Firewall - 10.11 From http://krypted.com/mac-security/command-line-firewall-management-in-os-x-10-10/ - /usr/libexec/ApplicationFirewall/socketfilterfw Tricks Show hardware info system_profiler SPHardwareDataType Install package from CLI sudo installer -pkg /Volumes/ExifTool-9.16/ExifTool-9.16.pkg -target / See also: https://brew.sh/ Start FTP server sudo -s launchctl load -w /System/Library/LaunchDaemons/ftp.plist Check swap usage sysctl vm.swapusage Disable wifi disconnect when locking screen sudo /System/Library/PrivateFrameworks/Apple80211.framework/Versions/Current/Resources airport en1 prefs DisconnectOnLogout=NO Show some downloaded files This shows a list of all the quarantine checked downloads: sqlite3 ~/Library/Preferences/com.apple.LaunchServices.QuarantineEventsV* 'select distinct LSQuarantineDataURLString from LSQuarantineEvent' Send Notifications from Terminal sudo gem install terminal-notifier terminal-notifier -message \"Hello, this is my message\" -title \"Message Title\" Enable verbose eap logging sudo defaults write /Library/Preferences/SystemConfiguration/com.apple.eapolclient LogFlags -int -1 Network gif0 - Generic Tunnel Interface. See man gif . stf0 - Six To Four tunnel. Migration After migrating, check these files: - /etc/hosts (not transferred using Migration Assistant in 10.7) - crontabs (not transferred using Migration Assistant in 10.7) - /etc/apache2/httpd.conf (not transferred using Migration Assistant - in 10.7)","title":"osx"},{"location":"osx/#nifty-commands","text":"serverinfo caffeinate sharing tccutil scutil","title":"Nifty Commands"},{"location":"osx/#firewall-1011","text":"From http://krypted.com/mac-security/command-line-firewall-management-in-os-x-10-10/ - /usr/libexec/ApplicationFirewall/socketfilterfw","title":"Firewall - 10.11"},{"location":"osx/#tricks","text":"","title":"Tricks"},{"location":"osx/#show-hardware-info","text":"system_profiler SPHardwareDataType","title":"Show hardware info"},{"location":"osx/#install-package-from-cli","text":"sudo installer -pkg /Volumes/ExifTool-9.16/ExifTool-9.16.pkg -target / See also: https://brew.sh/","title":"Install package from CLI"},{"location":"osx/#start-ftp-server","text":"sudo -s launchctl load -w /System/Library/LaunchDaemons/ftp.plist","title":"Start FTP server"},{"location":"osx/#check-swap-usage","text":"sysctl vm.swapusage","title":"Check swap usage"},{"location":"osx/#disable-wifi-disconnect-when-locking-screen","text":"sudo /System/Library/PrivateFrameworks/Apple80211.framework/Versions/Current/Resources airport en1 prefs DisconnectOnLogout=NO","title":"Disable wifi disconnect when locking screen"},{"location":"osx/#show-some-downloaded-files","text":"This shows a list of all the quarantine checked downloads: sqlite3 ~/Library/Preferences/com.apple.LaunchServices.QuarantineEventsV* 'select distinct LSQuarantineDataURLString from LSQuarantineEvent'","title":"Show some downloaded files"},{"location":"osx/#send-notifications-from-terminal","text":"sudo gem install terminal-notifier terminal-notifier -message \"Hello, this is my message\" -title \"Message Title\"","title":"Send Notifications from Terminal"},{"location":"osx/#enable-verbose-eap-logging","text":"sudo defaults write /Library/Preferences/SystemConfiguration/com.apple.eapolclient LogFlags -int -1","title":"Enable verbose eap logging"},{"location":"osx/#network","text":"gif0 - Generic Tunnel Interface. See man gif . stf0 - Six To Four tunnel.","title":"Network"},{"location":"osx/#migration","text":"After migrating, check these files: - /etc/hosts (not transferred using Migration Assistant in 10.7) - crontabs (not transferred using Migration Assistant in 10.7) - /etc/apache2/httpd.conf (not transferred using Migration Assistant - in 10.7)","title":"Migration"},{"location":"outline/","text":"\"Journalists need safe access to information to research issues, communicate with sources, and report the news. Outline lets news organizations easily provide their network safer access to the open internet.\" - https://getoutline.org/ Outline is a self hosted VPN that is geared around easy setup. Source code available at https://github.com/jigsaw-code","title":"outline"},{"location":"pac/","text":"Information about proxy auto-config files. Example pac file The following pac file will Redirect all traffic destined to 192.168.1.0/24 to a proxy running on localhost:47000 , but only if we do not have an ip address in that subnet Redirect all traffic destined to 172.16.0.0/16 to a proxy running on localhost:33001 All other traffic bypasses the proxy. function FindProxyForURL(url, host) { if ((isInNet(host, \"192.168.1.0\", \"255.255.255.0\")) && (! isInNet(myIpAddress(), \"192.168.1.0\", \"255.255.255.0\"))) { return \"SOCKS5 localhost:47000\" ; } else if (isInNet(host, \"172.16.0.0\", \"255.255.0.0\")) { return \"SOCKS5 localhost:33001\" ; } else { return \"DIRECT\" ; } } Links http://findproxyforurl.com/official-toolset https://github.com/pacparser/pacparser","title":"pac"},{"location":"pac/#example-pac-file","text":"The following pac file will Redirect all traffic destined to 192.168.1.0/24 to a proxy running on localhost:47000 , but only if we do not have an ip address in that subnet Redirect all traffic destined to 172.16.0.0/16 to a proxy running on localhost:33001 All other traffic bypasses the proxy. function FindProxyForURL(url, host) { if ((isInNet(host, \"192.168.1.0\", \"255.255.255.0\")) && (! isInNet(myIpAddress(), \"192.168.1.0\", \"255.255.255.0\"))) { return \"SOCKS5 localhost:47000\" ; } else if (isInNet(host, \"172.16.0.0\", \"255.255.0.0\")) { return \"SOCKS5 localhost:33001\" ; } else { return \"DIRECT\" ; } }","title":"Example pac file"},{"location":"pac/#links","text":"http://findproxyforurl.com/official-toolset https://github.com/pacparser/pacparser","title":"Links"},{"location":"pandoc/","text":"Convert between document formats. http://pandoc.org/ Examples Convert a doc from mediawiki to markdown pandoc -f mediawiki -t markdown nfs.mediawiki > nfs.md","title":"pandoc"},{"location":"pandoc/#examples","text":"","title":"Examples"},{"location":"pandoc/#convert-a-doc-from-mediawiki-to-markdown","text":"pandoc -f mediawiki -t markdown nfs.mediawiki > nfs.md","title":"Convert a doc from mediawiki to markdown"},{"location":"panopticlick/","text":"\"Panopticlick will analyze how well your browser and add-ons protect you against online tracking techniques. We\u2019ll also see if your system is uniquely configured\u2014and thus identifiable\u2014even if you are using privacy-protective software.\" - https://panopticlick.eff.org/","title":"panopticlick"},{"location":"passwords/","text":"\"A secret word or phrase that must be used to gain admission to something.\" - https://en.oxforddictionaries.com/definition/password Generation pwgen $ pwgen 12 3 ahZielooC4ei Ielui3ahh9su aiZoa7fioy1o apg This tool lets you show how to pronounce the random password $ apg -a 1 -m 6 -n 3 -l I[hM@}]t: India-LEFT_BRACKET-hotel-Mike-AT_SIGN-RIGHT_BRACE-RIGHT_BRACKET-tango-COLON WoqrJ}R+ps Whiskey-oscar-quebec-romeo-Juliett-RIGHT_BRACE-Romeo-PLUS_SIGN-papa-sierra zni6VC3 zulu-november-india-SIX-Victor-Charlie-THREE Links https://nakedsecurity.sophos.com/2016/08/18/nists-new-password-rules-what-you-need-to-know/ https://pages.nist.gov/800-63-3/sp800-63-3.html","title":"passwords"},{"location":"passwords/#generation","text":"","title":"Generation"},{"location":"passwords/#pwgen","text":"$ pwgen 12 3 ahZielooC4ei Ielui3ahh9su aiZoa7fioy1o","title":"pwgen"},{"location":"passwords/#apg","text":"This tool lets you show how to pronounce the random password $ apg -a 1 -m 6 -n 3 -l I[hM@}]t: India-LEFT_BRACKET-hotel-Mike-AT_SIGN-RIGHT_BRACE-RIGHT_BRACKET-tango-COLON WoqrJ}R+ps Whiskey-oscar-quebec-romeo-Juliett-RIGHT_BRACE-Romeo-PLUS_SIGN-papa-sierra zni6VC3 zulu-november-india-SIX-Victor-Charlie-THREE","title":"apg"},{"location":"passwords/#links","text":"https://nakedsecurity.sophos.com/2016/08/18/nists-new-password-rules-what-you-need-to-know/ https://pages.nist.gov/800-63-3/sp800-63-3.html","title":"Links"},{"location":"perl/","text":"Practical Extraction and Reporting Language Special Variables \"That thing\": $_ Record Separator: $/ Techniques Assign an array to some matches @array_of_matches = ($source_string =~ m/..pattern../g); Assign several variables to some matches my ($num, $a, $t) = ($_ =~ m/([0-9]*)\\. (.*) - (.*)\\.mp3/) ; Iterate a hash while(($key, $value) = each(%$_)){ print \"$value is $key\\n\" ; } Print out a file with line numbers cat ~/.bash_history | perl -nle 'print \"$.\\t$_\";' This should probably be done with nl -ba .bash_history instead. Edit a file in-place To change all instances of \"foo\" to \"bar\": perl -i -pe 's/foo/bar/g' filename.txt Remove blank lines from a file perl -pi -e \"s/^\\n//\" file.txt Remove lines from a file that match a certain regex perl -i -pe 'if ($_ =~ m/string to remove/ ){$_ = \"\";}' filename.txt Sort a line by spaces See bash for a bash-only way echo -n \"whiskey tango foxtrot \" \\ | perl -e ' $/=\" \" ; @foo = <STDIN> ; print (sort(@foo)) ; print \"\\n\" ; ' Sort records in a file that are separated by a blank line #!/usr/bin/perl $/ = \"\\n\\n\" ; my @input = (<STDIN>) ; my @sorted = sort { lc($a) cmp lc($b) } @input ; foreach (@sorted) { if (length($_) > 10) { print \"$_\"; } } Subtract two from the last octet of a MAC address for X in 24:b6:fd:ff:b7:f{{a..f},{0..9}} ; do echo -n \"${X} - 2 = \" ; echo ${X} \\ | perl -ne ' @foo = split(\":\",$_) ; $foo[5] = sprintf(\"%02x\", (hex($foo[5]) - 2)) ; $new = join(\":\",@foo) ; print \"$new\\n\" ; ' ; done ; Add one to the last octet of a MAC address for X in 24:b6:fd:ff:b7:c{{a..f},{0..9}} ; do echo ${X} \\ | perl -ne ' @foo = split(\":\",$_) ; $foo[5] = sprintf(\"%02x\", (hex($foo[5]) + 1)) ; $new = join(\":\",@foo) ; print \"$new\\n\"; ' ; done ;","title":"perl"},{"location":"perl/#special-variables","text":"\"That thing\": $_ Record Separator: $/","title":"Special Variables"},{"location":"perl/#techniques","text":"","title":"Techniques"},{"location":"perl/#assign-an-array-to-some-matches","text":"@array_of_matches = ($source_string =~ m/..pattern../g);","title":"Assign an array to some matches"},{"location":"perl/#assign-several-variables-to-some-matches","text":"my ($num, $a, $t) = ($_ =~ m/([0-9]*)\\. (.*) - (.*)\\.mp3/) ;","title":"Assign several variables to some matches"},{"location":"perl/#iterate-a-hash","text":"while(($key, $value) = each(%$_)){ print \"$value is $key\\n\" ; }","title":"Iterate a hash"},{"location":"perl/#print-out-a-file-with-line-numbers","text":"cat ~/.bash_history | perl -nle 'print \"$.\\t$_\";' This should probably be done with nl -ba .bash_history instead.","title":"Print out a file with line numbers"},{"location":"perl/#edit-a-file-in-place","text":"To change all instances of \"foo\" to \"bar\": perl -i -pe 's/foo/bar/g' filename.txt","title":"Edit a file in-place"},{"location":"perl/#remove-blank-lines-from-a-file","text":"perl -pi -e \"s/^\\n//\" file.txt","title":"Remove blank lines from a file"},{"location":"perl/#remove-lines-from-a-file-that-match-a-certain-regex","text":"perl -i -pe 'if ($_ =~ m/string to remove/ ){$_ = \"\";}' filename.txt","title":"Remove lines from a file that match a certain regex"},{"location":"perl/#sort-a-line-by-spaces","text":"See bash for a bash-only way echo -n \"whiskey tango foxtrot \" \\ | perl -e ' $/=\" \" ; @foo = <STDIN> ; print (sort(@foo)) ; print \"\\n\" ; '","title":"Sort a line by spaces"},{"location":"perl/#sort-records-in-a-file-that-are-separated-by-a-blank-line","text":"#!/usr/bin/perl $/ = \"\\n\\n\" ; my @input = (<STDIN>) ; my @sorted = sort { lc($a) cmp lc($b) } @input ; foreach (@sorted) { if (length($_) > 10) { print \"$_\"; } }","title":"Sort records in a file that are separated by a blank line"},{"location":"perl/#subtract-two-from-the-last-octet-of-a-mac-address","text":"for X in 24:b6:fd:ff:b7:f{{a..f},{0..9}} ; do echo -n \"${X} - 2 = \" ; echo ${X} \\ | perl -ne ' @foo = split(\":\",$_) ; $foo[5] = sprintf(\"%02x\", (hex($foo[5]) - 2)) ; $new = join(\":\",@foo) ; print \"$new\\n\" ; ' ; done ;","title":"Subtract two from the last octet of a MAC address"},{"location":"perl/#add-one-to-the-last-octet-of-a-mac-address","text":"for X in 24:b6:fd:ff:b7:c{{a..f},{0..9}} ; do echo ${X} \\ | perl -ne ' @foo = split(\":\",$_) ; $foo[5] = sprintf(\"%02x\", (hex($foo[5]) + 1)) ; $new = join(\":\",@foo) ; print \"$new\\n\"; ' ; done ;","title":"Add one to the last octet of a MAC address"},{"location":"pgp/","text":"\"Pretty Good Privacy (PGP) is an encryption program that provides cryptographic privacy and authentication for data communication. PGP is often used for signing, encrypting, and decrypting texts, e-mails, files, directories, and whole disk partitions and to increase the security of e-mail communications. It was created by Phil Zimmermann in 1991.\" - https://en.wikipedia.org/wiki/Pretty_Good_Privacy \"GNU Privacy Guard (GnuPG or GPG) is a free software replacement for Symantec's PGP cryptographic software suite. GnuPG is compliant with RFC 4880, which is the IETF standards track specification of OpenPGP. Modern versions of PGP and Veridis' Filecrypt are interoperable with GnuPG and other OpenPGP-compliant systems.\" - https://en.wikipedia.org/wiki/GNU_Privacy_Guard Links Technology The GNU Privacy Guard Creating a new GPG key How to create a PGP/GPG-key free of SHA-1 Web of Trust A draft guide to organizing or participating in a PGP key signing party OpenPGP key paper slip generator PIUS: The PGP Individual UID Signer Philosophy Op-ed: I'm throwing in the towel on PGP, and I work in security Op-ed: Why I'm not giving up on PGP","title":"pgp"},{"location":"pgp/#links","text":"","title":"Links"},{"location":"pgp/#technology","text":"The GNU Privacy Guard Creating a new GPG key How to create a PGP/GPG-key free of SHA-1","title":"Technology"},{"location":"pgp/#web-of-trust","text":"A draft guide to organizing or participating in a PGP key signing party OpenPGP key paper slip generator PIUS: The PGP Individual UID Signer","title":"Web of Trust"},{"location":"pgp/#philosophy","text":"Op-ed: I'm throwing in the towel on PGP, and I work in security Op-ed: Why I'm not giving up on PGP","title":"Philosophy"},{"location":"philips-hue/","text":"\"Philips Hue is your personal wireless lighting system that lets you easily control your light and create the right ambiance for every moment.\" - https://www2.meethue.com Siri integration Siri knows the names of all of the X11 colors","title":"philips-hue"},{"location":"philips-hue/#siri-integration","text":"Siri knows the names of all of the X11 colors","title":"Siri integration"},{"location":"php/","text":"The PHP scripting language. Code Guidelines The PEAR code guidelines are pretty good - http://pear.activeventure.com/standards.html Even better coding standards - https://github.com/php-fig/fig-standards/blob/master/accepted/PSR-1-basic-coding-standard.md Examples Convert date formats This converts mysql time to epoch unix timestamp and back $timestamp = strtotime($mysqltime); echo date(\"Y-m-d H:i:s\", $timestamp); Run code from CLI php -r \"phpinfo();\" Show php CLI env vars This shows the location of the ini file used for CLI. php -i Enable Errors Set display_errors = On in php.ini, or in a php file add: error_reporting(E_ALL); ini_set('display_errors', 1); Disable timeout set_time_limit(0); ini_set ('max_execution_time', 0); Random numbers rand() ; # random int rand(1,10) ; # random int between 1 and 10 rand(100,1000)/100 ; # workaround for generating floats with 2 decimal points","title":"php"},{"location":"php/#code-guidelines","text":"The PEAR code guidelines are pretty good - http://pear.activeventure.com/standards.html Even better coding standards - https://github.com/php-fig/fig-standards/blob/master/accepted/PSR-1-basic-coding-standard.md","title":"Code Guidelines"},{"location":"php/#examples","text":"","title":"Examples"},{"location":"php/#convert-date-formats","text":"This converts mysql time to epoch unix timestamp and back $timestamp = strtotime($mysqltime); echo date(\"Y-m-d H:i:s\", $timestamp);","title":"Convert date formats"},{"location":"php/#run-code-from-cli","text":"php -r \"phpinfo();\"","title":"Run code from CLI"},{"location":"php/#show-php-cli-env-vars","text":"This shows the location of the ini file used for CLI. php -i","title":"Show php CLI env vars"},{"location":"php/#enable-errors","text":"Set display_errors = On in php.ini, or in a php file add: error_reporting(E_ALL); ini_set('display_errors', 1);","title":"Enable Errors"},{"location":"php/#disable-timeout","text":"set_time_limit(0); ini_set ('max_execution_time', 0);","title":"Disable timeout"},{"location":"php/#random-numbers","text":"rand() ; # random int rand(1,10) ; # random int between 1 and 10 rand(100,1000)/100 ; # workaround for generating floats with 2 decimal points","title":"Random numbers"},{"location":"plex/","text":"Plex is a media center system that runs on a variety of platforms including Linux, Roku, macOS, iOS, tvOS, and a variety of smart TVs. Links Developer Channel: A Beginner's Guide to v2.1 Developer Channel: Using Chrome's Built-In Debugger for Channel Development https://support.plex.tv/articles/categories/media-preparation/","title":"plex"},{"location":"plex/#links","text":"Developer Channel: A Beginner's Guide to v2.1 Developer Channel: Using Chrome's Built-In Debugger for Channel Development https://support.plex.tv/articles/categories/media-preparation/","title":"Links"},{"location":"postgres/","text":"\"PostgreSQL is a powerful, open source object-relational database system with over 30 years of active development that has earned it a strong reputation for reliability, feature robustness, and performance.\" - https://www.postgresql.org Usage Meta Commands postgres shell has a lot of meta commands. See https://www.postgresql.org/docs/11/app-psql.html#APP-PSQL-META-COMMANDS for full descriptions. meta-command behavior \\d or \\d+ Show extended table description \\d $table_name Show full definition for the given table \\df List functions \\di List indexes \\c Create new connection. There are various syntaxes to accomplish this. Here is one: \\c \"host=localhost port=5432 dbname=mydb connect_timeout=10 sslmode=disable\" \\x Toggle expanded display. This is the same as \\G in MySQL, separating each record and showing each column as a row formatted as column | row value . See Also pgcli - \"Pgcli is a command line interface for Postgres with auto-completion and syntax highlighting.\" https://github.com/dbcli/pgcli http://www.pgadmin.org - Graphical UI for postgres MySQL - Another relational database SQLite - File based local database that does not require a server.","title":"PostgreSQL"},{"location":"postgres/#usage","text":"","title":"Usage"},{"location":"postgres/#meta-commands","text":"postgres shell has a lot of meta commands. See https://www.postgresql.org/docs/11/app-psql.html#APP-PSQL-META-COMMANDS for full descriptions. meta-command behavior \\d or \\d+ Show extended table description \\d $table_name Show full definition for the given table \\df List functions \\di List indexes \\c Create new connection. There are various syntaxes to accomplish this. Here is one: \\c \"host=localhost port=5432 dbname=mydb connect_timeout=10 sslmode=disable\" \\x Toggle expanded display. This is the same as \\G in MySQL, separating each record and showing each column as a row formatted as column | row value .","title":"Meta Commands"},{"location":"postgres/#see-also","text":"pgcli - \"Pgcli is a command line interface for Postgres with auto-completion and syntax highlighting.\" https://github.com/dbcli/pgcli http://www.pgadmin.org - Graphical UI for postgres MySQL - Another relational database SQLite - File based local database that does not require a server.","title":"See Also"},{"location":"powershell/","text":"PowerShell is a shell for Windows operating systems, and it was ported to Linux in 2016. https://github.com/PowerShell/PowerShell/ Profile.ps1 On startup, powershell will run any .ps1 files it finds in the WindowsPowerShell directory under my documents. There is allegedly a Profile.ps1 file in there by default. $env:Path = \"c:\\Users\\dhoherd\\Dropbox\\GWOS\\Scripts;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\\" Tricks Restart a remote computer Restart-Computer remotehostname -Force Find a command that matches a substring get-command *time* Get help on commands that match a substring get-help *time* Show ACLs of the current dir get-acl | format-list Show system BIOS information Get-WmiObject -ComputerName hostname win32_bios Show object methods $foo | Get-Member Browse the registry Set-Location HKCU:\\Software\\Microsoft\\Windows\\ Get-ChildItem Show top processes while (1) { ps | sort -desc cpu | select -first 30; sleep -seconds 1; cls } Browse the Cert store Set-Location cert:\\CurrentUser\\ Get-ChildItem Get a list of stopped services Get-Service | Where-Object { $_.Status -eq \"Stopped\" } Compare two objects This will only show the lines that are not common: Compare-Object $(Get-VIPrivilege -role admin) $(Get-VIPrivilege -role member) Save object to a csv Get-Process | Export-Csv -Encoding unicode processes.csv Load object from a csv and parse it Import-Csv ./processes.csv | Where-Object { $_.Name -like \"*systemd*\" } | Select-Object -last 10 | Format-Table Replacement for unix tail tail filename Get-Content [filename] | Select-Object -Last 10 tail -f Get-Content -Path \"C:\\scripts\\test.txt\" -Wait Replacement for unix wc Get-Content test.csv | Measure-Object -line -word -character Replacement for unix time Measure-Command { Sleep 5 } Replacement for unix grep -B2 -A1 Get-Content test.csv | Select-String \"searchstring\" -Context 2,1 -CaseSensitive See Also http://poshcode.org/ - Great PSH site PowerCLI - VMware CLI built on PSH http://support.microsoft.com/kb/968929 - download link for PowerShell and extras AD integration through Active Directory Web Services (ADWS) - http://www.microsoft.com/en-us/download/details.aspx?id=2852 Some PSH videos - http://www.blkmtn.org/TechEd-2012-Videos","title":"powershell"},{"location":"powershell/#profileps1","text":"On startup, powershell will run any .ps1 files it finds in the WindowsPowerShell directory under my documents. There is allegedly a Profile.ps1 file in there by default. $env:Path = \"c:\\Users\\dhoherd\\Dropbox\\GWOS\\Scripts;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\\"","title":"Profile.ps1"},{"location":"powershell/#tricks","text":"","title":"Tricks"},{"location":"powershell/#restart-a-remote-computer","text":"Restart-Computer remotehostname -Force","title":"Restart a remote computer"},{"location":"powershell/#find-a-command-that-matches-a-substring","text":"get-command *time*","title":"Find a command that matches a substring"},{"location":"powershell/#get-help-on-commands-that-match-a-substring","text":"get-help *time*","title":"Get help on commands that match a substring"},{"location":"powershell/#show-acls-of-the-current-dir","text":"get-acl | format-list","title":"Show ACLs of the current dir"},{"location":"powershell/#show-system-bios-information","text":"Get-WmiObject -ComputerName hostname win32_bios","title":"Show system BIOS information"},{"location":"powershell/#show-object-methods","text":"$foo | Get-Member","title":"Show object methods"},{"location":"powershell/#browse-the-registry","text":"Set-Location HKCU:\\Software\\Microsoft\\Windows\\ Get-ChildItem","title":"Browse the registry"},{"location":"powershell/#show-top-processes","text":"while (1) { ps | sort -desc cpu | select -first 30; sleep -seconds 1; cls }","title":"Show top processes"},{"location":"powershell/#browse-the-cert-store","text":"Set-Location cert:\\CurrentUser\\ Get-ChildItem","title":"Browse the Cert store"},{"location":"powershell/#get-a-list-of-stopped-services","text":"Get-Service | Where-Object { $_.Status -eq \"Stopped\" }","title":"Get a list of stopped services"},{"location":"powershell/#compare-two-objects","text":"This will only show the lines that are not common: Compare-Object $(Get-VIPrivilege -role admin) $(Get-VIPrivilege -role member)","title":"Compare two objects"},{"location":"powershell/#save-object-to-a-csv","text":"Get-Process | Export-Csv -Encoding unicode processes.csv","title":"Save object to a csv"},{"location":"powershell/#load-object-from-a-csv-and-parse-it","text":"Import-Csv ./processes.csv | Where-Object { $_.Name -like \"*systemd*\" } | Select-Object -last 10 | Format-Table","title":"Load object from a csv and parse it"},{"location":"powershell/#replacement-for-unix-tail","text":"tail filename Get-Content [filename] | Select-Object -Last 10 tail -f Get-Content -Path \"C:\\scripts\\test.txt\" -Wait","title":"Replacement for unix tail"},{"location":"powershell/#replacement-for-unix-wc","text":"Get-Content test.csv | Measure-Object -line -word -character","title":"Replacement for unix wc"},{"location":"powershell/#replacement-for-unix-time","text":"Measure-Command { Sleep 5 }","title":"Replacement for unix time"},{"location":"powershell/#replacement-for-unix-grep-b2-a1","text":"Get-Content test.csv | Select-String \"searchstring\" -Context 2,1 -CaseSensitive","title":"Replacement for unix grep -B2 -A1"},{"location":"powershell/#see-also","text":"http://poshcode.org/ - Great PSH site PowerCLI - VMware CLI built on PSH http://support.microsoft.com/kb/968929 - download link for PowerShell and extras AD integration through Active Directory Web Services (ADWS) - http://www.microsoft.com/en-us/download/details.aspx?id=2852 Some PSH videos - http://www.blkmtn.org/TechEd-2012-Videos","title":"See Also"},{"location":"powertop/","text":"\"PowerTOP is a Linux tool to diagnose issues with power consumption and power management.\" - https://01.org/powertop/ Examples Generate an html power report powertop --html=powertop.html See also Top variant list","title":"powertop"},{"location":"powertop/#examples","text":"","title":"Examples"},{"location":"powertop/#generate-an-html-power-report","text":"powertop --html=powertop.html","title":"Generate an html power report"},{"location":"powertop/#see-also","text":"Top variant list","title":"See also"},{"location":"procurve/","text":"Procurve switches from HP offer cheap layer 2 and layer 3 switching. Copy config files scp user@switch:cfg/startup-config ./ scp user@switch:cfg/running-config ./ Firmware update Via ssh: linuxclient$ scp /path/to/image user@switch-hostname:/os/primary Via tftp: switch# copy tftp flash 172.28.115.151 flashfilename.swi primary This doesn't always work, try scp if it fails. Then on the switch... system boot flash primary Config Examples Set an IP# for the default VLAN interface vlan 1 ip address 172.28.115.234 255.255.255.0 ip default-gateway 172.28.115.1 Set up additional VLANs vlan 100 untagged 2 vlan 100 ip address 172.28.100.1 vlan 102 untagged 3 vlan 102 ip address 172.28.102.1 Enable routing between connected networks ip routing Set up SNTP clock sntp server 172.28.111.16 timesync sntp sntp 120 sntp unicast Alter DST settings time daylight-time-rule User-defined begin-date 3/8 end-date 11/1 Enable SSH crypto key generate ssh ip ssh ip ssh version 2 ip ssh filetransfer Disable telnet no telnet-server Set up snmp snmp-server community \"foobar\" Operator Set up a VLAN 112 port group vlan 112 untagged 6-12 Set two groups of ports as a trunks (eg: to use with VMware in static LACP) trunk 1-4 trk1 trunk trunk 5-8 trk2 trunk Set up VLAN multiplexing vlan 114 tagged 24 vlan 115 tagged 24 vlan 114 tagged Trk1 vlan 115 tagged Trk1 See Also Plenty of config examples: https://www.cs.uwaterloo.ca Example Config hostname \"HP-CORE-0\" snmp-server location \"Cup1-Closet1\" max-vlans 64 time timezone -480 time daylight-time-rule User-defined begin-date 3/8 end-date 11/1 console inactivity-timer 5 no web-management web-management ssl no telnet-server interface 2 name \"Load Test Cluster\" exit interface 5 name \"hq-vm-1\" exit interface 6 name \"hq-vm-1\" exit interface 8 name \"beast\" exit interface 10 name \"Winserv\" exit interface 12 name \"IT\" exit interface 13 name \"Services\" exit interface 14 name \"IT\" exit interface 15 name \"IT\" exit interface 16 name \"IT\" exit interface 17 name \"beast\" exit interface 18 name \"VPN\" exit interface 19 name \"IT\" exit interface 20 name \"IT\" exit interface 21 name \"Radio Station\" exit interface 22 name \"AT&T Network\" exit interface 23 name \"HP-CORE trunk\" exit interface 24 name \"Jun1-trunk\" exit ip default-gateway 10.8.100.1 sntp server 10.8.5.220 ip routing timesync sntp sntp unicast snmp-server community \"public\" Unrestricted snmp-server host 10.8.5.189 \"public\" vlan 1 name \"DEFAULT_VLAN\" untagged 4,14 no ip address tagged 23 no untagged 1-3,5-13,15-22,24 exit vlan 101 name \"Services\" untagged 3,8,10,15,19 ip address 10.8.1.1 255.255.255.0 ip helper-address 10.8.5.220 tagged 2,5-6,23-24 exit vlan 102 name \"LoadTest\" no ip address ip helper-address 10.8.5.220 tagged 2,5-6,15,23-24 exit vlan 103 name \"QATest\" no ip address ip helper-address 10.8.5.220 tagged 15,23 exit vlan 104 name \"PS\" no ip address ip helper-address 10.8.5.220 tagged 15,23-24 exit vlan 105 name \"IT\" untagged 1,5-6,9,12-13,16,20 ip address 10.8.5.1 255.255.255.0 ip helper-address 10.8.5.220 tagged 2,15,23-24 exit vlan 110 name \"Wireless\" no ip address ip helper-address 10.8.5.220 tagged 15,23-24 exit vlan 111 name \"Eng\" no ip address ip helper-address 10.8.5.220 tagged 15,23-24 exit vlan 113 name \"SW2\" no ip address ip helper-address 10.8.5.220 tagged 23 exit vlan 112 name \"SW1\" untagged 21 ip address 10.8.12.1 255.255.255.0 ip helper-address 10.8.5.220 tagged 23 exit vlan 100 name \"Backbone\" ip address 10.8.100.100 255.255.255.0 tagged 23-24 exit vlan 114 name \"Upstairs\" no ip address ip helper-address 10.8.5.220 tagged 23-24 exit vlan 106 name \"VPN\" untagged 18 no ip address ip helper-address 10.8.5.220 tagged 15,23-24 exit vlan 188 name \"OldNet\" untagged 11,17 no ip address ip helper-address 10.8.5.220 tagged 23-24 exit vlan 42 name \"ATT\" untagged 22 tagged 23-24 exit vlan 107 name \"DMZ\" untagged 7 ip helper-address 10.8.5.220 tagged 15,24 exit vlan 109 name \"Jail\" tagged 23-24 exit dhcp-relay option 82 keep ip route 0.0.0.0 0.0.0.0 10.8.100.1 ip route 10.8.11.0 255.255.255.0 10.8.100.101 ip route 10.8.3.0 255.255.255.0 10.8.100.101 ip route 10.172.188.0 255.255.255.0 10.8.100.1 ip route 10.8.13.0 255.255.255.0 10.8.100.101 ip route 10.8.2.0 255.255.255.0 10.8.100.1 ip route 10.8.10.0 255.255.255.0 10.8.100.1 ip route 10.8.7.0 255.255.255.0 10.8.100.1 ip route 10.8.4.0 255.255.255.0 10.8.100.1 ip route 10.8.14.0 255.255.255.0 10.8.100.102 ip route 10.8.9.0 255.255.255.0 10.8.100.1 stack commander \"HP-CORE\" stack auto-grab stack member 1 mac-address 0016b90b4ea0 stack member 2 mac-address 0016b968df40 spanning-tree ip ssh ip ssh filetransfer no tftp client no tftp server password manager password operator","title":"procurve"},{"location":"procurve/#copy-config-files","text":"scp user@switch:cfg/startup-config ./ scp user@switch:cfg/running-config ./","title":"Copy config files"},{"location":"procurve/#firmware-update","text":"Via ssh: linuxclient$ scp /path/to/image user@switch-hostname:/os/primary Via tftp: switch# copy tftp flash 172.28.115.151 flashfilename.swi primary This doesn't always work, try scp if it fails. Then on the switch... system boot flash primary","title":"Firmware update"},{"location":"procurve/#config-examples","text":"","title":"Config Examples"},{"location":"procurve/#set-an-ip-for-the-default-vlan","text":"interface vlan 1 ip address 172.28.115.234 255.255.255.0 ip default-gateway 172.28.115.1","title":"Set an IP# for the default VLAN"},{"location":"procurve/#set-up-additional-vlans","text":"vlan 100 untagged 2 vlan 100 ip address 172.28.100.1 vlan 102 untagged 3 vlan 102 ip address 172.28.102.1","title":"Set up additional VLANs"},{"location":"procurve/#enable-routing-between-connected-networks","text":"ip routing","title":"Enable routing between connected networks"},{"location":"procurve/#set-up-sntp-clock","text":"sntp server 172.28.111.16 timesync sntp sntp 120 sntp unicast","title":"Set up SNTP clock"},{"location":"procurve/#alter-dst-settings","text":"time daylight-time-rule User-defined begin-date 3/8 end-date 11/1","title":"Alter DST settings"},{"location":"procurve/#enable-ssh","text":"crypto key generate ssh ip ssh ip ssh version 2 ip ssh filetransfer","title":"Enable SSH"},{"location":"procurve/#disable-telnet","text":"no telnet-server","title":"Disable telnet"},{"location":"procurve/#set-up-snmp","text":"snmp-server community \"foobar\" Operator","title":"Set up snmp"},{"location":"procurve/#set-up-a-vlan-112-port-group","text":"vlan 112 untagged 6-12","title":"Set up a VLAN 112 port group"},{"location":"procurve/#set-two-groups-of-ports-as-a-trunks-eg-to-use-with-vmware-in-static-lacp","text":"trunk 1-4 trk1 trunk trunk 5-8 trk2 trunk","title":"Set two groups of ports as a trunks (eg: to use with VMware in static LACP)"},{"location":"procurve/#set-up-vlan-multiplexing","text":"vlan 114 tagged 24 vlan 115 tagged 24 vlan 114 tagged Trk1 vlan 115 tagged Trk1","title":"Set up VLAN multiplexing"},{"location":"procurve/#see-also","text":"Plenty of config examples: https://www.cs.uwaterloo.ca","title":"See Also"},{"location":"procurve/#example-config","text":"hostname \"HP-CORE-0\" snmp-server location \"Cup1-Closet1\" max-vlans 64 time timezone -480 time daylight-time-rule User-defined begin-date 3/8 end-date 11/1 console inactivity-timer 5 no web-management web-management ssl no telnet-server interface 2 name \"Load Test Cluster\" exit interface 5 name \"hq-vm-1\" exit interface 6 name \"hq-vm-1\" exit interface 8 name \"beast\" exit interface 10 name \"Winserv\" exit interface 12 name \"IT\" exit interface 13 name \"Services\" exit interface 14 name \"IT\" exit interface 15 name \"IT\" exit interface 16 name \"IT\" exit interface 17 name \"beast\" exit interface 18 name \"VPN\" exit interface 19 name \"IT\" exit interface 20 name \"IT\" exit interface 21 name \"Radio Station\" exit interface 22 name \"AT&T Network\" exit interface 23 name \"HP-CORE trunk\" exit interface 24 name \"Jun1-trunk\" exit ip default-gateway 10.8.100.1 sntp server 10.8.5.220 ip routing timesync sntp sntp unicast snmp-server community \"public\" Unrestricted snmp-server host 10.8.5.189 \"public\" vlan 1 name \"DEFAULT_VLAN\" untagged 4,14 no ip address tagged 23 no untagged 1-3,5-13,15-22,24 exit vlan 101 name \"Services\" untagged 3,8,10,15,19 ip address 10.8.1.1 255.255.255.0 ip helper-address 10.8.5.220 tagged 2,5-6,23-24 exit vlan 102 name \"LoadTest\" no ip address ip helper-address 10.8.5.220 tagged 2,5-6,15,23-24 exit vlan 103 name \"QATest\" no ip address ip helper-address 10.8.5.220 tagged 15,23 exit vlan 104 name \"PS\" no ip address ip helper-address 10.8.5.220 tagged 15,23-24 exit vlan 105 name \"IT\" untagged 1,5-6,9,12-13,16,20 ip address 10.8.5.1 255.255.255.0 ip helper-address 10.8.5.220 tagged 2,15,23-24 exit vlan 110 name \"Wireless\" no ip address ip helper-address 10.8.5.220 tagged 15,23-24 exit vlan 111 name \"Eng\" no ip address ip helper-address 10.8.5.220 tagged 15,23-24 exit vlan 113 name \"SW2\" no ip address ip helper-address 10.8.5.220 tagged 23 exit vlan 112 name \"SW1\" untagged 21 ip address 10.8.12.1 255.255.255.0 ip helper-address 10.8.5.220 tagged 23 exit vlan 100 name \"Backbone\" ip address 10.8.100.100 255.255.255.0 tagged 23-24 exit vlan 114 name \"Upstairs\" no ip address ip helper-address 10.8.5.220 tagged 23-24 exit vlan 106 name \"VPN\" untagged 18 no ip address ip helper-address 10.8.5.220 tagged 15,23-24 exit vlan 188 name \"OldNet\" untagged 11,17 no ip address ip helper-address 10.8.5.220 tagged 23-24 exit vlan 42 name \"ATT\" untagged 22 tagged 23-24 exit vlan 107 name \"DMZ\" untagged 7 ip helper-address 10.8.5.220 tagged 15,24 exit vlan 109 name \"Jail\" tagged 23-24 exit dhcp-relay option 82 keep ip route 0.0.0.0 0.0.0.0 10.8.100.1 ip route 10.8.11.0 255.255.255.0 10.8.100.101 ip route 10.8.3.0 255.255.255.0 10.8.100.101 ip route 10.172.188.0 255.255.255.0 10.8.100.1 ip route 10.8.13.0 255.255.255.0 10.8.100.101 ip route 10.8.2.0 255.255.255.0 10.8.100.1 ip route 10.8.10.0 255.255.255.0 10.8.100.1 ip route 10.8.7.0 255.255.255.0 10.8.100.1 ip route 10.8.4.0 255.255.255.0 10.8.100.1 ip route 10.8.14.0 255.255.255.0 10.8.100.102 ip route 10.8.9.0 255.255.255.0 10.8.100.1 stack commander \"HP-CORE\" stack auto-grab stack member 1 mac-address 0016b90b4ea0 stack member 2 mac-address 0016b968df40 spanning-tree ip ssh ip ssh filetransfer no tftp client no tftp server password manager password operator","title":"Example Config"},{"location":"prometheus/","text":"\"Prometheus, a Cloud Native Computing Foundation project, is a systems and service monitoring system. It collects metrics from configured targets at given intervals, evaluates rule expressions, displays the results, and can trigger alerts if some condition is observed to be true.\" - https://github.com/prometheus/prometheus","title":"Prometheus"},{"location":"protobuf/","text":"\"Protocol buffers are a language-neutral, platform-neutral extensible mechanism for serializing structured data\" - https://developers.google.com/protocol-buffers/ \"Google's data interchange format\" - https://github.com/google/protobuf","title":"protobuf"},{"location":"ps/","text":"ps shows a list of processes in a *nix system. Examples show the exact command used to start all process ps axwwo command show a process tree view ps auxf show only all running processes This excludes sleeping processes and threads. ps auxr Show process list sorted by process start time ps hax -o lstart,pid,args | while read -r a b c d e f g ; do echo \"$(date -d \"$a $b $c $d $e\" \"+%F %T%z\") $f $g\" ; done | sort","title":"ps"},{"location":"ps/#examples","text":"","title":"Examples"},{"location":"ps/#show-the-exact-command-used-to-start-all-process","text":"ps axwwo command","title":"show the exact command used to start all process"},{"location":"ps/#show-a-process-tree-view","text":"ps auxf","title":"show a process tree view"},{"location":"ps/#show-only-all-running-processes","text":"This excludes sleeping processes and threads. ps auxr","title":"show only all running processes"},{"location":"ps/#show-process-list-sorted-by-process-start-time","text":"ps hax -o lstart,pid,args | while read -r a b c d e f g ; do echo \"$(date -d \"$a $b $c $d $e\" \"+%F %T%z\") $f $g\" ; done | sort","title":"Show process list sorted by process start time"},{"location":"ps_mem/","text":"\"A utility to accurately report the in core memory usage for a program.\" - https://github.com/pixelb/ps_mem Usage examples Simple usage $ sudo ps_mem Private + Shared = RAM used Program 144.0 KiB + 12.5 KiB = 156.5 KiB acpid 144.0 KiB + 31.5 KiB = 175.5 KiB hald-addon-acpi 160.0 KiB + 56.5 KiB = 216.5 KiB hald-addon-input ...snip... 17.9 MiB + 101.0 KiB = 18.0 MiB mysqld [updated] 25.5 MiB + 516.5 KiB = 26.0 MiB salt-minion 31.6 MiB + 730.0 KiB = 32.3 MiB python (2) 41.0 MiB + 309.5 KiB = 41.3 MiB ruby 45.5 MiB + 36.0 KiB = 45.6 MiB init 48.9 MiB + 4.1 MiB = 53.0 MiB ssh (48) 57.3 MiB + 2.5 MiB = 59.7 MiB bash (114) 115.0 MiB + 86.0 KiB = 115.1 MiB named 148.3 MiB + 132.5 KiB = 148.4 MiB java 1.4 GiB + 449.5 KiB = 1.4 GiB screen (15) --------------------------------- 2.0 GiB =================================","title":"ps_mem"},{"location":"ps_mem/#usage-examples","text":"","title":"Usage examples"},{"location":"ps_mem/#simple-usage","text":"$ sudo ps_mem Private + Shared = RAM used Program 144.0 KiB + 12.5 KiB = 156.5 KiB acpid 144.0 KiB + 31.5 KiB = 175.5 KiB hald-addon-acpi 160.0 KiB + 56.5 KiB = 216.5 KiB hald-addon-input ...snip... 17.9 MiB + 101.0 KiB = 18.0 MiB mysqld [updated] 25.5 MiB + 516.5 KiB = 26.0 MiB salt-minion 31.6 MiB + 730.0 KiB = 32.3 MiB python (2) 41.0 MiB + 309.5 KiB = 41.3 MiB ruby 45.5 MiB + 36.0 KiB = 45.6 MiB init 48.9 MiB + 4.1 MiB = 53.0 MiB ssh (48) 57.3 MiB + 2.5 MiB = 59.7 MiB bash (114) 115.0 MiB + 86.0 KiB = 115.1 MiB named 148.3 MiB + 132.5 KiB = 148.4 MiB java 1.4 GiB + 449.5 KiB = 1.4 GiB screen (15) --------------------------------- 2.0 GiB =================================","title":"Simple usage"},{"location":"psp/","text":"Playstation Portable Links Custom firmware and homebrew","title":"psp"},{"location":"psp/#links","text":"Custom firmware and homebrew","title":"Links"},{"location":"pssh/","text":"Parallel SSH tools for running commands on multiple system simultaneously. http://www.theether.org/pssh/ Examples Run a command on hosts contained in a file, showing stdin and stdout pssh -h hostnames.txt -i some_command some_arg Run commands and view results on many hosts o=$(date +%F-%T) pssh -o \"$o\" -h hosts.txt uname -a grep -r . $o Run two commands on many hosts using bash expansion for host list o=$(date +pssh-%T) pssh -p 50 -t 60 {-H\\ sea-z-app00{1..9},} -o $o 'whoami ; hostname ;' grep -r . $o Install a package on many hosts fping < hosts.txt | awk '$3 == \"alive\" {print $1}' > alive.txt pssh \\ -h alive.txt \\ -o out_dir \\ -l root \\ yum -y localinstall ~danielh/rpms/cfengine-community-3.6.2-1.x86_64.rpm or directly from a db query and fping... pssh \\ -h <( invdb -d sjc-z-01opsdbw 'select hostname from servers where colo = \"sjc\";' | sort -u | egrep '[0-9]+6[^0-9]' | fping 2> /dev/null | awk '$3 == \"alive\" {print $1}' ) \\ -o out_dir \\ -l root \\ yum -y localinstall ~danielh/rpms/cfengine-community-3.6.2-1.x86_64.rpm or from mco... o=$(date +pssh-%T) ; pssh -O GlobalKnownHostsFile=/dev/null -O UserKnownHostsFile=/dev/null -O StrictHostKeyChecking=no -t300 -p10 -h <(mco find -C role::devbox) -o \"$o\" 'sudo apt-get install -y silversearcher-ag' ; grep -r . \"$o\" ;","title":"pssh"},{"location":"pssh/#examples","text":"","title":"Examples"},{"location":"pssh/#run-a-command-on-hosts-contained-in-a-file-showing-stdin-and-stdout","text":"pssh -h hostnames.txt -i some_command some_arg","title":"Run a command on hosts contained in a file, showing stdin and stdout"},{"location":"pssh/#run-commands-and-view-results-on-many-hosts","text":"o=$(date +%F-%T) pssh -o \"$o\" -h hosts.txt uname -a grep -r . $o","title":"Run commands and view results on many hosts"},{"location":"pssh/#run-two-commands-on-many-hosts-using-bash-expansion-for-host-list","text":"o=$(date +pssh-%T) pssh -p 50 -t 60 {-H\\ sea-z-app00{1..9},} -o $o 'whoami ; hostname ;' grep -r . $o","title":"Run two commands on many hosts using bash expansion for host list"},{"location":"pssh/#install-a-package-on-many-hosts","text":"fping < hosts.txt | awk '$3 == \"alive\" {print $1}' > alive.txt pssh \\ -h alive.txt \\ -o out_dir \\ -l root \\ yum -y localinstall ~danielh/rpms/cfengine-community-3.6.2-1.x86_64.rpm or directly from a db query and fping... pssh \\ -h <( invdb -d sjc-z-01opsdbw 'select hostname from servers where colo = \"sjc\";' | sort -u | egrep '[0-9]+6[^0-9]' | fping 2> /dev/null | awk '$3 == \"alive\" {print $1}' ) \\ -o out_dir \\ -l root \\ yum -y localinstall ~danielh/rpms/cfengine-community-3.6.2-1.x86_64.rpm or from mco... o=$(date +pssh-%T) ; pssh -O GlobalKnownHostsFile=/dev/null -O UserKnownHostsFile=/dev/null -O StrictHostKeyChecking=no -t300 -p10 -h <(mco find -C role::devbox) -o \"$o\" 'sudo apt-get install -y silversearcher-ag' ; grep -r . \"$o\" ;","title":"Install a package on many hosts"},{"location":"ptp/","text":"\"The Precision Time Protocol (PTP) is a protocol used to synchronize clocks throughout a computer network. On a local area network, it achieves clock accuracy in the sub-microsecond range, making it suitable for measurement and control systems.\" - https://en.wikipedia.org/wiki/Precision_Time_Protocol Links RFC 8173: Precision Time Protocol Version 2 (PTPv2) Precision System Synchronization with the IEEE-1588 Precision Time Protocol (PTP) GigE Vision NTP vs PTP: Network Timing Smackdown! See Also time - Notes on time technologies ptp - Precision Time Protocol","title":"ptp"},{"location":"ptp/#links","text":"RFC 8173: Precision Time Protocol Version 2 (PTPv2) Precision System Synchronization with the IEEE-1588 Precision Time Protocol (PTP) GigE Vision NTP vs PTP: Network Timing Smackdown!","title":"Links"},{"location":"ptp/#see-also","text":"time - Notes on time technologies ptp - Precision Time Protocol","title":"See Also"},{"location":"puppet/","text":"\"Puppet is an open-source configuration management tool. It runs on many Unix-like systems as well as on Microsoft Windows, and includes its own declarative language to describe system configuration.\" - https://en.wikipedia.org/wiki/Puppet_(software) Videos and links Overview of Puppet's architecture Puppet Documentation Index Introduction to Puppet Function Reference stdlib is another good function reference. Language: Basics Include-like vs. resource-like class instantiation Style Guide Vagrant Docs - Puppet Apply Provisioner Downloads PuppetConf 2015 Designing Puppet: Roles/Profiles Pattern - based on the blog post Designing Puppet - Roles and Profiles Building a Functional Puppet Workflow Part 2: Roles and Profiles Configuration Management as Legos Examples Standalone mode puppet apply /path/to/manifests works, or you can specify a .pp file Show variables about the host that puppet knows (facts) facter Show how puppet interacts with a resource puppet describe cron Show available puppet types puppet resource --types Show the puppet code that will create a resource $ puppet resource file /etc/hosts file { '/etc/hosts': ensure => 'file', content => '{md5}9ffbd726fd5b15de760cc0150d607628', ctime => 'Wed Apr 01 17:05:59 -0700 2015', group => '0', mode => '644', mtime => 'Wed Apr 01 17:05:59 -0700 2015', owner => '0', type => 'file', } Tests http://rspec-puppet.com/matchers/ Marionette Collective \"The Marionette Collective, also known as MCollective, is a framework for building server orchestration or parallel job-execution systems. Most users programmatically execute administrative tasks on clusters of servers.\" - http://docs.puppetlabs.com/mcollective/ Overview of MCollective Components and Configuration Invoking MCollective actions Cheatsheet: https://coderwall.com/p/ig9mxa/mcollective-mco-cheat-sheet Vagrant demo: https://github.com/ripienaar/mcollective-vagrant mco Show some puppet cluster stats mco puppet summary mco puppet count mco puppet status Find a random node in the cluster mco find -1 Ping all nodes in the puppet cluster mco ping Show if a file exists on each host in the cluster mco filemgr -f /srv/nginx status Use fstat and md5 to detect files needing repair mco find -S \"fstat('/srv/somedir/somefile').md5=/af6db18c6dfa81c294895003e13a2eef/\" > files_needing_attention.txt pssh -h files_needing_attention.txt) 'do_something_to_the_file' Use fstat to find hosts where a directory has not been modified recently mco find -S \"fstat('/srv').mtime_seconds<$(date +%s -d '-8 hours')\" Show stats about which OSes you have mco facts lsbdistdescription Show all ip addreses on all hosts where a configured IP address matches a regex mco facts all_ipaddresses -F 'all_ipaddresses=~10\\.(56|29)\\.' Show a report about uptimes over a year mco facts uptime -F 'uptime_days>365' | awk '$2 == \"days\" {print}' | sort -n -k1 | column -t Find machines where a fact is true mco find is_ec2 Which is the same as mco find -W is_ec2=true Find machines that have a certain fact value mco find --with-fact lsbdistcodename=lucid Show a fact on machines that have a specific fact value mco facts role --with-fact lsbdistcodename=lucid -v Find ec2 hosts with low uptime mco find -W 'is_ec2=true uptime_seconds<7200' Show detailed info about a node mco inventory fqdn.example.com Find nodes that match a config management class mco find -C role::awsadmin Show the classes for a given host sort /var/lib/puppet/state/classes.txt Kick off a puppet run on all hosts of a certain class The following two syntaxes are essentially the same, using the same puppet agent of mco . The only differences are the use of runall vs runonce , and the method that performs parallel execution. I'm not sure what difference there is in the code path. mco rpc -C \"class_boolean\" -F \"fact_name=fact_value\" --batch 10 --agent puppet --action runonce mco puppet -C \"class_boolean\" -F \"fact_name=fact_value\" runall 10 Show the status and puppet policy about a package on all hosts mco rpc package status package=openssh-client --discovery-timeout 60 --json Upgrade an installed package on 10 random web hosts This upgrades, but does not install if the package is not already present. mco package update 'nginx' -I '/web/' --limit=10 Show breakdown of hosts by OS version by role mco facts -v --wc role::mon lsbdistdescription Use mco to find packages of a certain version on a certain OS mco rpc package status package=apt -j -F lsbdistcodename=trusty > cache.json jq -c '.[] | select(.data.ensure == \"1.0.1ubuntu2\") | { version: .data.ensure, hostname: .sender }' cache.json Hiera \"Hiera is a key/value lookup tool for configuration data, built to make Puppet better and let you set node-specific data without repeating yourself.\" - http://docs.puppetlabs.com/hiera/latest/ https://github.com/puppetlabs/hiera http://www.craigdunn.org/2011/10/puppet-configuration-variables-and-hiera/ r10k The suggested workflow for puppet is to use r10k on a control repo to manage the modules on your puppetmaster and the environments it provides. The general idea is that each module is represented by a puppetforge module name or a git repo listed inside of the ambiguously named Puppetfile . When r10k puppetfile install -v is run, all modules listed in this file are installed according to their definitions, and all modules that are not in this file are purged. Also, r10k will set up environments based on the git branches of the control repo. This workflow is described in detail at Managing and deploying Puppet code . It assumes you are not using a puppet apply type setup, which makes this difficult to follow for people who are playing with this at home in a non-puppetmaster scenario, such as in vagrant or on raspberry pi's.","title":"Puppet"},{"location":"puppet/#videos-and-links","text":"Overview of Puppet's architecture Puppet Documentation Index Introduction to Puppet Function Reference stdlib is another good function reference. Language: Basics Include-like vs. resource-like class instantiation Style Guide Vagrant Docs - Puppet Apply Provisioner Downloads PuppetConf 2015 Designing Puppet: Roles/Profiles Pattern - based on the blog post Designing Puppet - Roles and Profiles Building a Functional Puppet Workflow Part 2: Roles and Profiles Configuration Management as Legos","title":"Videos and links"},{"location":"puppet/#examples","text":"","title":"Examples"},{"location":"puppet/#standalone-mode","text":"puppet apply /path/to/manifests works, or you can specify a .pp file","title":"Standalone mode"},{"location":"puppet/#show-variables-about-the-host-that-puppet-knows-facts","text":"facter","title":"Show variables about the host that puppet knows (facts)"},{"location":"puppet/#show-how-puppet-interacts-with-a-resource","text":"puppet describe cron","title":"Show how puppet interacts with a resource"},{"location":"puppet/#show-available-puppet-types","text":"puppet resource --types","title":"Show available puppet types"},{"location":"puppet/#show-the-puppet-code-that-will-create-a-resource","text":"$ puppet resource file /etc/hosts file { '/etc/hosts': ensure => 'file', content => '{md5}9ffbd726fd5b15de760cc0150d607628', ctime => 'Wed Apr 01 17:05:59 -0700 2015', group => '0', mode => '644', mtime => 'Wed Apr 01 17:05:59 -0700 2015', owner => '0', type => 'file', }","title":"Show the puppet code that will create a resource"},{"location":"puppet/#tests","text":"http://rspec-puppet.com/matchers/","title":"Tests"},{"location":"puppet/#marionette-collective","text":"\"The Marionette Collective, also known as MCollective, is a framework for building server orchestration or parallel job-execution systems. Most users programmatically execute administrative tasks on clusters of servers.\" - http://docs.puppetlabs.com/mcollective/ Overview of MCollective Components and Configuration Invoking MCollective actions Cheatsheet: https://coderwall.com/p/ig9mxa/mcollective-mco-cheat-sheet Vagrant demo: https://github.com/ripienaar/mcollective-vagrant","title":"Marionette Collective"},{"location":"puppet/#mco","text":"","title":"mco"},{"location":"puppet/#show-some-puppet-cluster-stats","text":"mco puppet summary mco puppet count mco puppet status","title":"Show some puppet cluster stats"},{"location":"puppet/#find-a-random-node-in-the-cluster","text":"mco find -1","title":"Find a random node in the cluster"},{"location":"puppet/#ping-all-nodes-in-the-puppet-cluster","text":"mco ping","title":"Ping all nodes in the puppet cluster"},{"location":"puppet/#show-if-a-file-exists-on-each-host-in-the-cluster","text":"mco filemgr -f /srv/nginx status","title":"Show if a file exists on each host in the cluster"},{"location":"puppet/#use-fstat-and-md5-to-detect-files-needing-repair","text":"mco find -S \"fstat('/srv/somedir/somefile').md5=/af6db18c6dfa81c294895003e13a2eef/\" > files_needing_attention.txt pssh -h files_needing_attention.txt) 'do_something_to_the_file'","title":"Use fstat and md5 to detect files needing repair"},{"location":"puppet/#use-fstat-to-find-hosts-where-a-directory-has-not-been-modified-recently","text":"mco find -S \"fstat('/srv').mtime_seconds<$(date +%s -d '-8 hours')\"","title":"Use fstat to find hosts where a directory has not been modified recently"},{"location":"puppet/#show-stats-about-which-oses-you-have","text":"mco facts lsbdistdescription","title":"Show stats about which OSes you have"},{"location":"puppet/#show-all-ip-addreses-on-all-hosts-where-a-configured-ip-address-matches-a-regex","text":"mco facts all_ipaddresses -F 'all_ipaddresses=~10\\.(56|29)\\.'","title":"Show all ip addreses on all hosts where a configured IP address matches a regex"},{"location":"puppet/#show-a-report-about-uptimes-over-a-year","text":"mco facts uptime -F 'uptime_days>365' | awk '$2 == \"days\" {print}' | sort -n -k1 | column -t","title":"Show a report about uptimes over a year"},{"location":"puppet/#find-machines-where-a-fact-is-true","text":"mco find is_ec2 Which is the same as mco find -W is_ec2=true","title":"Find machines where a fact is true"},{"location":"puppet/#find-machines-that-have-a-certain-fact-value","text":"mco find --with-fact lsbdistcodename=lucid","title":"Find machines that have a certain fact value"},{"location":"puppet/#show-a-fact-on-machines-that-have-a-specific-fact-value","text":"mco facts role --with-fact lsbdistcodename=lucid -v","title":"Show a fact on machines that have a specific fact value"},{"location":"puppet/#find-ec2-hosts-with-low-uptime","text":"mco find -W 'is_ec2=true uptime_seconds<7200'","title":"Find ec2 hosts with low uptime"},{"location":"puppet/#show-detailed-info-about-a-node","text":"mco inventory fqdn.example.com","title":"Show detailed info about a node"},{"location":"puppet/#find-nodes-that-match-a-config-management-class","text":"mco find -C role::awsadmin","title":"Find nodes that match a config management class"},{"location":"puppet/#show-the-classes-for-a-given-host","text":"sort /var/lib/puppet/state/classes.txt","title":"Show the classes for a given host"},{"location":"puppet/#kick-off-a-puppet-run-on-all-hosts-of-a-certain-class","text":"The following two syntaxes are essentially the same, using the same puppet agent of mco . The only differences are the use of runall vs runonce , and the method that performs parallel execution. I'm not sure what difference there is in the code path. mco rpc -C \"class_boolean\" -F \"fact_name=fact_value\" --batch 10 --agent puppet --action runonce mco puppet -C \"class_boolean\" -F \"fact_name=fact_value\" runall 10","title":"Kick off a puppet run on all hosts of a certain class"},{"location":"puppet/#show-the-status-and-puppet-policy-about-a-package-on-all-hosts","text":"mco rpc package status package=openssh-client --discovery-timeout 60 --json","title":"Show the status and puppet policy about a package on all hosts"},{"location":"puppet/#upgrade-an-installed-package-on-10-random-web-hosts","text":"This upgrades, but does not install if the package is not already present. mco package update 'nginx' -I '/web/' --limit=10","title":"Upgrade an installed package on 10 random web hosts"},{"location":"puppet/#show-breakdown-of-hosts-by-os-version-by-role","text":"mco facts -v --wc role::mon lsbdistdescription","title":"Show breakdown of hosts by OS version by role"},{"location":"puppet/#use-mco-to-find-packages-of-a-certain-version-on-a-certain-os","text":"mco rpc package status package=apt -j -F lsbdistcodename=trusty > cache.json jq -c '.[] | select(.data.ensure == \"1.0.1ubuntu2\") | { version: .data.ensure, hostname: .sender }' cache.json","title":"Use mco to find packages of a certain version on a certain OS"},{"location":"puppet/#hiera","text":"\"Hiera is a key/value lookup tool for configuration data, built to make Puppet better and let you set node-specific data without repeating yourself.\" - http://docs.puppetlabs.com/hiera/latest/ https://github.com/puppetlabs/hiera http://www.craigdunn.org/2011/10/puppet-configuration-variables-and-hiera/","title":"Hiera"},{"location":"puppet/#r10k","text":"The suggested workflow for puppet is to use r10k on a control repo to manage the modules on your puppetmaster and the environments it provides. The general idea is that each module is represented by a puppetforge module name or a git repo listed inside of the ambiguously named Puppetfile . When r10k puppetfile install -v is run, all modules listed in this file are installed according to their definitions, and all modules that are not in this file are purged. Also, r10k will set up environments based on the git branches of the control repo. This workflow is described in detail at Managing and deploying Puppet code . It assumes you are not using a puppet apply type setup, which makes this difficult to follow for people who are playing with this at home in a non-puppetmaster scenario, such as in vagrant or on raspberry pi's.","title":"r10k"},{"location":"pv/","text":"pv - monitor the progress of data through a pipe This can be used in place of dd or dcfldd in some cases, such as copying disks or files. It's also useful for including with nc so you can see stats about the flow of that pipe. Examples Show the average growth rate of logs xtail /nail/scribe/buffer/some_service_gnerated | pv -a > /dev/null Write a disk image to usb This can be used in place of dd if=file of=/dev/disk # As root pv ~hoherd/Downloads/ubuntu-16.04.1-desktop-amd64.iso > /dev/rdisk4 See Also dcfldd dd ddrescue","title":"pv"},{"location":"pv/#examples","text":"","title":"Examples"},{"location":"pv/#show-the-average-growth-rate-of-logs","text":"xtail /nail/scribe/buffer/some_service_gnerated | pv -a > /dev/null","title":"Show the average growth rate of logs"},{"location":"pv/#write-a-disk-image-to-usb","text":"This can be used in place of dd if=file of=/dev/disk # As root pv ~hoherd/Downloads/ubuntu-16.04.1-desktop-amd64.iso > /dev/rdisk4","title":"Write a disk image to usb"},{"location":"pv/#see-also","text":"dcfldd dd ddrescue","title":"See Also"},{"location":"python/","text":"\"Python is a programming language that lets you work more quickly and integrate your systems more effectively.\" - https://www.python.org/ Tips and techniques Don't use assert statements for regular validation. assert statements can be disabled at the interpreter level, which would vastly change the flow of your code if they were used widespread. Variable names _varname - Semi-private. Basically a convention that developers use to indicate that the scope of a variable is local, but this locality is not enforced by the interpreter. __varname - Private variable in name, but not in logic or security. The interpreter mangles the name of the var to make it globally unique, but it is still globally accessible. var_ - Used to get around shadowing built-in variable names. EG: list_ won't conflict with list() __magic_method__ - See https://www.diveinto.org/python3/special-method-names.html Virtual Environments Virtual environments isolate your project away from the system's python interpreter and modules, so you can have full control over what code is available to your project. This makes it easy to develop, debug, and deploy to a new system. It's basically always a good idea to use a virtual environment. You will thank yourself later by learning this one up front. Virtual environments using venv Creating a venv echo \"venv\" >> .gitignore virtualenv venv . venv/bin/activate pip install requests pip freeze > requirements.txt # write code, interact with it, whatever deactivate Recreating a venv virtualenv venv . venv/bin/activate pip install -r requirements.txt # write code, interact with it, whatever deactivate Use venv to work around missing pip This is mostly useful for installing for your user, since if you can't install pip you won't be able to install into system-wide locations. virtualenv venv --system-site-packages && venv/bin/pip install --user $PACKAGENAME && rm -rf venv Virtual environments with pipenv \"Pipenv \u2014 the officially recommended Python packaging tool from Python.org, free (as in freedom). Pipenv is a tool that aims to bring the best of all packaging worlds (bundler, composer, npm, cargo, yarn, etc.) to the Python world. Windows is a first-class citizen, in our world.\" - https://docs.pipenv.org pipenv is the new-school 2017 way of doing virtual environments. pipenv creates a file called Pipfile whenever you install packages using the pipenv command. Once you have created a pipenv for the CWD, any subdirs will use that pipenv and not create new environments when you issue pipenv commands. This is a more robust system, but as of 2017 is not widely used. When using pipenv, any packages using pip install are not included in the virtual environment. You must use pipenv install . Creating a virtual environment using pipenv cd project_dir pipenv --three # to create a python 3 virtual environment pipenv install bpython boto3 pipenv shell # this spawns a subshell with the new python environment # interact with your python environment exit Recreate a pipenv cd project_dir pipenv shell # This automatically enforces the environment described in Pipfile # interact with your python environment exit Import module from absolute path sys.path.append('/Users/username/code/somedir') import module # from somedir Debugging Verbose environment var https://docs.python.org/3/using/cmdline.html#envvar-PYTHONVERBOSE export PYTHONVERBOSE=1 # or... python -v pip search beets Follow the flow of a python script This is equivalent to bash -x / bash -o xtrace , but is probably even more useful because it prefixes the name of the file and the line number to what is actually being executed, which aids in debugging large projects. python -m trace --trace foo.py Enter an interactive prompt after script ends https://docs.python.org/3/using/cmdline.html#envvar-PYTHONINSPECT This works when your code causes an exception, but none of your code will actually be executed, you will simply be dropped into a shell, which is not very useful. export PYTHONINSPECT=1 # or... sudo python -i ./ps_mem.py Enter a python terminal arbitrarily https://docs.python.org/3/library/pdb.html import pdb; pdb.set_trace() In python 3.6+ you can simply type breakpoint() This drops you into a pdb shell. This is not the same as a full python REPL. To get a python REPL, type interact . After you have inspected the current state, you can type continue . Alternatively there is the web-pdb package which allows you to debug via a web browser using web_pdb.set_trace Print variables from the local scope for var in dir(): print \"Debug: {0} = {1}\".format(var,eval(var)) Inspect things >>> import inspect >>> inspect.getargspec(inspect.getargspec) ArgSpec(args=['func'], varargs=None, keywords=None, defaults=None) Create an http server using PWD as document root python3 -m http.server 9980 or... python2 -m SimpleHTTPServer 9980 Discover the location for pip --user installs echo $(python -m site --user-base)/bin Links Decorators https://wiki.python.org/moin/PythonDecoratorLibrary http://stackoverflow.com/questions/739654/how-can-i-make-a-chain-of-function-decorators-in-python/1594484#1594484 http://ains.co/blog/things-which-arent-magic-flask-part-1.html Modules https://github.com/jonathanslenders/ptpython - improved python REPL https://docs.python.org/3/library/sched.html - cross-platform cron-like scheduler https://pypi.python.org/pypi/colorama - cross-platform colorized terminal output https://pypi.python.org/pypi/begins/ - Simplified CLI arguments https://pypi.python.org/pypi/watchdog - cross-platform filesystem events API https://github.com/giampaolo/psutil/ - system information https://github.com/timothycrosley/hug - simplified web API creation http://python-future.org - \"python-future is the missing compatibility layer between Python 2 and Python 3. It allows you to use a single, clean Python 3.x-compatible codebase to support both Python 2 and Python 3 with minimal overhead.\" https://pymotw.com/3/ - Python Module of the Week has lots of useful module examples Various links A gallery of interesting Jupyter and IPython Notebooks Drag'n'drop Pivot Tables and Charts in Jupyter Code Like a Pythonista: Idiomatic Python Dive Into Python 3 Google's Python Class Google Python Style Guide How to Think Like a Computer Scientist - iPython notebook on python data science Intermediate and Advanced Software Carpentry in Python Learn Python dot org Python Cheatsheets The Flask Mega-Tutorial The Python IAQ: Infrequently Answered Questions Why I use py.test and you probably should too PyCon 2017 videos PyCon 2018 videos Cookiecutter: Better Project Templates / A pantry full of cookiecutters https://caremad.io/posts/2013/07/setup-vs-requirement/ - Hard vs abstract dependencies in requirements.txt (and by extension Pipfile) vs setup.py https://plot.ly/python/ https://realpython.com/factory-method-python/ http://pythontesting.net/start-here/ How to structure a Flask-RESTPlus web service for production builds","title":"python"},{"location":"python/#tips-and-techniques","text":"Don't use assert statements for regular validation. assert statements can be disabled at the interpreter level, which would vastly change the flow of your code if they were used widespread.","title":"Tips and techniques"},{"location":"python/#variable-names","text":"_varname - Semi-private. Basically a convention that developers use to indicate that the scope of a variable is local, but this locality is not enforced by the interpreter. __varname - Private variable in name, but not in logic or security. The interpreter mangles the name of the var to make it globally unique, but it is still globally accessible. var_ - Used to get around shadowing built-in variable names. EG: list_ won't conflict with list() __magic_method__ - See https://www.diveinto.org/python3/special-method-names.html","title":"Variable names"},{"location":"python/#virtual-environments","text":"Virtual environments isolate your project away from the system's python interpreter and modules, so you can have full control over what code is available to your project. This makes it easy to develop, debug, and deploy to a new system. It's basically always a good idea to use a virtual environment. You will thank yourself later by learning this one up front.","title":"Virtual Environments"},{"location":"python/#virtual-environments-using-venv","text":"","title":"Virtual environments using venv"},{"location":"python/#creating-a-venv","text":"echo \"venv\" >> .gitignore virtualenv venv . venv/bin/activate pip install requests pip freeze > requirements.txt # write code, interact with it, whatever deactivate","title":"Creating a venv"},{"location":"python/#recreating-a-venv","text":"virtualenv venv . venv/bin/activate pip install -r requirements.txt # write code, interact with it, whatever deactivate","title":"Recreating a venv"},{"location":"python/#use-venv-to-work-around-missing-pip","text":"This is mostly useful for installing for your user, since if you can't install pip you won't be able to install into system-wide locations. virtualenv venv --system-site-packages && venv/bin/pip install --user $PACKAGENAME && rm -rf venv","title":"Use venv to work around missing pip"},{"location":"python/#virtual-environments-with-pipenv","text":"\"Pipenv \u2014 the officially recommended Python packaging tool from Python.org, free (as in freedom). Pipenv is a tool that aims to bring the best of all packaging worlds (bundler, composer, npm, cargo, yarn, etc.) to the Python world. Windows is a first-class citizen, in our world.\" - https://docs.pipenv.org pipenv is the new-school 2017 way of doing virtual environments. pipenv creates a file called Pipfile whenever you install packages using the pipenv command. Once you have created a pipenv for the CWD, any subdirs will use that pipenv and not create new environments when you issue pipenv commands. This is a more robust system, but as of 2017 is not widely used. When using pipenv, any packages using pip install are not included in the virtual environment. You must use pipenv install .","title":"Virtual environments with pipenv"},{"location":"python/#creating-a-virtual-environment-using-pipenv","text":"cd project_dir pipenv --three # to create a python 3 virtual environment pipenv install bpython boto3 pipenv shell # this spawns a subshell with the new python environment # interact with your python environment exit","title":"Creating a virtual environment using pipenv"},{"location":"python/#recreate-a-pipenv","text":"cd project_dir pipenv shell # This automatically enforces the environment described in Pipfile # interact with your python environment exit","title":"Recreate a pipenv"},{"location":"python/#import-module-from-absolute-path","text":"sys.path.append('/Users/username/code/somedir') import module # from somedir","title":"Import module from absolute path"},{"location":"python/#debugging","text":"","title":"Debugging"},{"location":"python/#verbose-environment-var","text":"https://docs.python.org/3/using/cmdline.html#envvar-PYTHONVERBOSE export PYTHONVERBOSE=1 # or... python -v pip search beets","title":"Verbose environment var"},{"location":"python/#follow-the-flow-of-a-python-script","text":"This is equivalent to bash -x / bash -o xtrace , but is probably even more useful because it prefixes the name of the file and the line number to what is actually being executed, which aids in debugging large projects. python -m trace --trace foo.py","title":"Follow the flow of a python script"},{"location":"python/#enter-an-interactive-prompt-after-script-ends","text":"https://docs.python.org/3/using/cmdline.html#envvar-PYTHONINSPECT This works when your code causes an exception, but none of your code will actually be executed, you will simply be dropped into a shell, which is not very useful. export PYTHONINSPECT=1 # or... sudo python -i ./ps_mem.py","title":"Enter an interactive prompt after script ends"},{"location":"python/#enter-a-python-terminal-arbitrarily","text":"https://docs.python.org/3/library/pdb.html import pdb; pdb.set_trace() In python 3.6+ you can simply type breakpoint() This drops you into a pdb shell. This is not the same as a full python REPL. To get a python REPL, type interact . After you have inspected the current state, you can type continue . Alternatively there is the web-pdb package which allows you to debug via a web browser using web_pdb.set_trace","title":"Enter a python terminal arbitrarily"},{"location":"python/#print-variables-from-the-local-scope","text":"for var in dir(): print \"Debug: {0} = {1}\".format(var,eval(var))","title":"Print variables from the local scope"},{"location":"python/#inspect-things","text":">>> import inspect >>> inspect.getargspec(inspect.getargspec) ArgSpec(args=['func'], varargs=None, keywords=None, defaults=None)","title":"Inspect things"},{"location":"python/#create-an-http-server-using-pwd-as-document-root","text":"python3 -m http.server 9980 or... python2 -m SimpleHTTPServer 9980","title":"Create an http server using PWD as document root"},{"location":"python/#discover-the-location-for-pip-user-installs","text":"echo $(python -m site --user-base)/bin","title":"Discover the location for pip --user installs"},{"location":"python/#links","text":"","title":"Links"},{"location":"python/#decorators","text":"https://wiki.python.org/moin/PythonDecoratorLibrary http://stackoverflow.com/questions/739654/how-can-i-make-a-chain-of-function-decorators-in-python/1594484#1594484 http://ains.co/blog/things-which-arent-magic-flask-part-1.html","title":"Decorators"},{"location":"python/#modules","text":"https://github.com/jonathanslenders/ptpython - improved python REPL https://docs.python.org/3/library/sched.html - cross-platform cron-like scheduler https://pypi.python.org/pypi/colorama - cross-platform colorized terminal output https://pypi.python.org/pypi/begins/ - Simplified CLI arguments https://pypi.python.org/pypi/watchdog - cross-platform filesystem events API https://github.com/giampaolo/psutil/ - system information https://github.com/timothycrosley/hug - simplified web API creation http://python-future.org - \"python-future is the missing compatibility layer between Python 2 and Python 3. It allows you to use a single, clean Python 3.x-compatible codebase to support both Python 2 and Python 3 with minimal overhead.\" https://pymotw.com/3/ - Python Module of the Week has lots of useful module examples","title":"Modules"},{"location":"python/#various-links","text":"A gallery of interesting Jupyter and IPython Notebooks Drag'n'drop Pivot Tables and Charts in Jupyter Code Like a Pythonista: Idiomatic Python Dive Into Python 3 Google's Python Class Google Python Style Guide How to Think Like a Computer Scientist - iPython notebook on python data science Intermediate and Advanced Software Carpentry in Python Learn Python dot org Python Cheatsheets The Flask Mega-Tutorial The Python IAQ: Infrequently Answered Questions Why I use py.test and you probably should too PyCon 2017 videos PyCon 2018 videos Cookiecutter: Better Project Templates / A pantry full of cookiecutters https://caremad.io/posts/2013/07/setup-vs-requirement/ - Hard vs abstract dependencies in requirements.txt (and by extension Pipfile) vs setup.py https://plot.ly/python/ https://realpython.com/factory-method-python/ http://pythontesting.net/start-here/ How to structure a Flask-RESTPlus web service for production builds","title":"Various links"},{"location":"q/","text":"\"q - Text as Data\" - http://harelba.github.io/q/ Examples Format the Pagerduty incidents.csv to be more readable # -d, = comma delimited input # -H = use the headings found in the input csv files # -T = tab delimited output # -f = python 2 format strings to be applied to 1-indexed output fields $ q -d, -H -T -f '1=https://pagerduty.com/incidents/%s,2=alerted at %s,3=Description: %s' 'select id,created_on,description from incidents.csv order by created_on asc limit 5' https://pagerduty.com/incidents/P66XNLT alerted at 2017-12-04T00:04:07-08:00 Description: proxy0302: 200 Status Code Proxy Log Watcher: Matches found in last run met or dropped below 0.0, dropping to 0.0 for 10 minutes at 12:00AM https://server.pingdom.com/a/3103869181 https://pagerduty.com/incidents/PLUG344 alerted at 2017-12-04T04:14:05-08:00 Description: sandbox-apigateway00: API Gateway Error Watcher: Occurrences met or exceeded 10.00 /min, increasing to 15.82 /min for 10 minutes at 04:10AM https://server.pingdom.com/a/3104379391 https://pagerduty.com/incidents/PT13M2B alerted at 2017-12-04T06:48:14-08:00 Description: hadoop-r21: Hadoop Resource Monitor: Lostnodes met or exceeded 4.0, increasing to 4.0 at 06:47AM https://server.pingdom.com/a/3104686551 https://pagerduty.com/incidents/P3RLOTT alerted at 2017-12-04T08:56:07-08:00 Description: hadoop-c05: /srv Disk Usage: Disk Capacity met or exceeded 90%, increasing to 90% for 10 minutes at 08:50AM https://server.pingdom.com/a/3104929931 https://pagerduty.com/incidents/PNOJZKC alerted at 2017-12-04T09:02:21-08:00 Description: sjc-http2: HTTP 500 error Watcher: Occurrences met or exceeded 10.00 /min, increasing to 31.91 /min for 10 minutes at 09:00AM https://server.pingdom.com/a/3104941911 Format pagerduty events as HTML for pasting into confluence for issue response tracking pagerduty-csv-download opens your browser and downloads the csv file for the last week of events. You'll have to change companyname to whatever your company URL is. pagerduty-csv-to-html uses q to reformat the csv into HTML lists you can paste into the source editor of your HTML friendly CMS like Confluence. This uses BSD relative date syntax, you'll have to change it for linux. pagerduty-csv-download() { rm -f incidents.csv TZ=America/Los_Angeles past=\"$(date -v-7d \"+%FT00:00:00\")\" present=\"$(date \"+%FT00:00:00\")\" open \"$(date \"+https://companyname.pagerduty.com/api/v1/reports/raw/incidents.csv?since=${past}&until=${present}&time_zone=${TZ}\")\" } pagerduty-csv-to-html() { q \\ -H \\ -d',' \\ -D' ' \\ -f '1=<li>%s,2=<a href \\\"https://companyname.pagerduty.com/incidents/%s\\\">,3=%s</a>,4=%s<ul><li>...</li></ul></li>' \\ 'select substr(created_on,12,5),id,id,description from incidents.csv order by created_on asc' | tail -n 50 | sed 's/href /href=/;s/> />/' } Select count of daily alerts by date from PagerDuty incidents.csv q -H --delimiter=',' -O --output-delimiter=',' 'select substr(created_on,0,11) as date,count(substr(created_on,0,11)) as count from incidents.csv group by date'","title":"q"},{"location":"q/#examples","text":"","title":"Examples"},{"location":"q/#format-the-pagerduty-incidentscsv-to-be-more-readable","text":"# -d, = comma delimited input # -H = use the headings found in the input csv files # -T = tab delimited output # -f = python 2 format strings to be applied to 1-indexed output fields $ q -d, -H -T -f '1=https://pagerduty.com/incidents/%s,2=alerted at %s,3=Description: %s' 'select id,created_on,description from incidents.csv order by created_on asc limit 5' https://pagerduty.com/incidents/P66XNLT alerted at 2017-12-04T00:04:07-08:00 Description: proxy0302: 200 Status Code Proxy Log Watcher: Matches found in last run met or dropped below 0.0, dropping to 0.0 for 10 minutes at 12:00AM https://server.pingdom.com/a/3103869181 https://pagerduty.com/incidents/PLUG344 alerted at 2017-12-04T04:14:05-08:00 Description: sandbox-apigateway00: API Gateway Error Watcher: Occurrences met or exceeded 10.00 /min, increasing to 15.82 /min for 10 minutes at 04:10AM https://server.pingdom.com/a/3104379391 https://pagerduty.com/incidents/PT13M2B alerted at 2017-12-04T06:48:14-08:00 Description: hadoop-r21: Hadoop Resource Monitor: Lostnodes met or exceeded 4.0, increasing to 4.0 at 06:47AM https://server.pingdom.com/a/3104686551 https://pagerduty.com/incidents/P3RLOTT alerted at 2017-12-04T08:56:07-08:00 Description: hadoop-c05: /srv Disk Usage: Disk Capacity met or exceeded 90%, increasing to 90% for 10 minutes at 08:50AM https://server.pingdom.com/a/3104929931 https://pagerduty.com/incidents/PNOJZKC alerted at 2017-12-04T09:02:21-08:00 Description: sjc-http2: HTTP 500 error Watcher: Occurrences met or exceeded 10.00 /min, increasing to 31.91 /min for 10 minutes at 09:00AM https://server.pingdom.com/a/3104941911","title":"Format the Pagerduty incidents.csv to be more readable"},{"location":"q/#format-pagerduty-events-as-html-for-pasting-into-confluence-for-issue-response-tracking","text":"pagerduty-csv-download opens your browser and downloads the csv file for the last week of events. You'll have to change companyname to whatever your company URL is. pagerduty-csv-to-html uses q to reformat the csv into HTML lists you can paste into the source editor of your HTML friendly CMS like Confluence. This uses BSD relative date syntax, you'll have to change it for linux. pagerduty-csv-download() { rm -f incidents.csv TZ=America/Los_Angeles past=\"$(date -v-7d \"+%FT00:00:00\")\" present=\"$(date \"+%FT00:00:00\")\" open \"$(date \"+https://companyname.pagerduty.com/api/v1/reports/raw/incidents.csv?since=${past}&until=${present}&time_zone=${TZ}\")\" } pagerduty-csv-to-html() { q \\ -H \\ -d',' \\ -D' ' \\ -f '1=<li>%s,2=<a href \\\"https://companyname.pagerduty.com/incidents/%s\\\">,3=%s</a>,4=%s<ul><li>...</li></ul></li>' \\ 'select substr(created_on,12,5),id,id,description from incidents.csv order by created_on asc' | tail -n 50 | sed 's/href /href=/;s/> />/' }","title":"Format pagerduty events as HTML for pasting into confluence for issue response tracking"},{"location":"q/#select-count-of-daily-alerts-by-date-from-pagerduty-incidentscsv","text":"q -H --delimiter=',' -O --output-delimiter=',' 'select substr(created_on,0,11) as date,count(substr(created_on,0,11)) as count from incidents.csv group by date'","title":"Select count of daily alerts by date from PagerDuty incidents.csv"},{"location":"raspberry-pi/","text":"A small computer, good for running linux. http://www.raspberrypi.org Fedora Remix for Pi - http://www.raspberrypi.org/archives/805 Software Raspbian dpkg-reconfigure locales Hass.io \"Hass.io turns your Raspberry Pi (or another device) into the ultimate home automation hub powered by Home Assistant. With Hass.io you can focus on integrating your devices and writing automations.\" - https://home-assistant.io/hassio/ SpillPassPi V1 Retired. 'A Simple Homebrew Plug and Play 3DS HomePass Relay and Fake \"Nintendo Zone\" Hotspot' - http://www.spillmonkey.com/?page_id=5 V2 'A Simple Homebrew Plug and Play 2DS/3DS/N3DS StreetPass Relay and Fake \"Nintendo Zone\" Hotspot' - http://www.spillmonkey.com/?page_id=169 Homepass \"Nintendo 3DS homepass resources and software.\" - https://github.com/danielhoherd/homepass/tree/master/RaspberryPi Links Raspberry Pi SD benchmarking blog post with script linked within: https://www.jeffgeerling.com/blog/2018/raspberry-pi-microsd-card-performance-comparison-2018","title":"raspberry-pi"},{"location":"raspberry-pi/#software","text":"","title":"Software"},{"location":"raspberry-pi/#raspbian","text":"dpkg-reconfigure locales","title":"Raspbian"},{"location":"raspberry-pi/#hassio","text":"\"Hass.io turns your Raspberry Pi (or another device) into the ultimate home automation hub powered by Home Assistant. With Hass.io you can focus on integrating your devices and writing automations.\" - https://home-assistant.io/hassio/","title":"Hass.io"},{"location":"raspberry-pi/#spillpasspi","text":"","title":"SpillPassPi"},{"location":"raspberry-pi/#v1","text":"Retired. 'A Simple Homebrew Plug and Play 3DS HomePass Relay and Fake \"Nintendo Zone\" Hotspot' - http://www.spillmonkey.com/?page_id=5","title":"V1"},{"location":"raspberry-pi/#v2","text":"'A Simple Homebrew Plug and Play 2DS/3DS/N3DS StreetPass Relay and Fake \"Nintendo Zone\" Hotspot' - http://www.spillmonkey.com/?page_id=169","title":"V2"},{"location":"raspberry-pi/#homepass","text":"\"Nintendo 3DS homepass resources and software.\" - https://github.com/danielhoherd/homepass/tree/master/RaspberryPi","title":"Homepass"},{"location":"raspberry-pi/#links","text":"Raspberry Pi SD benchmarking blog post with script linked within: https://www.jeffgeerling.com/blog/2018/raspberry-pi-microsd-card-performance-comparison-2018","title":"Links"},{"location":"redis/","text":"\"Redis is an open source (BSD licensed), in-memory data structure store, used as a database, cache and message broker.\" - https://redis.io Tips and Examples Solve memory allocation problems Errors like this can cause the disk to fill up over long periods of time: [2535] 02 Jan 19:58:52.376 * Starting automatic rewriting of AOF on 7885% growth [2535] 02 Jan 19:58:52.376 # Can't rewrite append only file in background: fork: Cannot allocate memory This problem can be solved without restarting anything: # df -h . Filesystem Size Used Avail Use% Mounted on /dev/xvdf 250G 135G 116G 54% /srv # cat /proc/sys/vm/overcommit_memory 0 # echo 1 > /proc/sys/vm/overcommit_memory # tail redis.log [2535] 02 Jan 22:03:23.707 * Starting automatic rewriting of AOF on 7885% growth [2535] 02 Jan 22:03:23.707 # Can't rewrite append only file in background: fork: Cannot allocate memory [2535] 02 Jan 22:03:23.807 * Starting automatic rewriting of AOF on 7885% growth [2535] 02 Jan 22:03:23.807 # Can't rewrite append only file in background: fork: Cannot allocate memory [2535] 02 Jan 22:03:23.907 * Starting automatic rewriting of AOF on 7885% growth [2535] 02 Jan 22:03:23.926 * Background append only file rewriting started by pid 27302 [27302] 02 Jan 22:04:05.337 * SYNC append only file rewrite performed [27302] 02 Jan 22:04:05.379 * AOF rewrite: 36 MB of memory used by copy-on-write [2535] 02 Jan 22:04:05.406 * Background AOF rewrite terminated with success [2535] 02 Jan 22:04:05.406 * Parent diff successfully flushed to the rewritten AOF (42 bytes) [2535] 02 Jan 22:04:05.406 * Background AOF rewrite finished successfully # df -h . Filesystem Size Used Avail Use% Mounted on /dev/xvdf 250G 4.5G 246G 2% /srv Find what is using the most memory redis-cli --bigkeys Links https://redis.io/topics/faq https://redis.io/commands","title":"redis"},{"location":"redis/#tips-and-examples","text":"","title":"Tips and Examples"},{"location":"redis/#solve-memory-allocation-problems","text":"Errors like this can cause the disk to fill up over long periods of time: [2535] 02 Jan 19:58:52.376 * Starting automatic rewriting of AOF on 7885% growth [2535] 02 Jan 19:58:52.376 # Can't rewrite append only file in background: fork: Cannot allocate memory This problem can be solved without restarting anything: # df -h . Filesystem Size Used Avail Use% Mounted on /dev/xvdf 250G 135G 116G 54% /srv # cat /proc/sys/vm/overcommit_memory 0 # echo 1 > /proc/sys/vm/overcommit_memory # tail redis.log [2535] 02 Jan 22:03:23.707 * Starting automatic rewriting of AOF on 7885% growth [2535] 02 Jan 22:03:23.707 # Can't rewrite append only file in background: fork: Cannot allocate memory [2535] 02 Jan 22:03:23.807 * Starting automatic rewriting of AOF on 7885% growth [2535] 02 Jan 22:03:23.807 # Can't rewrite append only file in background: fork: Cannot allocate memory [2535] 02 Jan 22:03:23.907 * Starting automatic rewriting of AOF on 7885% growth [2535] 02 Jan 22:03:23.926 * Background append only file rewriting started by pid 27302 [27302] 02 Jan 22:04:05.337 * SYNC append only file rewrite performed [27302] 02 Jan 22:04:05.379 * AOF rewrite: 36 MB of memory used by copy-on-write [2535] 02 Jan 22:04:05.406 * Background AOF rewrite terminated with success [2535] 02 Jan 22:04:05.406 * Parent diff successfully flushed to the rewritten AOF (42 bytes) [2535] 02 Jan 22:04:05.406 * Background AOF rewrite finished successfully # df -h . Filesystem Size Used Avail Use% Mounted on /dev/xvdf 250G 4.5G 246G 2% /srv","title":"Solve memory allocation problems"},{"location":"redis/#find-what-is-using-the-most-memory","text":"redis-cli --bigkeys","title":"Find what is using the most memory"},{"location":"redis/#links","text":"https://redis.io/topics/faq https://redis.io/commands","title":"Links"},{"location":"rhel/","text":"\"Red Hat, Inc. is an American multinational software company providing open-source software products to the enterprise community.\" - https://en.wikipedia.org/wiki/Red_Hat See Also CentOS is now owned by RedHat Inline with Upstream Stable Community Project","title":"rhel"},{"location":"rhel/#see-also","text":"CentOS is now owned by RedHat Inline with Upstream Stable Community Project","title":"See Also"},{"location":"robotics/","text":"Links http://botbench.com http://www.andymark.com http://www.vexrobotics.com https://www.adafruit.com https://www.reddit.com/r/robotics/wiki/index https://www.sparkfun.com/ See also Lego Mindstorms Wonder Workshop","title":"robotics"},{"location":"robotics/#links","text":"http://botbench.com http://www.andymark.com http://www.vexrobotics.com https://www.adafruit.com https://www.reddit.com/r/robotics/wiki/index https://www.sparkfun.com/","title":"Links"},{"location":"robotics/#see-also","text":"Lego Mindstorms Wonder Workshop","title":"See also"},{"location":"roku/","text":"Links https://sdkdocs.roku.com/display/sdkdoc/BrightScript+Language+Reference","title":"Roku"},{"location":"roku/#links","text":"https://sdkdocs.roku.com/display/sdkdoc/BrightScript+Language+Reference","title":"Links"},{"location":"rook/","text":"\"File, Block, and Object Storage Services for your Cloud-Native Environments\" - https://rook.io/ Rook is based on ceph.","title":"rook"},{"location":"ros/","text":"\"The Robot Operating System (ROS) is a set of software libraries and tools that help you build robot applications. From drivers to state-of-the-art algorithms, and with powerful developer tools, ROS has what you need for your next robotics project. And it's all open source.\" - http://www.ros.org Links ROS Distributions - http://wiki.ros.org/Distributions ROS tutorials - http://wiki.ros.org/ROS/Tutorials ROS Turtle Example - http://wiki.ros.org/turtlesim/Tutorials Autoware built on top of ROS for Self-driving cars - https://github.com/CPFL/Autoware Robot Operating System for ev3 - http://wiki.ros.org/Robots/EV3 A Gentle Introduction to ROS - https://www.cse.sc.edu/~jokane/agitr/ ROS2 / Apex.AI slides - https://github.com/svautosarusersgroup/meetupslides/","title":"ros"},{"location":"ros/#links","text":"ROS Distributions - http://wiki.ros.org/Distributions ROS tutorials - http://wiki.ros.org/ROS/Tutorials ROS Turtle Example - http://wiki.ros.org/turtlesim/Tutorials Autoware built on top of ROS for Self-driving cars - https://github.com/CPFL/Autoware Robot Operating System for ev3 - http://wiki.ros.org/Robots/EV3 A Gentle Introduction to ROS - https://www.cse.sc.edu/~jokane/agitr/ ROS2 / Apex.AI slides - https://github.com/svautosarusersgroup/meetupslides/","title":"Links"},{"location":"rpm/","text":"Redhat Package Manager. \"rpm is a powerful Package Manager, which can be used to build, install, query, verify, update, and erase individual software packages.\" - man rpm Tricks Show installed keys rpm -qa gpg-pubkey Show extended info about all keys rpm -qa gpg-pubkey | xargs -n1 -P1 rpm -qi Show information about an rpm file rpm -qpi Show all installed packages and when they were installed rpm -qa --last Show information about the installed wget package rpm -qi wget Output formatted information about packages rpm -qa --queryformat \"%{NAME} %{PACKAGER} %{URL}\\n\" tomcat7 More info on queryformat: http://www.rpm.org/max-rpm/ch-queryformat-tags.html Show which package installed a file rpm -qf /usr/bin/wget Show all files that were installed by package wget rpm -ql wget Show all files in a package that is not yet installed rpm -qpl ~/downloads/wget-1.10.2-78.i586.rpm Show which documentation files get installed with a package rpm -qd wget Show what has changed on the system since installing a package This will verify file integrity and show you what has changed for each file. rpm -V openssl Show installation and uninstallation scripts rpm -qp --scripts foo.rpm Check the integrity of an RPM rpm -K ~/downloads/filename.rpm Show which packages are hogging all the space rpm -qa --queryformat \"%{SIZE} %{NAME}\\n\" |sort -rn |head -n50 | column -t Show a table about RPM files versions and creators in a directory rpm -qp --queryformat \"%{NAME},%{VERSION},%{PACKAGER}\\n\" * | column -s, -t Show what files were installed into /var/log rpm -qa --filesbypkg | grep \" /var/log\" # space before /var is necessary to weed out things like /usr/var Rebuild a corrupt rpm db rm -rf /var/lib/rpm/__db* rpm --rebuilddb See Also How to create RPMs - http://fedoraproject.org/wiki/How_to_create_an_RPM_package yum - supplement to rpm command","title":"rpm"},{"location":"rpm/#tricks","text":"","title":"Tricks"},{"location":"rpm/#show-installed-keys","text":"rpm -qa gpg-pubkey","title":"Show installed keys"},{"location":"rpm/#show-extended-info-about-all-keys","text":"rpm -qa gpg-pubkey | xargs -n1 -P1 rpm -qi","title":"Show extended info about all keys"},{"location":"rpm/#show-information-about-an-rpm-file","text":"rpm -qpi","title":"Show information about an rpm file"},{"location":"rpm/#show-all-installed-packages-and-when-they-were-installed","text":"rpm -qa --last","title":"Show all installed packages and when they were installed"},{"location":"rpm/#show-information-about-the-installed-wget-package","text":"rpm -qi wget","title":"Show information about the installed wget package"},{"location":"rpm/#output-formatted-information-about-packages","text":"rpm -qa --queryformat \"%{NAME} %{PACKAGER} %{URL}\\n\" tomcat7 More info on queryformat: http://www.rpm.org/max-rpm/ch-queryformat-tags.html","title":"Output formatted information about packages"},{"location":"rpm/#show-which-package-installed-a-file","text":"rpm -qf /usr/bin/wget","title":"Show which package installed a file"},{"location":"rpm/#show-all-files-that-were-installed-by-package-wget","text":"rpm -ql wget","title":"Show all files that were installed by package wget"},{"location":"rpm/#show-all-files-in-a-package-that-is-not-yet-installed","text":"rpm -qpl ~/downloads/wget-1.10.2-78.i586.rpm","title":"Show all files in a package that is not yet installed"},{"location":"rpm/#show-which-documentation-files-get-installed-with-a-package","text":"rpm -qd wget","title":"Show which documentation files get installed with a package"},{"location":"rpm/#show-what-has-changed-on-the-system-since-installing-a-package","text":"This will verify file integrity and show you what has changed for each file. rpm -V openssl","title":"Show what has changed on the system since installing a package"},{"location":"rpm/#show-installation-and-uninstallation-scripts","text":"rpm -qp --scripts foo.rpm","title":"Show installation and uninstallation scripts"},{"location":"rpm/#check-the-integrity-of-an-rpm","text":"rpm -K ~/downloads/filename.rpm","title":"Check the integrity of an RPM"},{"location":"rpm/#show-which-packages-are-hogging-all-the-space","text":"rpm -qa --queryformat \"%{SIZE} %{NAME}\\n\" |sort -rn |head -n50 | column -t","title":"Show which packages are hogging all the space"},{"location":"rpm/#show-a-table-about-rpm-files-versions-and-creators-in-a-directory","text":"rpm -qp --queryformat \"%{NAME},%{VERSION},%{PACKAGER}\\n\" * | column -s, -t","title":"Show a table about RPM files versions and creators in a directory"},{"location":"rpm/#show-what-files-were-installed-into-varlog","text":"rpm -qa --filesbypkg | grep \" /var/log\" # space before /var is necessary to weed out things like /usr/var","title":"Show what files were installed into /var/log"},{"location":"rpm/#rebuild-a-corrupt-rpm-db","text":"rm -rf /var/lib/rpm/__db* rpm --rebuilddb","title":"Rebuild a corrupt rpm db"},{"location":"rpm/#see-also","text":"How to create RPMs - http://fedoraproject.org/wiki/How_to_create_an_RPM_package yum - supplement to rpm command","title":"See Also"},{"location":"rrd/","text":"\"RRDtool is the OpenSource industry standard, high performance data logging and graphing system for time series data. RRDtool can be easily integrated in shell scripts, perl, python, ruby, lua or tcl applications.\" - https://oss.oetiker.ch/rrdtool/index.en.html Acronyms cs = consolidation function ds = data source dst = data source type rra = round robin archive Examples Reconfigure the X-axis precision of an RRD Assuming the first value (eg: 5856) is the value you want and 244 is the value you currently have, reconfigure data index 0,1,2: sudo rrdtool tune coral/pkts_in.rrd \"RRA#0:+$((5856-244))\" \"RRA#1:+$((20160-244))\" \"RRA#2:+$((52704-244))\" Links https://oss.oetiker.ch/rrdtool/tut/rrd-beginners.en.html","title":"rrd"},{"location":"rrd/#acronyms","text":"cs = consolidation function ds = data source dst = data source type rra = round robin archive","title":"Acronyms"},{"location":"rrd/#examples","text":"","title":"Examples"},{"location":"rrd/#reconfigure-the-x-axis-precision-of-an-rrd","text":"Assuming the first value (eg: 5856) is the value you want and 244 is the value you currently have, reconfigure data index 0,1,2: sudo rrdtool tune coral/pkts_in.rrd \"RRA#0:+$((5856-244))\" \"RRA#1:+$((20160-244))\" \"RRA#2:+$((52704-244))\"","title":"Reconfigure the X-axis precision of an RRD"},{"location":"rrd/#links","text":"https://oss.oetiker.ch/rrdtool/tut/rrd-beginners.en.html","title":"Links"},{"location":"rst/","text":"\"reStructuredText is an easy-to-read, what-you-see-is-what-you-get plaintext markup syntax and parser system.\" - http://docutils.sourceforge.net/rst.html http://rst.ninjs.org/","title":"reStructuredText"},{"location":"rsync/","text":"Great way to sync one location to another, local or remote. Note that this does not mean full synchronization, two commands with reversed source and destinations are required to accomplish that. Syntax Examples Giving additional ssh options rsync -e 'ssh -o ConnectTimeout=10 -o PasswordAuthentication=no' -Rai /home target:/ Exclude Filters Exclude filters are kinda weird. - They're case sensitive and there's no way to be case insensitive. - They are relative to the root of the source URI. EG, rsync --exclude=\"Desktop/\" ~/ remotehost:~/ Here is an example of what to use in --exclude-from=file.txt **Cache **Caches **cache **caches **/.dropbox **Previews.lrdata **/Library/Application\\ Support/Google/Chrome Long and Partial Transfers If you're doing transfers which you'd like to monitor and risk being cut off, use this syntax: rsync -e ssh -az --partial --progress ./foo remotehost:~/bar/ This will resume broken file transfers where they were left off, and give you completion statistics with transfer rate, percent complete and estimated time left. Recursively link src to dst rsync can be used to create a hard linked local copy of a whole tree. This is useful if you don't have GNU cp where the same could be done with simply cp -lrp . On OS X with homebrew , GNU cp can be installed via brew install coreutils and accessed via gcp . See also ls -la /usr/local/opt/coreutils/bin/ . Slashes are really really important here; this won't work if you get them wrong. Absolute paths must be given, thus ${PWD} and ${HOME} vs ~ rsync -aP --link-dest=\"${PWD}/src\" ./src/ dst #recursively hard link ./src to dst For example: rsync -aivv --link-dest=\"${HOME}/Dropbox\" ${HOME}/Dropbox/some_dir ${HOME}/temp/ This will create the directory ${HOME}/temp/some_dir and hard link all the files from the source into the destination. It should only take a few seconds. Lines with 'hf' indicate a hard linked file. Lines with 'cd' indicate 'created directory'. rsync can copy not only data, but also filesystem attributes, and if these differ between the link-dest and the src, a hard link may not be created but instead a copy of the file from the local filesystem is made and correct metadata is applied from the source. Move files to another server in small batches This is useful if you want to gradually clear up disk space rather than waiting until the end of a transfer of a large number of files to clear up disk space in one large operation. while date ; files=$(find /srv/backups/scribe/./ -type f -mtime +400 | head -n 500) ; echo md5 of files ${#files} is $(echo ${files} | md5sum) ; [ ! -z \"${files}\" ] ; do sudo rsync --bwlimit 20000 -RaPi --remove-source-files ${files} root@10.2.17.7:/srv/backups/scribe-sea/ ; echo sleeping ; sleep 10 ; done ; Move all datestamped files older than the beginning of the previous month, excluding symlinks This relies on gnu date, so use gdate if used on OS X. rsync -aPiv \\ --remove-source-files \\ --bwlimit 20000 \\ --exclude=\"**$(date -d \"1 month ago\" \"+%Y-%m\")**\" \\ --exclude=\"**$(date \"+%Y-%m\")**\" \\ --no-links \\ /srv/backups/scribe/* \\ root@10.2.17.7:/srv/backups/scribe-sea/ Reduce time precision during comparison This is useful for rsyncing to FAT filesystems where time precision is 2 seconds. rsync --modify-window=1 # allow 1 second of difference in timestamps See Also prsync pssh pscp","title":"rsync"},{"location":"rsync/#syntax-examples","text":"","title":"Syntax Examples"},{"location":"rsync/#giving-additional-ssh-options","text":"rsync -e 'ssh -o ConnectTimeout=10 -o PasswordAuthentication=no' -Rai /home target:/","title":"Giving additional ssh options"},{"location":"rsync/#exclude-filters","text":"Exclude filters are kinda weird. - They're case sensitive and there's no way to be case insensitive. - They are relative to the root of the source URI. EG, rsync --exclude=\"Desktop/\" ~/ remotehost:~/ Here is an example of what to use in --exclude-from=file.txt **Cache **Caches **cache **caches **/.dropbox **Previews.lrdata **/Library/Application\\ Support/Google/Chrome","title":"Exclude Filters"},{"location":"rsync/#long-and-partial-transfers","text":"If you're doing transfers which you'd like to monitor and risk being cut off, use this syntax: rsync -e ssh -az --partial --progress ./foo remotehost:~/bar/ This will resume broken file transfers where they were left off, and give you completion statistics with transfer rate, percent complete and estimated time left.","title":"Long and Partial Transfers"},{"location":"rsync/#recursively-link-src-to-dst","text":"rsync can be used to create a hard linked local copy of a whole tree. This is useful if you don't have GNU cp where the same could be done with simply cp -lrp . On OS X with homebrew , GNU cp can be installed via brew install coreutils and accessed via gcp . See also ls -la /usr/local/opt/coreutils/bin/ . Slashes are really really important here; this won't work if you get them wrong. Absolute paths must be given, thus ${PWD} and ${HOME} vs ~ rsync -aP --link-dest=\"${PWD}/src\" ./src/ dst #recursively hard link ./src to dst For example: rsync -aivv --link-dest=\"${HOME}/Dropbox\" ${HOME}/Dropbox/some_dir ${HOME}/temp/ This will create the directory ${HOME}/temp/some_dir and hard link all the files from the source into the destination. It should only take a few seconds. Lines with 'hf' indicate a hard linked file. Lines with 'cd' indicate 'created directory'. rsync can copy not only data, but also filesystem attributes, and if these differ between the link-dest and the src, a hard link may not be created but instead a copy of the file from the local filesystem is made and correct metadata is applied from the source.","title":"Recursively link src to dst"},{"location":"rsync/#move-files-to-another-server-in-small-batches","text":"This is useful if you want to gradually clear up disk space rather than waiting until the end of a transfer of a large number of files to clear up disk space in one large operation. while date ; files=$(find /srv/backups/scribe/./ -type f -mtime +400 | head -n 500) ; echo md5 of files ${#files} is $(echo ${files} | md5sum) ; [ ! -z \"${files}\" ] ; do sudo rsync --bwlimit 20000 -RaPi --remove-source-files ${files} root@10.2.17.7:/srv/backups/scribe-sea/ ; echo sleeping ; sleep 10 ; done ;","title":"Move files to another server in small batches"},{"location":"rsync/#move-all-datestamped-files-older-than-the-beginning-of-the-previous-month-excluding-symlinks","text":"This relies on gnu date, so use gdate if used on OS X. rsync -aPiv \\ --remove-source-files \\ --bwlimit 20000 \\ --exclude=\"**$(date -d \"1 month ago\" \"+%Y-%m\")**\" \\ --exclude=\"**$(date \"+%Y-%m\")**\" \\ --no-links \\ /srv/backups/scribe/* \\ root@10.2.17.7:/srv/backups/scribe-sea/","title":"Move all datestamped files older than the beginning of the previous month, excluding symlinks"},{"location":"rsync/#reduce-time-precision-during-comparison","text":"This is useful for rsyncing to FAT filesystems where time precision is 2 seconds. rsync --modify-window=1 # allow 1 second of difference in timestamps","title":"Reduce time precision during comparison"},{"location":"rsync/#see-also","text":"prsync pssh pscp","title":"See Also"},{"location":"saltstack/","text":"\"Software to automate the management and configuration of any infrastructure or application at scale.\" - http://github.com/saltstack/salt https://github.com/saltstack/salt https://docs.saltstack.com/en/latest/topics/tutorials https://docs.saltstack.com/en/latest/topics/development/index.html Design characteristics Glossary: https://docs.saltstack.com/en/latest/glossary.html Common commands All salt* commands require root access, so use sudo or log in as root. salt : Salt allows for commands to be executed across a swath of remote systems in parallel. This means that remote systems can be both controlled and queried with ease. salt-call : The salt-call command is used to run module functions locally on a minion instead of executing them from the master. Salt-call is used to run a Standalone Minion, and was originally created for troubleshooting. salt-cloud : Salt Cloud is the system used to provision virtual machines on various public clouds via a cleanly controlled profile and mapping system. salt-cp : Salt copy copies a local file out to all of the Salt minions matched by the given target. salt-key : Salt-key executes simple management of Salt server public keys used for authentication. salt-minion : The Salt minion daemon, receives commands from a remote Salt master. salt-run : salt-run is the frontend command for executing Salt Runners. Salt runners are simple modules used to execute convenience functions on the master. salt-ssh : Salt SSH allows for salt routines to be executed using only SSH for transport. State files These are desired state files, not the view of the current state. These are where you describe how you want the system to be Grains Facts about a system. Similar to facter in puppet land. Pillar Hierarchical data to be interpolated into variables in state files. Similar to hiera in puppet land. Examples Configure output options Unfortunately this only applies to the salt command, not salt-run , salt-key , etc.. $ cat ~/.saltrc output: yaml View salt versions For simple salt version: salt --version For more specific versions: salt --versions Show all minions Show all responding minions salt-run manage.up Show all minions, listed by hostst that are up and hosts that are down salt-run manage.status Example output: down: - hadoop4.chn1.example up: - appserver1.chn1.example - backups1.chn1.example Show any host that has had salt applied at some point This shows only accepted keys. Without the jq part, rejected and denied keys would also show up in this list. salt-key --out json | jq '.minions[]' Accept a key that has not yet been accepted After finding the hostname in the Unaccepted list returned by salt-key : salt-key -a hostname.example.com Show the version of an installed package on all hosts salt '*' pkg.version bash Targeting hosts https://docs.saltstack.com/en/latest/topics/targeting/#advanced-targeting-methods Target using globs salt '*dev*' pkg.install pre-commit Target using regular expressions salt -b1 -E 'miner..-aws' cmd.run 'service miner restart' Target an IP subnet salt -t 15 -S '172.21.5.0/24' cmd.run 'dpkg -l linux-image' Target a specific OS https://docs.saltstack.com/en/latest/topics/targeting/compound.html salt -C 'G@lsb_distrib_codename:trusty' pkg.install cmatrix Run a command on a subset of hosts Check ntp stats on hadoop hosts. salt \"*hadoop*\" cmd.run \"ntpq -p\" Many more complicated examples of remote command execution: https://docs.saltstack.com/en/latest/topics/execution/remote_execution.html Show IP addresses https://docs.saltstack.com/en/latest/ref/modules/all/salt.modules.network.html salt '*itni*' network.ip_addrs Show available grains This just lists the grain keys, not the values salt '*minecraft*' grains.ls Show grain data for a subset of hosts This lists the keys and values salt '*dorks*' grains.items Show one grain for a subset of hosts salt '*elk*' grains.fetch lsb_distrib_release or... salt '*elk*' grains.item os Look up grain data while logged into a minion While logged into a minion, you can view what pillar data would be applied: salt-call pillar.get users Append a username to the accounts grain and apply the users saltstate salt '*searchstring*' grains.append accounts user-to-add salt '*searchstring*' state.sls users salt '*searchstring*' user.list_users --out yaml > list_users.yaml Or as a function to run locally add_user_via_salt_grains() { new_user=$1 id \"${new_user}\" && return 0 salt-call grains.append accounts \"$new_user\" && \\ salt-call state.sls users id \"$new_user\" }","title":"saltstack"},{"location":"saltstack/#design-characteristics","text":"Glossary: https://docs.saltstack.com/en/latest/glossary.html","title":"Design characteristics"},{"location":"saltstack/#common-commands","text":"All salt* commands require root access, so use sudo or log in as root. salt : Salt allows for commands to be executed across a swath of remote systems in parallel. This means that remote systems can be both controlled and queried with ease. salt-call : The salt-call command is used to run module functions locally on a minion instead of executing them from the master. Salt-call is used to run a Standalone Minion, and was originally created for troubleshooting. salt-cloud : Salt Cloud is the system used to provision virtual machines on various public clouds via a cleanly controlled profile and mapping system. salt-cp : Salt copy copies a local file out to all of the Salt minions matched by the given target. salt-key : Salt-key executes simple management of Salt server public keys used for authentication. salt-minion : The Salt minion daemon, receives commands from a remote Salt master. salt-run : salt-run is the frontend command for executing Salt Runners. Salt runners are simple modules used to execute convenience functions on the master. salt-ssh : Salt SSH allows for salt routines to be executed using only SSH for transport.","title":"Common commands"},{"location":"saltstack/#state-files","text":"These are desired state files, not the view of the current state. These are where you describe how you want the system to be","title":"State files"},{"location":"saltstack/#grains","text":"Facts about a system. Similar to facter in puppet land.","title":"Grains"},{"location":"saltstack/#pillar","text":"Hierarchical data to be interpolated into variables in state files. Similar to hiera in puppet land.","title":"Pillar"},{"location":"saltstack/#examples","text":"","title":"Examples"},{"location":"saltstack/#configure-output-options","text":"Unfortunately this only applies to the salt command, not salt-run , salt-key , etc.. $ cat ~/.saltrc output: yaml","title":"Configure output options"},{"location":"saltstack/#view-salt-versions","text":"For simple salt version: salt --version For more specific versions: salt --versions","title":"View salt versions"},{"location":"saltstack/#show-all-minions","text":"","title":"Show all minions"},{"location":"saltstack/#show-all-responding-minions","text":"salt-run manage.up","title":"Show all responding minions"},{"location":"saltstack/#show-all-minions-listed-by-hostst-that-are-up-and-hosts-that-are-down","text":"salt-run manage.status Example output: down: - hadoop4.chn1.example up: - appserver1.chn1.example - backups1.chn1.example","title":"Show all minions, listed by hostst that are up and hosts that are down"},{"location":"saltstack/#show-any-host-that-has-had-salt-applied-at-some-point","text":"This shows only accepted keys. Without the jq part, rejected and denied keys would also show up in this list. salt-key --out json | jq '.minions[]'","title":"Show any host that has had salt applied at some point"},{"location":"saltstack/#accept-a-key-that-has-not-yet-been-accepted","text":"After finding the hostname in the Unaccepted list returned by salt-key : salt-key -a hostname.example.com","title":"Accept a key that has not yet been accepted"},{"location":"saltstack/#show-the-version-of-an-installed-package-on-all-hosts","text":"salt '*' pkg.version bash","title":"Show the version of an installed package on all hosts"},{"location":"saltstack/#targeting-hosts","text":"https://docs.saltstack.com/en/latest/topics/targeting/#advanced-targeting-methods","title":"Targeting hosts"},{"location":"saltstack/#target-using-globs","text":"salt '*dev*' pkg.install pre-commit","title":"Target using globs"},{"location":"saltstack/#target-using-regular-expressions","text":"salt -b1 -E 'miner..-aws' cmd.run 'service miner restart'","title":"Target using regular expressions"},{"location":"saltstack/#target-an-ip-subnet","text":"salt -t 15 -S '172.21.5.0/24' cmd.run 'dpkg -l linux-image'","title":"Target an IP subnet"},{"location":"saltstack/#target-a-specific-os","text":"https://docs.saltstack.com/en/latest/topics/targeting/compound.html salt -C 'G@lsb_distrib_codename:trusty' pkg.install cmatrix","title":"Target a specific OS"},{"location":"saltstack/#run-a-command-on-a-subset-of-hosts","text":"Check ntp stats on hadoop hosts. salt \"*hadoop*\" cmd.run \"ntpq -p\" Many more complicated examples of remote command execution: https://docs.saltstack.com/en/latest/topics/execution/remote_execution.html","title":"Run a command on a subset of hosts"},{"location":"saltstack/#show-ip-addresses","text":"https://docs.saltstack.com/en/latest/ref/modules/all/salt.modules.network.html salt '*itni*' network.ip_addrs","title":"Show IP addresses"},{"location":"saltstack/#show-available-grains","text":"This just lists the grain keys, not the values salt '*minecraft*' grains.ls","title":"Show available grains"},{"location":"saltstack/#show-grain-data-for-a-subset-of-hosts","text":"This lists the keys and values salt '*dorks*' grains.items","title":"Show grain data for a subset of hosts"},{"location":"saltstack/#show-one-grain-for-a-subset-of-hosts","text":"salt '*elk*' grains.fetch lsb_distrib_release or... salt '*elk*' grains.item os","title":"Show one grain for a subset of hosts"},{"location":"saltstack/#look-up-grain-data-while-logged-into-a-minion","text":"While logged into a minion, you can view what pillar data would be applied: salt-call pillar.get users","title":"Look up grain data while logged into a minion"},{"location":"saltstack/#append-a-username-to-the-accounts-grain-and-apply-the-users-saltstate","text":"salt '*searchstring*' grains.append accounts user-to-add salt '*searchstring*' state.sls users salt '*searchstring*' user.list_users --out yaml > list_users.yaml Or as a function to run locally add_user_via_salt_grains() { new_user=$1 id \"${new_user}\" && return 0 salt-call grains.append accounts \"$new_user\" && \\ salt-call state.sls users id \"$new_user\" }","title":"Append a username to the accounts grain and apply the users saltstate"},{"location":"samsung/","text":"Android links relevant to Samsung http://www.sammobile.com/firmwares/database/SM-T700/ https://samsung-firmware.org/all/ https://en.wikipedia.org/wiki/Android_version_history https://en.wikipedia.org/wiki/Samsung_Galaxy_Tab_S_8.4 https://www.kingoapp.com/help/samsung-knox-counter.htm TV links UN55D6000 specs UN65KS8000 specs","title":"Samsung"},{"location":"samsung/#android-links-relevant-to-samsung","text":"http://www.sammobile.com/firmwares/database/SM-T700/ https://samsung-firmware.org/all/ https://en.wikipedia.org/wiki/Android_version_history https://en.wikipedia.org/wiki/Samsung_Galaxy_Tab_S_8.4 https://www.kingoapp.com/help/samsung-knox-counter.htm","title":"Android links relevant to Samsung"},{"location":"samsung/#tv-links","text":"UN55D6000 specs UN65KS8000 specs","title":"TV links"},{"location":"screenshot/","text":"Different methods for grabbing an graphical image of what is being displayed on a physical or virtual device. Linux Grab all vt screenshots for X in {0..10} ; do sudo DISPLAY=:0 fbgrab -c${X} fbgrab_vt${X}_screenshot.png ; done ; Screenshot X using scrot sudo DISPLAY=:0 scrot -b -d 5 'scrot_%F-%T.png' Screenshot X using imagemagick sudo DISPLAY=:0 import -window root imagemagick_screenshot.png macOS screencapture screenshot.png There are a lot of command line args for this tool.","title":"Screenshots"},{"location":"screenshot/#linux","text":"","title":"Linux"},{"location":"screenshot/#grab-all-vt-screenshots","text":"for X in {0..10} ; do sudo DISPLAY=:0 fbgrab -c${X} fbgrab_vt${X}_screenshot.png ; done ;","title":"Grab all vt screenshots"},{"location":"screenshot/#screenshot-x-using-scrot","text":"sudo DISPLAY=:0 scrot -b -d 5 'scrot_%F-%T.png'","title":"Screenshot X using scrot"},{"location":"screenshot/#screenshot-x-using-imagemagick","text":"sudo DISPLAY=:0 import -window root imagemagick_screenshot.png","title":"Screenshot X using imagemagick"},{"location":"screenshot/#macos","text":"screencapture screenshot.png There are a lot of command line args for this tool.","title":"macOS"},{"location":"sdr/","text":"Software Defined Radio Overview SDR is dominated by windows software, so this is going to leave all that out and deal with Linux and Mac OS software. Links NOAA weather by state - http://www.nws.noaa.gov/nwr/coverage/ccov.php Good scanning software - http://gqrx.dk/ https://www.radioreference.com/ http://www.sigidwiki.com/ Baudline - \"Baudline is a time-frequency browser designed for scientific visualization of the spectral domain.\" GNU Radio - \"GNU Radio is a free & open-source software development toolkit that provides signal processing blocks to implement software radios.\" Inspectrum - \"inspectrum is a tool for analysing captured signals, primarily from software-defined radio receivers.\"","title":"sdr"},{"location":"sdr/#overview","text":"SDR is dominated by windows software, so this is going to leave all that out and deal with Linux and Mac OS software.","title":"Overview"},{"location":"sdr/#links","text":"NOAA weather by state - http://www.nws.noaa.gov/nwr/coverage/ccov.php Good scanning software - http://gqrx.dk/ https://www.radioreference.com/ http://www.sigidwiki.com/ Baudline - \"Baudline is a time-frequency browser designed for scientific visualization of the spectral domain.\" GNU Radio - \"GNU Radio is a free & open-source software development toolkit that provides signal processing blocks to implement software radios.\" Inspectrum - \"inspectrum is a tool for analysing captured signals, primarily from software-defined radio receivers.\"","title":"Links"},{"location":"sed/","text":"sed is the stream editor. Tips OSX Pitfalls Beware that BSD sed -i requires a mandatory flag for the backup file. You can use -i '' to have no backup file. Also, OS X sed doesn't support case insensitivity! WTF?! We have to use perl -pe 's/foo/bar/i' foo.txt or homebrew's gsed . Only print a specific line This will print only the second line of the file sed -n ' 2{p;q;}' foo.txt Only print if match This will perform a replacement and print the result. Use -i (with caution!) to edit the file at the same time. sed -n 's/\\(127.[0-9]\\{1,3\\}.[0-9]\\{1,3\\}.[0-9]\\{1,3\\}\\)/\\1 localhost localhost4/p' /etc/hosts Add a new line with content after a match Since sed can't insert things like \\n, this has to take place on multiple lines, so it's a bit funky looking but still functional. sed -i \"\" -e '/respawn/a\\ respawn limit 10 5' zoosk_worker_*.conf Uncomment a line that matches a regex This removes the comment and adds wheel to the sudoers list /bin/sed -i '/^#\\s\\+%wheel\\s\\+ALL=(ALL)\\s\\+ALL$/s/^#\\s*//' /etc/sudoers Delete lines containing a string sed -i -e '/root/d' asdf.txt Delete lines not containing a string sed -i '/foo/!d' wy.txt Or not containing a MAC address: sed -i '' -E '/[0-9a-f]{2}:[0-9a-f]{2}:[0-9a-f]{2}:[0-9a-f]{2}:[0-9a-f]{2}:[0-9a-f]{2}/!d' * Do a replacement on all files in a dir sed -i \"s/foo/bar/g\" /etc/apache2/sites-available/* Switch all github urls from http to ssh sed '/url = /s%https?://github.com/\\([^/]*/[^/]*\\)%git@github.com:\\1%' ~/code/*/.git/config Word boundaries Normally, word boundaries look like this: /\\bMyWord\\b/ or /\\<myword\\>/ But in OS X, you have to do them like this: /[[:<:]]MyWord[[:>:]]/ Which is just ridiculous, so use homebrew's gsed if you can. Add a bell to tail -f tail -n 0 -f /var/log/messages | sed 's/$/\\a' See Also Some great sed tips - http://www-rohan.sdsu.edu/doc/sed.html","title":"sed"},{"location":"sed/#tips","text":"","title":"Tips"},{"location":"sed/#osx-pitfalls","text":"Beware that BSD sed -i requires a mandatory flag for the backup file. You can use -i '' to have no backup file. Also, OS X sed doesn't support case insensitivity! WTF?! We have to use perl -pe 's/foo/bar/i' foo.txt or homebrew's gsed .","title":"OSX Pitfalls"},{"location":"sed/#only-print-a-specific-line","text":"This will print only the second line of the file sed -n ' 2{p;q;}' foo.txt","title":"Only print a specific line"},{"location":"sed/#only-print-if-match","text":"This will perform a replacement and print the result. Use -i (with caution!) to edit the file at the same time. sed -n 's/\\(127.[0-9]\\{1,3\\}.[0-9]\\{1,3\\}.[0-9]\\{1,3\\}\\)/\\1 localhost localhost4/p' /etc/hosts","title":"Only print if match"},{"location":"sed/#add-a-new-line-with-content-after-a-match","text":"Since sed can't insert things like \\n, this has to take place on multiple lines, so it's a bit funky looking but still functional. sed -i \"\" -e '/respawn/a\\ respawn limit 10 5' zoosk_worker_*.conf","title":"Add a new line with content after a match"},{"location":"sed/#uncomment-a-line-that-matches-a-regex","text":"This removes the comment and adds wheel to the sudoers list /bin/sed -i '/^#\\s\\+%wheel\\s\\+ALL=(ALL)\\s\\+ALL$/s/^#\\s*//' /etc/sudoers","title":"Uncomment a line that matches a regex"},{"location":"sed/#delete-lines-containing-a-string","text":"sed -i -e '/root/d' asdf.txt","title":"Delete lines containing a string"},{"location":"sed/#delete-lines-not-containing-a-string","text":"sed -i '/foo/!d' wy.txt Or not containing a MAC address: sed -i '' -E '/[0-9a-f]{2}:[0-9a-f]{2}:[0-9a-f]{2}:[0-9a-f]{2}:[0-9a-f]{2}:[0-9a-f]{2}/!d' *","title":"Delete lines not containing a string"},{"location":"sed/#do-a-replacement-on-all-files-in-a-dir","text":"sed -i \"s/foo/bar/g\" /etc/apache2/sites-available/*","title":"Do a replacement on all files in a dir"},{"location":"sed/#switch-all-github-urls-from-http-to-ssh","text":"sed '/url = /s%https?://github.com/\\([^/]*/[^/]*\\)%git@github.com:\\1%' ~/code/*/.git/config","title":"Switch all github urls from http to ssh"},{"location":"sed/#word-boundaries","text":"Normally, word boundaries look like this: /\\bMyWord\\b/ or /\\<myword\\>/ But in OS X, you have to do them like this: /[[:<:]]MyWord[[:>:]]/ Which is just ridiculous, so use homebrew's gsed if you can.","title":"Word boundaries"},{"location":"sed/#add-a-bell-to-tail-f","text":"tail -n 0 -f /var/log/messages | sed 's/$/\\a'","title":"Add a bell to tail -f"},{"location":"sed/#see-also","text":"Some great sed tips - http://www-rohan.sdsu.edu/doc/sed.html","title":"See Also"},{"location":"selinux/","text":"Security Enhanced Linux Notes Tutorial Video: https://www.youtube.com/watch?v=MxjenQ31b70 CentOS HowTo: http://wiki.centos.org/HowTos/SELinux Labels are in user:role:type:level(optional) Logs go in /var/log/audit/audit.log and /var/log/messages Additional tools: semanage and more are included in CentOS package policycoreutils setroubleshoot has a bunch of tools included. Lots of prerequisites setroubleshoot-server has a bunch of tools included. Lots of prerequisites Examples Show status of selinux sestatus getenforce Disable without rebooting echo 0 >/selinux/enforce or... setenforce 0 List selinux contexts for processes ps auxZ List selinux contexts for processes that have open sockets lsof -i -Z # See `man lsof` for more specific selinux syntaxes List selinux contexts for the current user id -Z List selinux contexts for files ls -lZ Recursively set a context type chcon -R -t httpd_sys_content_t sole Copy the selinux context from another file or directory chcon --reference /file/or/dir/to/reference /target/file Restore default contexts This command restores the contexts as referenced in /etc/selinux/targeted/contexts/files/file_contexts restorecon /path/to/broken/file restorecon -vR /path/to/broken/dir Restore defaults context automatically at system reboot This should take roughly the same amount of time as a fsck would. touch /.autorelabel Define a default context for a directory semanage fcontext -a /z5/sole Define a default context for a directory, using a reference from the original policy semanage fcontext -a -e /var/www /z5/sole cat /etc/selinux/targeted/contexts/files/file_contexts.subs # view the result List policies semanage port -l semanage user -l Show selinux booleans getsebool -a Permanetnly set an selinux boolean setsebool -P booleanname 1","title":"selinux"},{"location":"selinux/#notes","text":"Tutorial Video: https://www.youtube.com/watch?v=MxjenQ31b70 CentOS HowTo: http://wiki.centos.org/HowTos/SELinux Labels are in user:role:type:level(optional) Logs go in /var/log/audit/audit.log and /var/log/messages Additional tools: semanage and more are included in CentOS package policycoreutils setroubleshoot has a bunch of tools included. Lots of prerequisites setroubleshoot-server has a bunch of tools included. Lots of prerequisites","title":"Notes"},{"location":"selinux/#examples","text":"","title":"Examples"},{"location":"selinux/#show-status-of-selinux","text":"sestatus getenforce","title":"Show status of selinux"},{"location":"selinux/#disable-without-rebooting","text":"echo 0 >/selinux/enforce or... setenforce 0","title":"Disable without rebooting"},{"location":"selinux/#list-selinux-contexts-for-processes","text":"ps auxZ","title":"List selinux contexts for processes"},{"location":"selinux/#list-selinux-contexts-for-processes-that-have-open-sockets","text":"lsof -i -Z # See `man lsof` for more specific selinux syntaxes","title":"List selinux contexts for processes that have open sockets"},{"location":"selinux/#list-selinux-contexts-for-the-current-user","text":"id -Z","title":"List selinux contexts for the current user"},{"location":"selinux/#list-selinux-contexts-for-files","text":"ls -lZ","title":"List selinux contexts for files"},{"location":"selinux/#recursively-set-a-context-type","text":"chcon -R -t httpd_sys_content_t sole","title":"Recursively set a context type"},{"location":"selinux/#copy-the-selinux-context-from-another-file-or-directory","text":"chcon --reference /file/or/dir/to/reference /target/file","title":"Copy the selinux context from another file or directory"},{"location":"selinux/#restore-default-contexts","text":"This command restores the contexts as referenced in /etc/selinux/targeted/contexts/files/file_contexts restorecon /path/to/broken/file restorecon -vR /path/to/broken/dir","title":"Restore default contexts"},{"location":"selinux/#restore-defaults-context-automatically-at-system-reboot","text":"This should take roughly the same amount of time as a fsck would. touch /.autorelabel","title":"Restore defaults context automatically at system reboot"},{"location":"selinux/#define-a-default-context-for-a-directory","text":"semanage fcontext -a /z5/sole","title":"Define a default context for a directory"},{"location":"selinux/#define-a-default-context-for-a-directory-using-a-reference-from-the-original-policy","text":"semanage fcontext -a -e /var/www /z5/sole cat /etc/selinux/targeted/contexts/files/file_contexts.subs # view the result","title":"Define a default context for a directory, using a reference from the original policy"},{"location":"selinux/#list-policies","text":"semanage port -l semanage user -l","title":"List policies"},{"location":"selinux/#show-selinux-booleans","text":"getsebool -a","title":"Show selinux booleans"},{"location":"selinux/#permanetnly-set-an-selinux-boolean","text":"setsebool -P booleanname 1","title":"Permanetnly set an selinux boolean"},{"location":"semver/","text":"Semantic Versioning","title":"semver"},{"location":"sensu/","text":"\"Monitor servers, services, application health, and business KPIs. Get notified about failures before your users do. Collect and analyze custom metrics. Give your business the competitive advantage it deserves.\" - https://sensuapp.org Overview Checks - used to monitor services or measure resources Handlers - for taking action on Sensu events, which are produced by checks Filters - for filtering (removing) events destined for one or more event handlers Mutators - transform event data for handlers Checks standalone checks are scheduled to run periodically on the client (eg: all hosts need to check disks every 15 minutes) subscription checks are requested by the server to hosts with a given tag (eg: all web hosts need to run check_http) See Also Uchiwa - Open source dashboard for Sensu. Puppet + Sensu = Love; Infrastructure as Code and Monitoring, Sharing the Same Development Workflow SF DevOps Meetup: Kyle Anderson, Sensu @ Yelp Part 1 , Part 2","title":"sensu"},{"location":"sensu/#overview","text":"Checks - used to monitor services or measure resources Handlers - for taking action on Sensu events, which are produced by checks Filters - for filtering (removing) events destined for one or more event handlers Mutators - transform event data for handlers","title":"Overview"},{"location":"sensu/#checks","text":"standalone checks are scheduled to run periodically on the client (eg: all hosts need to check disks every 15 minutes) subscription checks are requested by the server to hosts with a given tag (eg: all web hosts need to run check_http)","title":"Checks"},{"location":"sensu/#see-also","text":"Uchiwa - Open source dashboard for Sensu. Puppet + Sensu = Love; Infrastructure as Code and Monitoring, Sharing the Same Development Workflow SF DevOps Meetup: Kyle Anderson, Sensu @ Yelp Part 1 , Part 2","title":"See Also"},{"location":"serverless/","text":"\"Serverless is your toolkit for deploying and operating serverless architectures. Focus on your application, not your infrastructure.\" - https://serverless.com/","title":"serverless"},{"location":"sgdisk/","text":"\"sgdisk - Command-line GUID partition table (GPT) manipulator for Linux and Unix\" - man sgdisk Examples Randomize GUIDs to ensure uniqueness after cloning sgdisk --randomize-guids /dev/sdz Print info about partitions on a disk sgdisk -p /dev/sdz Print last block number of the largest available section of the disk sgdisk -E /dev/sdz Create a new partition The syntax is partitionNumber:firstBlock:lastBlock sgdisk -n 1:2048:732566636 /dev/sdz Print extended info about the first partition on /dev/sda sgdisk -i 1 /dev/sdz Backup a GUID partition table sgdisk -b ~/sdz_partition_backup /dev/sdz Restore a GUID partition table sgdisk -l ~/sdz_partition_backup /dev/sdz Create a new partition This creates a 4th partition that is 50G using the default starting point (0 is default) sgdisk /dev/sdz --new=4:0:+50G Delete the 4th partition sgdisk /dev/sdz -d 4 Create a new partition number 4 that fills the biggest available section of the disk sgdisk /dev/sdz -N 4 Grab the name of a partition # sloppy, doesn't handle spaces or single quotes sgdisk /dev/sdk -i 1 | grep '^Partition name' | awk '{print $NF}' | sed \"s/'//g\"","title":"sgdisk"},{"location":"sgdisk/#examples","text":"","title":"Examples"},{"location":"sgdisk/#randomize-guids-to-ensure-uniqueness-after-cloning","text":"sgdisk --randomize-guids /dev/sdz","title":"Randomize GUIDs to ensure uniqueness after cloning"},{"location":"sgdisk/#print-info-about-partitions-on-a-disk","text":"sgdisk -p /dev/sdz","title":"Print info about partitions on a disk"},{"location":"sgdisk/#print-last-block-number-of-the-largest-available-section-of-the-disk","text":"sgdisk -E /dev/sdz","title":"Print last block number of the largest available section of the disk"},{"location":"sgdisk/#create-a-new-partition","text":"The syntax is partitionNumber:firstBlock:lastBlock sgdisk -n 1:2048:732566636 /dev/sdz","title":"Create a new partition"},{"location":"sgdisk/#print-extended-info-about-the-first-partition-on-devsda","text":"sgdisk -i 1 /dev/sdz","title":"Print extended info about the first partition on /dev/sda"},{"location":"sgdisk/#backup-a-guid-partition-table","text":"sgdisk -b ~/sdz_partition_backup /dev/sdz","title":"Backup a GUID partition table"},{"location":"sgdisk/#restore-a-guid-partition-table","text":"sgdisk -l ~/sdz_partition_backup /dev/sdz","title":"Restore a GUID partition table"},{"location":"sgdisk/#create-a-new-partition_1","text":"This creates a 4th partition that is 50G using the default starting point (0 is default) sgdisk /dev/sdz --new=4:0:+50G","title":"Create a new partition"},{"location":"sgdisk/#delete-the-4th-partition","text":"sgdisk /dev/sdz -d 4","title":"Delete the 4th partition"},{"location":"sgdisk/#create-a-new-partition-number-4-that-fills-the-biggest-available-section-of-the-disk","text":"sgdisk /dev/sdz -N 4","title":"Create a new partition number 4 that fills the biggest available section of the disk"},{"location":"sgdisk/#grab-the-name-of-a-partition","text":"# sloppy, doesn't handle spaces or single quotes sgdisk /dev/sdk -i 1 | grep '^Partition name' | awk '{print $NF}' | sed \"s/'//g\"","title":"Grab the name of a partition"},{"location":"shairport-sync/","text":"\"AirPlay audio player. Shairport Sync adds multi-room capability with Audio Synchronisation\" - https://github.com/mikebrady/shairport-sync Example shairport-sync in docker This is a really easy way to get shairport-sync running on Linux. The container is 12mb. docker run \\ --rm \\ --detach \\ --network=host \\ --device /dev/snd \\ -e AIRPLAY_NAME=\"shairport ${HOSTNAME}\" \\ kevineye/shairport-sync","title":"shairport-sync"},{"location":"shairport-sync/#example","text":"","title":"Example"},{"location":"shairport-sync/#shairport-sync-in-docker","text":"This is a really easy way to get shairport-sync running on Linux. The container is 12mb. docker run \\ --rm \\ --detach \\ --network=host \\ --device /dev/snd \\ -e AIRPLAY_NAME=\"shairport ${HOSTNAME}\" \\ kevineye/shairport-sync","title":"shairport-sync in docker"},{"location":"shred/","text":"\"shred - overwrite a file to hide its contents, and optionally delete it.\" - man shred Examples Write random data to a hard disk 5 times, then once with zeroes shred -n 5 -z /dev/disk/by-id/usb-Seagate_External_2HC015KJ-0:0 Write zeroes to a disk 5 times and report progress for _ in {1..5} ; do shred -n 0 -z -v /dev/disk/by-id/usb-Seagate_External_2HC015KJ-0:0 ; done ;","title":"Shred"},{"location":"shred/#examples","text":"","title":"Examples"},{"location":"shred/#write-random-data-to-a-hard-disk-5-times-then-once-with-zeroes","text":"shred -n 5 -z /dev/disk/by-id/usb-Seagate_External_2HC015KJ-0:0","title":"Write random data to a hard disk 5 times, then once with zeroes"},{"location":"shred/#write-zeroes-to-a-disk-5-times-and-report-progress","text":"for _ in {1..5} ; do shred -n 0 -z -v /dev/disk/by-id/usb-Seagate_External_2HC015KJ-0:0 ; done ;","title":"Write zeroes to a disk 5 times and report progress"},{"location":"shutdown/","text":"a command to reboot a *nix server. Examples Reboot a server in 5 minutes with a message shutdown -r +5 \"Please announce in #dev if you would like to cancel this reboot\" Power off a server in 5 minutes shutdown -P +5 Cancel a scheduled shutdown shutdown -c","title":"shutdown"},{"location":"shutdown/#examples","text":"","title":"Examples"},{"location":"shutdown/#reboot-a-server-in-5-minutes-with-a-message","text":"shutdown -r +5 \"Please announce in #dev if you would like to cancel this reboot\"","title":"Reboot a server in 5 minutes with a message"},{"location":"shutdown/#power-off-a-server-in-5-minutes","text":"shutdown -P +5","title":"Power off a server in 5 minutes"},{"location":"shutdown/#cancel-a-scheduled-shutdown","text":"shutdown -c","title":"Cancel a scheduled shutdown"},{"location":"sips/","text":"\"scriptable image processing system.\" - sip --help Examples Resize a DNG and save the output as JPG SOURCE=foo.dng sips --resampleHeightWidthMax 1024 --setProperty format jpeg \"$SOURCE\" --out \"${SOURCE%.dng}.jpg\" Resize all images in the CWD that were taken by the D5100 mdfind -onlyin \"$PWD\" 'kMDItemAcquisitionModel == \"NIKON D5100\"' | while read -r line ; do sips --resampleHeightWidthMax 1600 --setProperty format jpeg \"${line}\" --out \"${line%.*}.jpg\" done Resize all images in a dir tree, convert them to jpg and output them to a different folder In the following example it is important to leave off the trailing slash on the target dir: SRC_DIR=\"${HOME}/Pictures/photo_queue\" OUT_DIR=\"${HOME}/Desktop/Stuff\" MAX_WIDTH=1600 find \"${SRC_DIR}\" \\ -type f \\ -exec sips \\ --resampleHeightWidthMax \"${MAX_WIDTH}\" \\ --setProperty format jpeg {} \\ --out \"${OUT_DIR}\" \\; See Also exiftool graphicsmagick imagemagick jpeginfo","title":"sips"},{"location":"sips/#examples","text":"","title":"Examples"},{"location":"sips/#resize-a-dng-and-save-the-output-as-jpg","text":"SOURCE=foo.dng sips --resampleHeightWidthMax 1024 --setProperty format jpeg \"$SOURCE\" --out \"${SOURCE%.dng}.jpg\"","title":"Resize a DNG and save the output as JPG"},{"location":"sips/#resize-all-images-in-the-cwd-that-were-taken-by-the-d5100","text":"mdfind -onlyin \"$PWD\" 'kMDItemAcquisitionModel == \"NIKON D5100\"' | while read -r line ; do sips --resampleHeightWidthMax 1600 --setProperty format jpeg \"${line}\" --out \"${line%.*}.jpg\" done","title":"Resize all images in the CWD that were taken by the D5100"},{"location":"sips/#resize-all-images-in-a-dir-tree-convert-them-to-jpg-and-output-them-to-a-different-folder","text":"In the following example it is important to leave off the trailing slash on the target dir: SRC_DIR=\"${HOME}/Pictures/photo_queue\" OUT_DIR=\"${HOME}/Desktop/Stuff\" MAX_WIDTH=1600 find \"${SRC_DIR}\" \\ -type f \\ -exec sips \\ --resampleHeightWidthMax \"${MAX_WIDTH}\" \\ --setProperty format jpeg {} \\ --out \"${OUT_DIR}\" \\;","title":"Resize all images in a dir tree, convert them to jpg and output them to a different folder"},{"location":"sips/#see-also","text":"exiftool graphicsmagick imagemagick jpeginfo","title":"See Also"},{"location":"smartctl/","text":"Linux interface to SMART data for hard disks. Examples Show identifying information about a device smartctl -i /dev/sda Show drive attributes This shows a bunch of recorded information that is updated over the life of the drive. smartctl -A /dev/sda Show the same information with better output showing vendor flags smartctl -A -f brief /dev/sda Show all data smartctl -x /dev/sda See Also https://www.backblaze.com/blog/what-smart-stats-indicate-hard-drive-failures/","title":"smartctl"},{"location":"smartctl/#examples","text":"","title":"Examples"},{"location":"smartctl/#show-identifying-information-about-a-device","text":"smartctl -i /dev/sda","title":"Show identifying information about a device"},{"location":"smartctl/#show-drive-attributes","text":"This shows a bunch of recorded information that is updated over the life of the drive. smartctl -A /dev/sda Show the same information with better output showing vendor flags smartctl -A -f brief /dev/sda","title":"Show drive attributes"},{"location":"smartctl/#show-all-data","text":"smartctl -x /dev/sda","title":"Show all data"},{"location":"smartctl/#see-also","text":"https://www.backblaze.com/blog/what-smart-stats-indicate-hard-drive-failures/","title":"See Also"},{"location":"smartstack/","text":"SmartStack is an automated service discovery and registration framework . Components Synapse: Announces state of local services Nerve: Query zookeeper for healthy services to connect to, then configures HAProxy Zookeper: Service registry HAProxy: Load balancing Links DockerCon 14: Tomas Doran - Building a smarter application stack Smartstack ( HAProxy + Serf ) Automated service discovery without rewriting apps GetYourGuide's SmartStack handbook","title":"smartstack"},{"location":"smartstack/#components","text":"Synapse: Announces state of local services Nerve: Query zookeeper for healthy services to connect to, then configures HAProxy Zookeper: Service registry HAProxy: Load balancing","title":"Components"},{"location":"smartstack/#links","text":"DockerCon 14: Tomas Doran - Building a smarter application stack Smartstack ( HAProxy + Serf ) Automated service discovery without rewriting apps GetYourGuide's SmartStack handbook","title":"Links"},{"location":"snap/","text":"\"Package any app for every Linux desktop, server, cloud or device, and deliver updates directly.\" - http://snapcraft.io/ A snap is a fancy zip file containing an application together with its dependencies, and a description of how it should safely be run on your system, especially the different ways it should talk to other software. Most importantly snaps are designed to be secure, sandboxed, containerised applications isolated from the underlying system and from other applications. Snaps allow the safe installation of apps from any vendor on mission critical devices and desktops. Links https://www.ubuntu.com/internet-of-things https://developer.ubuntu.com/en/snappy/ http://snapcraft.io/","title":"snap"},{"location":"snap/#links","text":"https://www.ubuntu.com/internet-of-things https://developer.ubuntu.com/en/snappy/ http://snapcraft.io/","title":"Links"},{"location":"snmp/","text":"Simple Network Management Protocol Links Third party MIBs: http://www.plixer.com/support/mib_resources.php How-To: http://www.linuxhomenetworking.com/wiki/index.php/Quick_HOWTO_:_Ch22_:_Monitoring_Server_Performance APC has some snmp tricks specific to those devices. http://support.ipmonitor.com/mibs_byname_A.aspx - Good MIB reference Examples Install snmp utils on redhat / centos yum install net-snmp-utils Show the system description of a host snmpwalk -v 1 -c public 192.168.9.1 SNMPv2-MIB::sysDescr.0 Walk 172.28.111.10 with community string itgwrk snmpwalk -v 1 -c \"public\" 172.28.111.10 IPV6 is different... snmpwalk -v 1 -c public udp6:fe80::a2e:5fff:feba:f586%en0 enterprises Show Network Info snmpwalk -c public 192.168.9.1 1.3.6.1.2.1 Show Airport Upload and Download bytes snmpwalk -c public 192.168.9.1 IF-MIB::ifOutOctets snmpwalk -c public 192.168.9.1 IF-MIB::ifInOctets Show configured IP addresses IPV4 by querying IPV6: snmpwalk -v 1 -c public udp6:fe80::a2e:5fff:feba:f586%en0 ipAdEntAddr","title":"snmp"},{"location":"snmp/#links","text":"Third party MIBs: http://www.plixer.com/support/mib_resources.php How-To: http://www.linuxhomenetworking.com/wiki/index.php/Quick_HOWTO_:_Ch22_:_Monitoring_Server_Performance APC has some snmp tricks specific to those devices. http://support.ipmonitor.com/mibs_byname_A.aspx - Good MIB reference","title":"Links"},{"location":"snmp/#examples","text":"","title":"Examples"},{"location":"snmp/#install-snmp-utils-on-redhat-centos","text":"yum install net-snmp-utils","title":"Install snmp utils on redhat / centos"},{"location":"snmp/#show-the-system-description-of-a-host","text":"snmpwalk -v 1 -c public 192.168.9.1 SNMPv2-MIB::sysDescr.0","title":"Show the system description of a host"},{"location":"snmp/#walk-1722811110-with-community-string-itgwrk","text":"snmpwalk -v 1 -c \"public\" 172.28.111.10 IPV6 is different... snmpwalk -v 1 -c public udp6:fe80::a2e:5fff:feba:f586%en0 enterprises","title":"Walk 172.28.111.10 with community string itgwrk"},{"location":"snmp/#show-network-info","text":"snmpwalk -c public 192.168.9.1 1.3.6.1.2.1","title":"Show Network Info"},{"location":"snmp/#show-airport-upload-and-download-bytes","text":"snmpwalk -c public 192.168.9.1 IF-MIB::ifOutOctets snmpwalk -c public 192.168.9.1 IF-MIB::ifInOctets","title":"Show Airport Upload and Download bytes"},{"location":"snmp/#show-configured-ip-addresses","text":"IPV4 by querying IPV6: snmpwalk -v 1 -c public udp6:fe80::a2e:5fff:feba:f586%en0 ipAdEntAddr","title":"Show configured IP addresses"},{"location":"solo/","text":"solo is a command that uses a per-user loopback IP# and a designated port to ensure that multiple copies of a command are not run. This takes the place of pid files and process tracking, and has the benefit of never leaving around a false positive. It also lets you skip building trap, pid file and process checking into every shell script you write. http://timkay.com/solo","title":"solo"},{"location":"sort/","text":"sort is a command to sort lines of data. GNU Syntax Examples In OS X, this is gsort . Randomly sort a file in place By giving the -o the same output file as the input file we can shuffle in-place without errors. Trying this same thing using a pipe or a redirect will usually cause an empty file. Beware that this will put duplicate lines right next to each other. If you need better file content shuffling use shuf . sort -o foo -R foo Sort by two fields, first dictionary, second reverse numeric # -k defines the sort key as starting position, sort style, ending position # -r is included in the second key to reverse numeric sort gsort -k1d,1 -k2nr,2 Sort IP Addresses by first octet then last octet, showing which fields are sorted ip neigh show | sort -k1,1n -k4,4n -t. --debug Console example: $ ip neigh show | sort -k1,1n -k4,4n -t. --debug sort: using \u2018en_US.UTF-8\u2019 sorting rules 10.0.2.2 dev eth0 lladdr 52:54:00:12:35:02 REACHABLE __ _ ____________________________________________________ 10.0.2.3 dev eth0 lladdr 52:54:00:12:35:03 STALE __ _ ________________________________________________ 192.16.35.10 dev eth1 lladdr 08:00:27:7a:50:42 STALE ___ __ ____________________________________________________ 192.16.35.11 dev eth1 lladdr 08:00:27:56:64:2f STALE ___ __ ____________________________________________________ BSD Syntax Examples GNU sort and BSD sort behave differently, which is mostly lame. Sort by the third column sort -k 3 filename Sort dates by the day This example shows how to sort dates in ISO Year format by date. (EG: 2017-01-19). Assumes use of bash 4 to generate the example dates. # -n for numeric sort # -k3 for column 3 # -t- to use - as a column delimiter for X in {2016..2017}-{01..12..03}-{01..19..06} ; do echo ${X} ; done | sort -n -k3 -t- Sort the /etc/passwd by UID Also works on /etc/group file and GID sort -n -t: -k 3 /etc/passwd Scan for airport and sort by columns Print out two rows signifying column numbers, which makes it easier to find which columns you want to sort by, then run a command and sort by column numbers. This assumes you're on macOS. perl -e ' foreach ( 1 .. 9 ) { foreach ( 1 .. 9 ) { print \" \"; } print $_; } print \"\\n\"; foreach ( 1 .. 9 ) { foreach ( 1 .. 9, 0 ) { print $_; } } print \"\\n\";' ; \\ airport --scan | sort -k 1.52,1.54","title":"sort"},{"location":"sort/#gnu-syntax-examples","text":"In OS X, this is gsort .","title":"GNU Syntax Examples"},{"location":"sort/#randomly-sort-a-file-in-place","text":"By giving the -o the same output file as the input file we can shuffle in-place without errors. Trying this same thing using a pipe or a redirect will usually cause an empty file. Beware that this will put duplicate lines right next to each other. If you need better file content shuffling use shuf . sort -o foo -R foo","title":"Randomly sort a file in place"},{"location":"sort/#sort-by-two-fields-first-dictionary-second-reverse-numeric","text":"# -k defines the sort key as starting position, sort style, ending position # -r is included in the second key to reverse numeric sort gsort -k1d,1 -k2nr,2","title":"Sort by two fields, first dictionary, second reverse numeric"},{"location":"sort/#sort-ip-addresses-by-first-octet-then-last-octet-showing-which-fields-are-sorted","text":"ip neigh show | sort -k1,1n -k4,4n -t. --debug Console example: $ ip neigh show | sort -k1,1n -k4,4n -t. --debug sort: using \u2018en_US.UTF-8\u2019 sorting rules 10.0.2.2 dev eth0 lladdr 52:54:00:12:35:02 REACHABLE __ _ ____________________________________________________ 10.0.2.3 dev eth0 lladdr 52:54:00:12:35:03 STALE __ _ ________________________________________________ 192.16.35.10 dev eth1 lladdr 08:00:27:7a:50:42 STALE ___ __ ____________________________________________________ 192.16.35.11 dev eth1 lladdr 08:00:27:56:64:2f STALE ___ __ ____________________________________________________","title":"Sort IP Addresses by first octet then last octet, showing which fields are sorted"},{"location":"sort/#bsd-syntax-examples","text":"GNU sort and BSD sort behave differently, which is mostly lame.","title":"BSD Syntax Examples"},{"location":"sort/#sort-by-the-third-column","text":"sort -k 3 filename","title":"Sort by the third column"},{"location":"sort/#sort-dates-by-the-day","text":"This example shows how to sort dates in ISO Year format by date. (EG: 2017-01-19). Assumes use of bash 4 to generate the example dates. # -n for numeric sort # -k3 for column 3 # -t- to use - as a column delimiter for X in {2016..2017}-{01..12..03}-{01..19..06} ; do echo ${X} ; done | sort -n -k3 -t-","title":"Sort dates by the day"},{"location":"sort/#sort-the-etcpasswd-by-uid","text":"Also works on /etc/group file and GID sort -n -t: -k 3 /etc/passwd","title":"Sort the /etc/passwd by UID"},{"location":"sort/#scan-for-airport-and-sort-by-columns","text":"Print out two rows signifying column numbers, which makes it easier to find which columns you want to sort by, then run a command and sort by column numbers. This assumes you're on macOS. perl -e ' foreach ( 1 .. 9 ) { foreach ( 1 .. 9 ) { print \" \"; } print $_; } print \"\\n\"; foreach ( 1 .. 9 ) { foreach ( 1 .. 9, 0 ) { print $_; } } print \"\\n\";' ; \\ airport --scan | sort -k 1.52,1.54","title":"Scan for airport and sort by columns"},{"location":"sphinx/","text":"Sphinx is how many opensource projects generate sites for their documentation. - http://sphinx-doc.org/contents.html","title":"sphinx"},{"location":"split/","text":"\"split - split a file into pieces\" - man split split is a common unix command. Usage Examples Split into DVD sized chunks This example isn't practical, the size needs to be smaller than specified because of DVD filesystem overhead, so you should use 4700000000 or similar if you want to actually burn the data. split -b 4707319808 source Split a big file and compress it with gzip before writing to disk This is useful for splitting up large uncompressed logs. This command is background safe. split -a4 --additional-suffix='-redis.log.1' -l500000 --filter='gzip -9 > $FILE.gz' redis.log.1 And if you want to see read stats, throw pv in the mix: $ split -a4 --additional-suffix='-redis.log.1' -l500000 --filter='pv | gzip -9 > $FILE.gz' redis.log.1 1.94GB 0:00:58 [21.2MB/s] [ <=> ]","title":"split"},{"location":"split/#usage-examples","text":"","title":"Usage Examples"},{"location":"split/#split-into-dvd-sized-chunks","text":"This example isn't practical, the size needs to be smaller than specified because of DVD filesystem overhead, so you should use 4700000000 or similar if you want to actually burn the data. split -b 4707319808 source","title":"Split into DVD sized chunks"},{"location":"split/#split-a-big-file-and-compress-it-with-gzip-before-writing-to-disk","text":"This is useful for splitting up large uncompressed logs. This command is background safe. split -a4 --additional-suffix='-redis.log.1' -l500000 --filter='gzip -9 > $FILE.gz' redis.log.1 And if you want to see read stats, throw pv in the mix: $ split -a4 --additional-suffix='-redis.log.1' -l500000 --filter='pv | gzip -9 > $FILE.gz' redis.log.1 1.94GB 0:00:58 [21.2MB/s] [ <=> ]","title":"Split a big file and compress it with gzip before writing to disk"},{"location":"splunk/","text":"Enterprise log consumption and analysis. http://www.splunk.com/ http://docs.splunk.com/Documentation/Splunk/latest/SearchReference/Sort Examples Get a list of indexes | REST /services/data/indexes | dedup title | table title Get a list of sourcetypes | metadata type=sourcetypes index=* OR index=_* CFEngine runs per hour by version source=\"/var/log/messages\" OR source=\"udp:514\" \"Starting CFEngine\" earliest=\"-1w\" | rex \"Starting CFEngine (?<version>3.[0-9]+.[0-9]+).*\" | timechart span=4h usenull=0 dc(host) by version Strip domain from hostname for consistent UQDNs rex mode=sed field=host \"s/\\.foo\\.example\\.com//\" Count of records per hour by host earliest=\"-7d\" | timechart span=1h count(_raw) by host Count of records per source by 5m with no limit on list earliest=\"-8h\" | timechart span=5m count(_raw) by source limit=0 Count of records per source with a given list earliest=\"-1d\" source=\"/var/log/messages\" OR source=\"udp:10514\" OR source=\"udp:514\" | timechart count by source Count of records per splunk server for a given time period Stupidly, splunk doesn't support ISO date format by default (in the version I'm using). earliest=\"06/19/2015:3:0:0\" latest=\"06/19/2015:3:3:0\" | timechart count(_raw) by splunk_server Order number of hits for a given string by an extracted IP address earliest=\"7/6/2015:9:30:0\" \"verifying pingback from\" | rex \"verifying pingback from (?<pingback_source_ip>[0-9\\.]*)\\\"\" | stats count(_raw) as pingback_source_ip_total by pingback_source_ip | sort pingback_source_ip_total desc Order an RPM report Given a report where RPM fields are exported as field=\"value\", such as: rpm -qa --queryformat 'report=\"rpm\", name=\"%{NAME}\", release=\"%{RELEASE}\", version=\"%{VERSION}\", packager=\"%{PACKAGER}\", url=\"%{URL}\", installtime=\"%{INSTALLTIME}\"\\n' This search in splunk will show a useful table: earliest=\"-1d\" report=\"rpm\" | dedup name | eval install_timestamp = strftime(installtime, \"%F %T.%3N\") | sort installtime desc | table host,name,version,release,install_timestamp See also: http://docs.splunk.com/Documentation/Splunk/latest/Admin/Propsconf http://answers.splunk.com/answers/140493/timestamp-contain-t-between-date-and-time.html#answer-140495 Count of kernel versions Assuming you have a report that sends kernel_version=$(uname -r) : kernel_version | stats count(kernel_version) by kernel_version, host","title":"splunk"},{"location":"splunk/#examples","text":"","title":"Examples"},{"location":"splunk/#get-a-list-of-indexes","text":"| REST /services/data/indexes | dedup title | table title","title":"Get a list of indexes"},{"location":"splunk/#get-a-list-of-sourcetypes","text":"| metadata type=sourcetypes index=* OR index=_*","title":"Get a list of sourcetypes"},{"location":"splunk/#cfengine-runs-per-hour-by-version","text":"source=\"/var/log/messages\" OR source=\"udp:514\" \"Starting CFEngine\" earliest=\"-1w\" | rex \"Starting CFEngine (?<version>3.[0-9]+.[0-9]+).*\" | timechart span=4h usenull=0 dc(host) by version","title":"CFEngine runs per hour by version"},{"location":"splunk/#strip-domain-from-hostname-for-consistent-uqdns","text":"rex mode=sed field=host \"s/\\.foo\\.example\\.com//\"","title":"Strip domain from hostname for consistent UQDNs"},{"location":"splunk/#count-of-records-per-hour-by-host","text":"earliest=\"-7d\" | timechart span=1h count(_raw) by host","title":"Count of records per hour by host"},{"location":"splunk/#count-of-records-per-source-by-5m-with-no-limit-on-list","text":"earliest=\"-8h\" | timechart span=5m count(_raw) by source limit=0","title":"Count of records per source by 5m with no limit on list"},{"location":"splunk/#count-of-records-per-source-with-a-given-list","text":"earliest=\"-1d\" source=\"/var/log/messages\" OR source=\"udp:10514\" OR source=\"udp:514\" | timechart count by source","title":"Count of records per source with a given list"},{"location":"splunk/#count-of-records-per-splunk-server-for-a-given-time-period","text":"Stupidly, splunk doesn't support ISO date format by default (in the version I'm using). earliest=\"06/19/2015:3:0:0\" latest=\"06/19/2015:3:3:0\" | timechart count(_raw) by splunk_server","title":"Count of records per splunk server for a given time period"},{"location":"splunk/#order-number-of-hits-for-a-given-string-by-an-extracted-ip-address","text":"earliest=\"7/6/2015:9:30:0\" \"verifying pingback from\" | rex \"verifying pingback from (?<pingback_source_ip>[0-9\\.]*)\\\"\" | stats count(_raw) as pingback_source_ip_total by pingback_source_ip | sort pingback_source_ip_total desc","title":"Order number of hits for a given string by an extracted IP address"},{"location":"splunk/#order-an-rpm-report","text":"Given a report where RPM fields are exported as field=\"value\", such as: rpm -qa --queryformat 'report=\"rpm\", name=\"%{NAME}\", release=\"%{RELEASE}\", version=\"%{VERSION}\", packager=\"%{PACKAGER}\", url=\"%{URL}\", installtime=\"%{INSTALLTIME}\"\\n' This search in splunk will show a useful table: earliest=\"-1d\" report=\"rpm\" | dedup name | eval install_timestamp = strftime(installtime, \"%F %T.%3N\") | sort installtime desc | table host,name,version,release,install_timestamp See also: http://docs.splunk.com/Documentation/Splunk/latest/Admin/Propsconf http://answers.splunk.com/answers/140493/timestamp-contain-t-between-date-and-time.html#answer-140495","title":"Order an RPM report"},{"location":"splunk/#count-of-kernel-versions","text":"Assuming you have a report that sends kernel_version=$(uname -r) : kernel_version | stats count(kernel_version) by kernel_version, host","title":"Count of kernel versions"},{"location":"sqlite/","text":"Links CLI Shell info: http://www.sqlite.org/sqlite.html Better CLI tutorial: http://souptonuts.sourceforge.net/readme_sqlite_tutorial.html FAQ - http://www.sqlite.org/faq.html When to use SQLite - https://www.sqlite.org/whentouse.html https://www.gaia-gis.it/fossil/libspatialite/index - \"SpatiaLite is an open source library intended to extend the SQLite core to support fully fledged Spatial SQL capabilities.\" Syntax Examples Import a csv file This method does not appear to support ~/filename or $HOME/filename , but does support relative and absolute paths. The sqlite3 help text says that -csv will \"set output mode to 'csv'\", but it also affects .import statements. sqlite3 -csv filename.db \".import path/to/some_file.csv destination_table_name\" Create a table CREATE TABLE servers (id INTEGER NOT NULL, hostname VARCHAR(255), ip_addr VARCHAR(32), PRIMARY KEY (id), UNIQUE (id,hostname)); Or from a unix shell sqlite3 foo.db \"CREATE TABLE servers (id INTEGER NOT NULL, hostname VARCHAR(255), ip_addr VARCHAR(32), PRIMARY KEY (id), UNIQUE (id,hostname));\" Add a column to the table ALTER TABLE servers ADD os varchar(255); Add rows to the table from unix shell sqlite3 foo.db \"insert into baz values ('50','some text');\" Add rows or update if the row already exists This syntax is different from other SQL implementations insert or replace into tablename(filename, hash) values ('/etc/hosts', 'somehash'), ('/etc/resolv.conf', 'someotherhash'); Output rows via the unix shell This outputs as columns, but csv, html line, and list exist too. sqlite3 -column foo.db \"select * from baz limit 5;\" If -column is truncating your output, instead use -list -separator ' ' Dump the db from CLI sqlite3 foo.db \".dump\" > db.sql Search Skype chat history sqlite3 ~/Library/Application\\ Support/Skype/daniel.austin/main.db \"select author, timestamp, body_xml from messages where body_xml like '%veewee%' ;\" Expanded functionality skype history search function skypesearch(){ skypeusername=$1 searchstring=$2 /usr/bin/env sqlite3 ~/Library/Application\\ Support/Skype/${skypeusername}/main.db \"select author, datetime(timestamp,'unixepoch','localtime'), body_xml from messages where body_xml like '%${searchstring}%' ;\" } alias ss=\"skypesearch username\" Quickly create an image database # Create the database sqlite3 images.db \"create table images (filename varchar(255), createdate timestamp, unique(filename))\" # Populate the database. This can be blindly re-run when new files are added. exiftool -d \"%s\" -p 'insert into images values (\"$filename\", \"$DateTimeOriginal\");' -q -f -r . | sqlite3 images.db 2> /dev/null # Query the database sqlite3 images.db \"select filename,datetime(createdate,'unixepoch','localtime') as date from images where date like '2014-08-02%';\" Use sqlite3 in PHP <?php $db = new SQLite3(\"images.db\"); if (!is_object($db)) { echo \"Couldn't connect to DB.\" ; exit ; } $query = \"select filename,datetime(createdate,'unixepoch','localtime') as date from images where date like '2014-08-02%';\" ; $result = $db->query($query); while($row = $result->fetchArray(SQLITE3_ASSOC)){ $i++ ; if(!isset($row['filename'])) continue; echo \"$row[filename] : $row[date]&lt;br>\\n\"; } ?>","title":"sqlite"},{"location":"sqlite/#links","text":"CLI Shell info: http://www.sqlite.org/sqlite.html Better CLI tutorial: http://souptonuts.sourceforge.net/readme_sqlite_tutorial.html FAQ - http://www.sqlite.org/faq.html When to use SQLite - https://www.sqlite.org/whentouse.html https://www.gaia-gis.it/fossil/libspatialite/index - \"SpatiaLite is an open source library intended to extend the SQLite core to support fully fledged Spatial SQL capabilities.\"","title":"Links"},{"location":"sqlite/#syntax-examples","text":"","title":"Syntax Examples"},{"location":"sqlite/#import-a-csv-file","text":"This method does not appear to support ~/filename or $HOME/filename , but does support relative and absolute paths. The sqlite3 help text says that -csv will \"set output mode to 'csv'\", but it also affects .import statements. sqlite3 -csv filename.db \".import path/to/some_file.csv destination_table_name\"","title":"Import a csv file"},{"location":"sqlite/#create-a-table","text":"CREATE TABLE servers (id INTEGER NOT NULL, hostname VARCHAR(255), ip_addr VARCHAR(32), PRIMARY KEY (id), UNIQUE (id,hostname)); Or from a unix shell sqlite3 foo.db \"CREATE TABLE servers (id INTEGER NOT NULL, hostname VARCHAR(255), ip_addr VARCHAR(32), PRIMARY KEY (id), UNIQUE (id,hostname));\"","title":"Create a table"},{"location":"sqlite/#add-a-column-to-the-table","text":"ALTER TABLE servers ADD os varchar(255);","title":"Add a column to the table"},{"location":"sqlite/#add-rows-to-the-table-from-unix-shell","text":"sqlite3 foo.db \"insert into baz values ('50','some text');\"","title":"Add rows to the table from unix shell"},{"location":"sqlite/#add-rows-or-update-if-the-row-already-exists","text":"This syntax is different from other SQL implementations insert or replace into tablename(filename, hash) values ('/etc/hosts', 'somehash'), ('/etc/resolv.conf', 'someotherhash');","title":"Add rows or update if the row already exists"},{"location":"sqlite/#output-rows-via-the-unix-shell","text":"This outputs as columns, but csv, html line, and list exist too. sqlite3 -column foo.db \"select * from baz limit 5;\" If -column is truncating your output, instead use -list -separator ' '","title":"Output rows via the unix shell"},{"location":"sqlite/#dump-the-db-from-cli","text":"sqlite3 foo.db \".dump\" > db.sql","title":"Dump the db from CLI"},{"location":"sqlite/#search-skype-chat-history","text":"sqlite3 ~/Library/Application\\ Support/Skype/daniel.austin/main.db \"select author, timestamp, body_xml from messages where body_xml like '%veewee%' ;\"","title":"Search Skype chat history"},{"location":"sqlite/#expanded-functionality-skype-history-search","text":"function skypesearch(){ skypeusername=$1 searchstring=$2 /usr/bin/env sqlite3 ~/Library/Application\\ Support/Skype/${skypeusername}/main.db \"select author, datetime(timestamp,'unixepoch','localtime'), body_xml from messages where body_xml like '%${searchstring}%' ;\" } alias ss=\"skypesearch username\"","title":"Expanded functionality skype history search"},{"location":"sqlite/#quickly-create-an-image-database","text":"# Create the database sqlite3 images.db \"create table images (filename varchar(255), createdate timestamp, unique(filename))\" # Populate the database. This can be blindly re-run when new files are added. exiftool -d \"%s\" -p 'insert into images values (\"$filename\", \"$DateTimeOriginal\");' -q -f -r . | sqlite3 images.db 2> /dev/null # Query the database sqlite3 images.db \"select filename,datetime(createdate,'unixepoch','localtime') as date from images where date like '2014-08-02%';\"","title":"Quickly create an image database"},{"location":"sqlite/#use-sqlite3-in-php","text":"<?php $db = new SQLite3(\"images.db\"); if (!is_object($db)) { echo \"Couldn't connect to DB.\" ; exit ; } $query = \"select filename,datetime(createdate,'unixepoch','localtime') as date from images where date like '2014-08-02%';\" ; $result = $db->query($query); while($row = $result->fetchArray(SQLITE3_ASSOC)){ $i++ ; if(!isset($row['filename'])) continue; echo \"$row[filename] : $row[date]&lt;br>\\n\"; } ?>","title":"Use sqlite3 in PHP"},{"location":"ss/","text":"\"ss - another utility to investigate sockets\" - man ss This tool shows all sockets, not just networking sockets. Examples Options can be concatenated, so ss -t -n -l -p can be ss -tnlp Show all established connections, don't resolve service names ss -n Show all listening sockets ss -l Display all TCP sockets ss -t -a Show ipv4 listening sockets sorted by port ss -4 -ltn | sort -k2 -t: -n Show ssh connections ss -at '( sport = :ssh or dport = :ssh )' Show ipv4 sockets in a particular state ss -t4 state time-wait Show the processes for listening ipv4 sockets ss -lt4p","title":"ss"},{"location":"ss/#examples","text":"Options can be concatenated, so ss -t -n -l -p can be ss -tnlp","title":"Examples"},{"location":"ss/#show-all-established-connections-dont-resolve-service-names","text":"ss -n","title":"Show all established connections, don't resolve service names"},{"location":"ss/#show-all-listening-sockets","text":"ss -l","title":"Show all listening sockets"},{"location":"ss/#display-all-tcp-sockets","text":"ss -t -a","title":"Display all TCP sockets"},{"location":"ss/#show-ipv4-listening-sockets-sorted-by-port","text":"ss -4 -ltn | sort -k2 -t: -n","title":"Show ipv4 listening sockets sorted by port"},{"location":"ss/#show-ssh-connections","text":"ss -at '( sport = :ssh or dport = :ssh )'","title":"Show ssh connections"},{"location":"ss/#show-ipv4-sockets-in-a-particular-state","text":"ss -t4 state time-wait","title":"Show ipv4 sockets in a particular state"},{"location":"ss/#show-the-processes-for-listening-ipv4-sockets","text":"ss -lt4p","title":"Show the processes for listening ipv4 sockets"},{"location":"ssh/","text":"ssh is the secure shell, an encrypted version of telnet and a whole lot more ssh The secure shell itself, very useful for administering remote systems, tunneling arbitrary ports, tunneling X sessions, and a whole lot more. scp scp is like cp, but it happens securely and allows host-to-host transfers over ssh. Very handy when used with ssh_config and key-based authentication . sftp A secure FTP client built into ssh. The native client sucks, try lftp or rsync if it's available. sshd Output effective server configuration variables This is useful for troubleshooting ssh_config matching. sshd -T # requires root ssh Output effective client configuration variables ssh -G user@host tunnel local port to the destination through the SSH connection This will only listen on localhost, not ethernet interfaces. Use -g to listen on all interfaces ssh -L LocalPort:Destination:DestinationPort Tunnel from remote port through the local machine to destination ssh -R RemotePort:Destination:DestinationPort Create a socks 5 proxy on a local port ssh -D [PORT] user@host Loop through some ssh hosts and execute a command -n is required in order to proceed past the first host. cat hostnames.txt | while read -r host ; do ssh -o ConnectTimeout=10 -o PasswordAuthentication=no -n \"$host\" 'some_command ; another_command' done ssh_config The user ssh config file, ~/.ssh/config , lets you override default options. This makes it handy for command line stuff where the syntax is funky such as using non-standard ports. Notably, global vriables need to come at the end of the file, not the beginning! Simple host aliasing The following example will let you simply ssh sugarbeast to log in on the non-standard port on the proper IP# with the specified user. Host sugarbeast HostName 66.134.66.42 User daniel Port 888 Multiplexed connections After running mkdir -p -m 700 ~/.ssh/sockets add this to your ~/.ssh/config Host * ControlPersist yes ControlMaster auto ControlPath ~/.ssh/sockets/%r@%h:%p To kill a multiplexed connection, run ssh -O exit user@host ProxyCommand This command lets you execute an arbitrary series of commands to connect with. SSH proxy through ssh host for openSSH v4 and earlier (Ubuntu 8): ProxyCommand ssh -q bastion nc -q 0 %h %p SSH proxy through ssh host for openSSH v5 and later: ProxyCommand ssh -W %h:%p bastion HTTP proxy (from man ssh_config ): ProxyCommand nc -X connect -x 192.0.2.0:8080 %h %p key-based authentication Key-based authentication lets you log in without specifying a password. This is useful for rsync , scp and just plain old ssh shell. Adding comments to the public key makes it easy to sort through the keys that authorized_keys file. The $HOME/.ssh/authorized_keys file is the default list of public keys which are allowed password-less login. See also man authorized_keys for more info. Key-based auth Permissions Permissions on this file need to be set like this: #!/bin/sh # This will repair permissions for the current user's ssh key-pair authentication. mkdir ~/.ssh/ touch ~/.ssh/authorized_keys chmod go-w ~ && \\ chmod 700 ~/.ssh && \\ chmod 600 ~/.ssh/* && \\ echo \"Successfully fixed ssh authentication files permissions.\" ssh-keygen Validate each entry of authorized_keys ssh-keygen -lvf ~/.ssh/authorized_keys Generate Keys Not all systems support ed25519, but it is currently the most secure key type. ssh-keygen -t ssh-ed25519 -c \"Daniel Hoherd <danielhoherd@xyzhost> If you require backwards compatibility, use 4096 bit RSA keys. ssh-keygen -b 4096 -t rsa -c \"Daniel Hoherd <danielhoherd@xyzhost>\" Create or change a password for an ssh identity This will update the password used to unlock an ssh identity. ssh-keygen -p -f ~/.ssh/id_ed25519 Limit root login to key based auth In /etc/ssh/sshd_config: PermitRootLogin without-password See Also sshuttle - IP network router over ssh sslh - lets one accept both HTTPS and SSH connections on the same port. It makes it possible to connect to an SSH server on port 443 (e.g. from inside a corporate firewall) Corkscrew - a tool for tunneling SSH through HTTP proxies Putty - An SSH (and telnet) client for windows. Passwordless SSH logins SSH server for Windows SSH jump hosts The Secure Shell (SSH) Connection Protocol - https://www.ietf.org/rfc/rfc4254.txt The Secure Shell (SSH) Authentication Protocol - https://www.ietf.org/rfc/rfc4252.txt The Secure Shell (SSH) Transport Layer Protocol - https://www.ietf.org/rfc/rfc4253.txt","title":"ssh"},{"location":"ssh/#ssh","text":"The secure shell itself, very useful for administering remote systems, tunneling arbitrary ports, tunneling X sessions, and a whole lot more.","title":"ssh"},{"location":"ssh/#scp","text":"scp is like cp, but it happens securely and allows host-to-host transfers over ssh. Very handy when used with ssh_config and key-based authentication .","title":"scp"},{"location":"ssh/#sftp","text":"A secure FTP client built into ssh. The native client sucks, try lftp or rsync if it's available.","title":"sftp"},{"location":"ssh/#sshd","text":"","title":"sshd"},{"location":"ssh/#output-effective-server-configuration-variables","text":"This is useful for troubleshooting ssh_config matching. sshd -T # requires root","title":"Output effective server configuration variables"},{"location":"ssh/#ssh_1","text":"","title":"ssh"},{"location":"ssh/#output-effective-client-configuration-variables","text":"ssh -G user@host","title":"Output effective client configuration variables"},{"location":"ssh/#tunnel-local-port-to-the-destination-through-the-ssh-connection","text":"This will only listen on localhost, not ethernet interfaces. Use -g to listen on all interfaces ssh -L LocalPort:Destination:DestinationPort","title":"tunnel local port to the destination through the SSH connection"},{"location":"ssh/#tunnel-from-remote-port-through-the-local-machine-to-destination","text":"ssh -R RemotePort:Destination:DestinationPort","title":"Tunnel from remote port through the local machine to destination"},{"location":"ssh/#create-a-socks-5-proxy-on-a-local-port","text":"ssh -D [PORT] user@host","title":"Create a socks 5 proxy on a local port"},{"location":"ssh/#loop-through-some-ssh-hosts-and-execute-a-command","text":"-n is required in order to proceed past the first host. cat hostnames.txt | while read -r host ; do ssh -o ConnectTimeout=10 -o PasswordAuthentication=no -n \"$host\" 'some_command ; another_command' done","title":"Loop through some ssh hosts and execute a command"},{"location":"ssh/#ssh_config","text":"The user ssh config file, ~/.ssh/config , lets you override default options. This makes it handy for command line stuff where the syntax is funky such as using non-standard ports. Notably, global vriables need to come at the end of the file, not the beginning!","title":"ssh_config"},{"location":"ssh/#simple-host-aliasing","text":"The following example will let you simply ssh sugarbeast to log in on the non-standard port on the proper IP# with the specified user. Host sugarbeast HostName 66.134.66.42 User daniel Port 888","title":"Simple host aliasing"},{"location":"ssh/#multiplexed-connections","text":"After running mkdir -p -m 700 ~/.ssh/sockets add this to your ~/.ssh/config Host * ControlPersist yes ControlMaster auto ControlPath ~/.ssh/sockets/%r@%h:%p To kill a multiplexed connection, run ssh -O exit user@host","title":"Multiplexed connections"},{"location":"ssh/#proxycommand","text":"This command lets you execute an arbitrary series of commands to connect with. SSH proxy through ssh host for openSSH v4 and earlier (Ubuntu 8): ProxyCommand ssh -q bastion nc -q 0 %h %p SSH proxy through ssh host for openSSH v5 and later: ProxyCommand ssh -W %h:%p bastion HTTP proxy (from man ssh_config ): ProxyCommand nc -X connect -x 192.0.2.0:8080 %h %p","title":"ProxyCommand"},{"location":"ssh/#key-based-authentication","text":"Key-based authentication lets you log in without specifying a password. This is useful for rsync , scp and just plain old ssh shell. Adding comments to the public key makes it easy to sort through the keys that authorized_keys file. The $HOME/.ssh/authorized_keys file is the default list of public keys which are allowed password-less login. See also man authorized_keys for more info.","title":"key-based authentication"},{"location":"ssh/#key-based-auth-permissions","text":"Permissions on this file need to be set like this: #!/bin/sh # This will repair permissions for the current user's ssh key-pair authentication. mkdir ~/.ssh/ touch ~/.ssh/authorized_keys chmod go-w ~ && \\ chmod 700 ~/.ssh && \\ chmod 600 ~/.ssh/* && \\ echo \"Successfully fixed ssh authentication files permissions.\"","title":"Key-based auth Permissions"},{"location":"ssh/#ssh-keygen","text":"","title":"ssh-keygen"},{"location":"ssh/#validate-each-entry-of-authorized_keys","text":"ssh-keygen -lvf ~/.ssh/authorized_keys","title":"Validate each entry of authorized_keys"},{"location":"ssh/#generate-keys","text":"Not all systems support ed25519, but it is currently the most secure key type. ssh-keygen -t ssh-ed25519 -c \"Daniel Hoherd <danielhoherd@xyzhost> If you require backwards compatibility, use 4096 bit RSA keys. ssh-keygen -b 4096 -t rsa -c \"Daniel Hoherd <danielhoherd@xyzhost>\"","title":"Generate Keys"},{"location":"ssh/#create-or-change-a-password-for-an-ssh-identity","text":"This will update the password used to unlock an ssh identity. ssh-keygen -p -f ~/.ssh/id_ed25519","title":"Create or change a password for an ssh identity"},{"location":"ssh/#limit-root-login-to-key-based-auth","text":"In /etc/ssh/sshd_config: PermitRootLogin without-password","title":"Limit root login to key based auth"},{"location":"ssh/#see-also","text":"sshuttle - IP network router over ssh sslh - lets one accept both HTTPS and SSH connections on the same port. It makes it possible to connect to an SSH server on port 443 (e.g. from inside a corporate firewall) Corkscrew - a tool for tunneling SSH through HTTP proxies Putty - An SSH (and telnet) client for windows. Passwordless SSH logins SSH server for Windows SSH jump hosts The Secure Shell (SSH) Connection Protocol - https://www.ietf.org/rfc/rfc4254.txt The Secure Shell (SSH) Authentication Protocol - https://www.ietf.org/rfc/rfc4252.txt The Secure Shell (SSH) Transport Layer Protocol - https://www.ietf.org/rfc/rfc4253.txt","title":"See Also"},{"location":"sshuttle/","text":"sshuttle is an SSH powered ipv4 routed VPN that doesn't require admin rights on the target host. - https://github.com/apenwarr/sshuttle Usage Tunnel DNS queries and create a route through the given host to the given subnet, and be verbose about it. sshuttle --dns -vr user@host:port 192.168.1.0/24","title":"sshuttle"},{"location":"sshuttle/#usage","text":"Tunnel DNS queries and create a route through the given host to the given subnet, and be verbose about it. sshuttle --dns -vr user@host:port 192.168.1.0/24","title":"Usage"},{"location":"stat/","text":"show filesystem metadata about a file GNU stat examples Show permissions, modify date, ownership and long filename stat -c \"%a/%A %y %G(%g):%U(%u) %N\" /srv/log/apache2/ Sum file sizes stat -c '%s' *2016* | awk '{sum += $1} END {print sum}' GNU stat -c variables The valid format sequences for files (without --filesystem): %a - Access rights in octal %A - Access rights in human readable form %b - Number of blocks allocated (see %B) %B - The size in bytes of each block reported by %b %C - SELinux security context string %d - Device number in decimal %D - Device number in hex %f - Raw mode in hex %F - File type %g - Group ID of owner %G - Group name of owner %h - Number of hard links %i - Inode number %n - File name %N - Quoted file name with dereference if symbolic link %o - I/O block size %s - Total size, in bytes %t - Major device type in hex %T - Minor device type in hex %u - User ID of owner %U - User name of owner %x - Time of last access %X - Time of last access as seconds since Epoch %y - Time of last modification %Y - Time of last modification as seconds since Epoch %z - Time of last change %Z - Time of last change as seconds since Epoch Valid format sequences for file systems: %a - Free blocks available to non-superuser %b - Total data blocks in file system %c - Total file nodes in file system %d - Free file nodes in file system %f - Free blocks in file system %C - SELinux security context string %i - File System ID in hex %l - Maximum length of filenames %n - File name %s - Block size (for faster transfers) %S - Fundamental block size (for block counts) %t - Type in hex %T - Type in human readable form","title":"stat"},{"location":"stat/#gnu-stat-examples","text":"","title":"GNU stat examples"},{"location":"stat/#show-permissions-modify-date-ownership-and-long-filename","text":"stat -c \"%a/%A %y %G(%g):%U(%u) %N\" /srv/log/apache2/","title":"Show permissions, modify date, ownership and long filename"},{"location":"stat/#sum-file-sizes","text":"stat -c '%s' *2016* | awk '{sum += $1} END {print sum}'","title":"Sum file sizes"},{"location":"stat/#gnu-stat-c-variables","text":"The valid format sequences for files (without --filesystem): %a - Access rights in octal %A - Access rights in human readable form %b - Number of blocks allocated (see %B) %B - The size in bytes of each block reported by %b %C - SELinux security context string %d - Device number in decimal %D - Device number in hex %f - Raw mode in hex %F - File type %g - Group ID of owner %G - Group name of owner %h - Number of hard links %i - Inode number %n - File name %N - Quoted file name with dereference if symbolic link %o - I/O block size %s - Total size, in bytes %t - Major device type in hex %T - Minor device type in hex %u - User ID of owner %U - User name of owner %x - Time of last access %X - Time of last access as seconds since Epoch %y - Time of last modification %Y - Time of last modification as seconds since Epoch %z - Time of last change %Z - Time of last change as seconds since Epoch Valid format sequences for file systems: %a - Free blocks available to non-superuser %b - Total data blocks in file system %c - Total file nodes in file system %d - Free file nodes in file system %f - Free blocks in file system %C - SELinux security context string %i - File System ID in hex %l - Maximum length of filenames %n - File name %s - Block size (for faster transfers) %S - Fundamental block size (for block counts) %t - Type in hex %T - Type in human readable form","title":"GNU stat -c variables"},{"location":"strace/","text":"strace is a tool to trace system calls and signals in Linux. Examples Trace a running process strace -p 5789 Trace only exec calls of a command and all child processes strace -f -eexecve cf-agent -K","title":"strace"},{"location":"strace/#examples","text":"","title":"Examples"},{"location":"strace/#trace-a-running-process","text":"strace -p 5789","title":"Trace a running process"},{"location":"strace/#trace-only-exec-calls-of-a-command-and-all-child-processes","text":"strace -f -eexecve cf-agent -K","title":"Trace only exec calls of a command and all child processes"},{"location":"sudo/","text":"super user do Examples #includedir /etc/sudoers.d #includedir /etc/sudoers.d This line is in some sudoers files, and is not a comment. #includedir is a configuration directive. This directory does not like to have .conf files, so they should be flatly named. eg: 00_default Check sudoers for errors visudo -c Show sudo rules that match for the given user sudo -l -U username The order shown is important. sudo picks the last matching rule. Rules are parsed in order from /etc/sudoers and all included files. Because of this, #includedir /etc/sudoers.d should be the last line in the /etc/sudoers file, and the order of the /etc/sudoers.d/ files will be important when fine tuning rules. Allow admins to run mtr without a password Add the following line to /etc/sudoers %admin ALL=(ALL)NOPASSWD:/usr/local/sbin/mtr Allow several groups and users to execute upstart commands User_Alias UPSTART_USERS = %wheel, %sysadmin, %adm UPSTART_USERS ALL=(ALL) NOPASSWD: /sbin/start UPSTART_USERS ALL=(ALL) NOPASSWD: /sbin/stop UPSTART_USERS ALL=(ALL) NOPASSWD: /sbin/initctl UPSTART_USERS ALL=(ALL) NOPASSWD: /sbin/restart Run several commands with one sudo command sudo -s -- <<EOF whoami whoami EOF or sudo bash -c \"whoami ; whoami ;\" Links sudoers config - http://ubuntuforums.org/showthread.php?t=1132821","title":"sudo"},{"location":"sudo/#examples","text":"","title":"Examples"},{"location":"sudo/#includedir-etcsudoersd","text":"#includedir /etc/sudoers.d This line is in some sudoers files, and is not a comment. #includedir is a configuration directive. This directory does not like to have .conf files, so they should be flatly named. eg: 00_default","title":"#includedir /etc/sudoers.d"},{"location":"sudo/#check-sudoers-for-errors","text":"visudo -c","title":"Check sudoers for errors"},{"location":"sudo/#show-sudo-rules-that-match-for-the-given-user","text":"sudo -l -U username The order shown is important. sudo picks the last matching rule. Rules are parsed in order from /etc/sudoers and all included files. Because of this, #includedir /etc/sudoers.d should be the last line in the /etc/sudoers file, and the order of the /etc/sudoers.d/ files will be important when fine tuning rules.","title":"Show sudo rules that match for the given user"},{"location":"sudo/#allow-admins-to-run-mtr-without-a-password","text":"Add the following line to /etc/sudoers %admin ALL=(ALL)NOPASSWD:/usr/local/sbin/mtr","title":"Allow admins to run mtr without a password"},{"location":"sudo/#allow-several-groups-and-users-to-execute-upstart-commands","text":"User_Alias UPSTART_USERS = %wheel, %sysadmin, %adm UPSTART_USERS ALL=(ALL) NOPASSWD: /sbin/start UPSTART_USERS ALL=(ALL) NOPASSWD: /sbin/stop UPSTART_USERS ALL=(ALL) NOPASSWD: /sbin/initctl UPSTART_USERS ALL=(ALL) NOPASSWD: /sbin/restart","title":"Allow several groups and users to execute upstart commands"},{"location":"sudo/#run-several-commands-with-one-sudo-command","text":"sudo -s -- <<EOF whoami whoami EOF or sudo bash -c \"whoami ; whoami ;\"","title":"Run several commands with one sudo command"},{"location":"sudo/#links","text":"sudoers config - http://ubuntuforums.org/showthread.php?t=1132821","title":"Links"},{"location":"swagger/","text":"\"Swagger is the world\u2019s largest framework of API developer tools for the OpenAPI Specification(OAS), enabling development across the entire API lifecycle, from design and documentation, to test and deployment.\" - https://swagger.io/ Links Hello World with Swagger - https://swagger.io/blog/getting-started-with-swagger-i-what-is-swagger/ Swagger 101 - https://app.swaggerhub.com/help/tutorials/writing-swagger-definitions Online editor with example app definition https://editor.swagger.io/ https://swagger.io/tools/","title":"swagger"},{"location":"swagger/#links","text":"Hello World with Swagger - https://swagger.io/blog/getting-started-with-swagger-i-what-is-swagger/ Swagger 101 - https://app.swaggerhub.com/help/tutorials/writing-swagger-definitions Online editor with example app definition https://editor.swagger.io/ https://swagger.io/tools/","title":"Links"},{"location":"swift/","text":"\"Swift is a powerful and intuitive programming language for macOS, iOS, watchOS and tvOS. Writing Swift code is interactive and fun, the syntax is concise yet expressive, and Swift includes modern features developers love. Swift code is safe by design, yet also produces software that runs lightning-fast.\" https://developer.apple.com/swift/ https://www.appcoda.com/learnswift/","title":"Swift"},{"location":"sysctl/","text":"\"sysctl - configure kernel parameters at runtime\" - man sysctl /etc/sysctl.conf is for storing permanent changes, sysctl is used for making changes to the running system. sysctl command examples Show all kernel variables for the in-memory kernel sysctl -a Assign a new variable for the running kernel to use sysctl -w variable=value Load values from /etc/sysctl.conf sysctl -p sysctl.conf examples Reboot after 10 seconds if kernel panics kernel.panic = 10 Treat all oopses as panics kernel.panic_on_oops = 1","title":"sysctl"},{"location":"sysctl/#sysctl-command-examples","text":"","title":"sysctl command examples"},{"location":"sysctl/#show-all-kernel-variables-for-the-in-memory-kernel","text":"sysctl -a","title":"Show all kernel variables for the in-memory kernel"},{"location":"sysctl/#assign-a-new-variable-for-the-running-kernel-to-use","text":"sysctl -w variable=value","title":"Assign a new variable for the running kernel to use"},{"location":"sysctl/#load-values-from-etcsysctlconf","text":"sysctl -p","title":"Load values from /etc/sysctl.conf"},{"location":"sysctl/#sysctlconf-examples","text":"","title":"sysctl.conf examples"},{"location":"sysctl/#reboot-after-10-seconds-if-kernel-panics","text":"kernel.panic = 10","title":"Reboot after 10 seconds if kernel panics"},{"location":"sysctl/#treat-all-oopses-as-panics","text":"kernel.panic_on_oops = 1","title":"Treat all oopses as panics"},{"location":"sysdig/","text":"An awesome host inspection tool, with tcpdump like tool and an interface similar to [[top]] et al. - http://www.sysdig.org/ Installation - CentOS https://github.com/draios/sysdig/wiki/How-to-Install-Sysdig-for-Linux Their shell script installs epel from a 3rd party source, so it's best to use this method instead: rpm --import https://s3.amazonaws.com/download.draios.com/DRAIOS-GPG-KEY.public && \\ curl -s -o /etc/yum.repos.d/draios.repo http://download.draios.com/stable/rpm/draios.repo && \\ yum -y install kernel-devel-$(uname -r) && \\ yum -y install sysdig Examples Simple usage sysdig Write a system trace file sysdig -w tracefile.scap Replay a tracefile sysdig -r tracefile.scap Show filters sysdig -l Show activity for access to a given file sysdig -l fd.name=/etc/hosts Show shell commands for all users sysdig -pc -c spy_users Spy on a user and exclude a process sysdig -pc -c spy_users proc.name!=gmetric Or exclude multiple processes sysdig -pc -c spy_users \"not proc.name in ( gmetric, awk, sed, grep )\" Show a top like interface csysdig Links https://github.com/draios/sysdig/wiki https://github.com/draios/sysdig/wiki/Sysdig%20Examples http://man7.org/linux/man-pages/man8/sysdig.8.html Getting Started With Sysdig Getting Started With Csysdig","title":"sysdig"},{"location":"sysdig/#installation-centos","text":"https://github.com/draios/sysdig/wiki/How-to-Install-Sysdig-for-Linux Their shell script installs epel from a 3rd party source, so it's best to use this method instead: rpm --import https://s3.amazonaws.com/download.draios.com/DRAIOS-GPG-KEY.public && \\ curl -s -o /etc/yum.repos.d/draios.repo http://download.draios.com/stable/rpm/draios.repo && \\ yum -y install kernel-devel-$(uname -r) && \\ yum -y install sysdig","title":"Installation - CentOS"},{"location":"sysdig/#examples","text":"","title":"Examples"},{"location":"sysdig/#simple-usage","text":"sysdig","title":"Simple usage"},{"location":"sysdig/#write-a-system-trace-file","text":"sysdig -w tracefile.scap","title":"Write a system trace file"},{"location":"sysdig/#replay-a-tracefile","text":"sysdig -r tracefile.scap","title":"Replay a tracefile"},{"location":"sysdig/#show-filters","text":"sysdig -l","title":"Show filters"},{"location":"sysdig/#show-activity-for-access-to-a-given-file","text":"sysdig -l fd.name=/etc/hosts","title":"Show activity for access to a given file"},{"location":"sysdig/#show-shell-commands-for-all-users","text":"sysdig -pc -c spy_users","title":"Show shell commands for all users"},{"location":"sysdig/#spy-on-a-user-and-exclude-a-process","text":"sysdig -pc -c spy_users proc.name!=gmetric Or exclude multiple processes sysdig -pc -c spy_users \"not proc.name in ( gmetric, awk, sed, grep )\"","title":"Spy on a user and exclude a process"},{"location":"sysdig/#show-a-top-like-interface","text":"csysdig","title":"Show a top like interface"},{"location":"sysdig/#links","text":"https://github.com/draios/sysdig/wiki https://github.com/draios/sysdig/wiki/Sysdig%20Examples http://man7.org/linux/man-pages/man8/sysdig.8.html Getting Started With Sysdig Getting Started With Csysdig","title":"Links"},{"location":"systemd-resolved/","text":"\"systemd-resolved - Network Name Resolution manager\" - man systemd-resolved systemd-resolved enhances (ie: interferes with) old school simple methods of managing DNS on a linux system. The way it reads the /etc/resolv.conf file as either a file or symlink alters the behavior of the service, which is a new mechanic that can have unexpected effects if you're not familiar with it. Examples Resolve a hostname $ systemd-resolve google.com google.com: 216.58.192.238 -- Information acquired via protocol DNS in 23.9ms. -- Data is authenticated: no Check name resolution statistics $ systemd-resolve --statistics DNSSEC supported by current servers: no Transactions Current Transactions: 0 Total Transactions: 36 Cache Current Cache Size: 2 Cache Hits: 2 Cache Misses: 103 DNSSEC Verdicts Secure: 0 Insecure: 0 Bogus: 0 Indeterminate: 0 See Also http://man7.org/linux/man-pages/man1/systemd-resolve.1.html http://manpages.ubuntu.com/manpages/xenial/man8/systemd-resolved.service.8.html http://manpages.ubuntu.com/manpages/bionic/man8/systemd-resolved.service.8.html","title":"systemd-resolved"},{"location":"systemd-resolved/#examples","text":"","title":"Examples"},{"location":"systemd-resolved/#resolve-a-hostname","text":"$ systemd-resolve google.com google.com: 216.58.192.238 -- Information acquired via protocol DNS in 23.9ms. -- Data is authenticated: no","title":"Resolve a hostname"},{"location":"systemd-resolved/#check-name-resolution-statistics","text":"$ systemd-resolve --statistics DNSSEC supported by current servers: no Transactions Current Transactions: 0 Total Transactions: 36 Cache Current Cache Size: 2 Cache Hits: 2 Cache Misses: 103 DNSSEC Verdicts Secure: 0 Insecure: 0 Bogus: 0 Indeterminate: 0","title":"Check name resolution statistics"},{"location":"systemd-resolved/#see-also","text":"http://man7.org/linux/man-pages/man1/systemd-resolve.1.html http://manpages.ubuntu.com/manpages/xenial/man8/systemd-resolved.service.8.html http://manpages.ubuntu.com/manpages/bionic/man8/systemd-resolved.service.8.html","title":"See Also"},{"location":"systemd/","text":"\"systemd is a system and service manager for Linux, compatible with SysV and LSB init scripts. systemd provides aggressive parallelization capabilities, uses socket and D-Bus activation for starting services, offers on-demand starting of daemons, keeps track of processes using Linux control groups, supports snapshotting and restoring of the system state, maintains mount and automount points and implements an elaborate transactional dependency-based service control logic. It can work as a drop-in replacement for sysvinit.\" - http://www.freedesktop.org/wiki/Software/systemd/ Tips and tricks - http://www.freedesktop.org/wiki/Software/systemd/TipsAndTricks/ Man page for control groups - http://man7.org/linux/man-pages/man5/systemd.cgroup.5.html FAQ - http://www.freedesktop.org/wiki/Software/systemd/FrequentlyAskedQuestions/ sysvinit to chkconfig - https://fedoraproject.org/wiki/SysVinit_to_Systemd_Cheatsheet systemd for upstart users - https://wiki.ubuntu.com/SystemdForUpstartUsers Tips Init file locations /usr/lib/systemd/system /etc/systemd/system # has precedence Show the full systemd journal The systemd journal is syslog and more. journalctl --full Show the nginx journal for today journalctl -u nginx.service --since today Show units Units are things that are handled by systemd, including services. systemctl list-units Show dependencies This works on any .target or .service systemctl list-dependencies network.service Enable a service This behavior replaces chkconfig systemctl enable docker.service Check the status of a service and show 20 lines systemctl -n 20 status nodejs Per-user services https://wiki.archlinux.org/index.php/Systemd/User /usr/lib/systemd/user/ - where services provided by installed packages go. /etc/systemd/user/ - where system-wide user services are placed by the system administrator. ~/.config/systemd/user/ - where the user puts its own services. Alter power / sleep / hibernate button behaviors /etc/systemd/logind.conf Show name resolution status systemd-resolve --status Show boot performance systemd-analyze blame systemd-analyze critical-chain Show cgroup contexts systemd-cgls Show top control groups by their resource usage systemd-cgtop Detect if you are running in a container or on a VM systemd-detect-virt","title":"systemd"},{"location":"systemd/#tips","text":"","title":"Tips"},{"location":"systemd/#init-file-locations","text":"/usr/lib/systemd/system /etc/systemd/system # has precedence","title":"Init file locations"},{"location":"systemd/#show-the-full-systemd-journal","text":"The systemd journal is syslog and more. journalctl --full","title":"Show the full systemd journal"},{"location":"systemd/#show-the-nginx-journal-for-today","text":"journalctl -u nginx.service --since today","title":"Show the nginx journal for today"},{"location":"systemd/#show-units","text":"Units are things that are handled by systemd, including services. systemctl list-units","title":"Show units"},{"location":"systemd/#show-dependencies","text":"This works on any .target or .service systemctl list-dependencies network.service","title":"Show dependencies"},{"location":"systemd/#enable-a-service","text":"This behavior replaces chkconfig systemctl enable docker.service","title":"Enable a service"},{"location":"systemd/#check-the-status-of-a-service-and-show-20-lines","text":"systemctl -n 20 status nodejs","title":"Check the status of a service and show 20 lines"},{"location":"systemd/#per-user-services","text":"https://wiki.archlinux.org/index.php/Systemd/User /usr/lib/systemd/user/ - where services provided by installed packages go. /etc/systemd/user/ - where system-wide user services are placed by the system administrator. ~/.config/systemd/user/ - where the user puts its own services.","title":"Per-user services"},{"location":"systemd/#alter-power-sleep-hibernate-button-behaviors","text":"/etc/systemd/logind.conf","title":"Alter power / sleep / hibernate button behaviors"},{"location":"systemd/#show-name-resolution-status","text":"systemd-resolve --status","title":"Show name resolution status"},{"location":"systemd/#show-boot-performance","text":"systemd-analyze blame systemd-analyze critical-chain","title":"Show boot performance"},{"location":"systemd/#show-cgroup-contexts","text":"systemd-cgls","title":"Show cgroup contexts"},{"location":"systemd/#show-top-control-groups-by-their-resource-usage","text":"systemd-cgtop","title":"Show top control groups by their resource usage"},{"location":"systemd/#detect-if-you-are-running-in-a-container-or-on-a-vm","text":"systemd-detect-virt","title":"Detect if you are running in a container or on a VM"},{"location":"tcl/","text":"\"TCL Corporation is a Chinese multinational electronics company headquartered in Huizhou, Guangdong Province.\" - https://en.wikipedia.org/wiki/Tcl TCL 65C807 https://www.tclusa.com/products/home-theater/c-series/tcl-65-class-c-series-4k-uhd-hdr-roku-smart-tv-65c807 Atribute Value Product Size (WxHxD) with Stand 57.2\" x 36.0\" x 12.8\" Product Size (WxHxD) without Stand 57.2\" x 34.4\" x 2.3\" Stand Separation Distance 55.6\" Product Weight with Stand (lbs) 53 lbs Product Weight without stand (lbs) 52 lbs","title":"TCL"},{"location":"tcl/#tcl-65c807","text":"https://www.tclusa.com/products/home-theater/c-series/tcl-65-class-c-series-4k-uhd-hdr-roku-smart-tv-65c807 Atribute Value Product Size (WxHxD) with Stand 57.2\" x 36.0\" x 12.8\" Product Size (WxHxD) without Stand 57.2\" x 34.4\" x 2.3\" Stand Separation Distance 55.6\" Product Weight with Stand (lbs) 53 lbs Product Weight without stand (lbs) 52 lbs","title":"TCL 65C807"},{"location":"tcpdump/","text":"Network sniffing tool. Syntax Examples Capture packets to and from an IP address Only captures data that includes 1.2.3.4 as source or destination address tcpdump host 1.2.3.4 Capture traffic that contains a given mac address writes capfile.cap containing all traffic to or from the specified mac address on the network attached to eth1 tcpdump -w capfile.cap -i eth1 ether host 00:03:fa:46:2c:08 Filter packets from an existing capture Filters port 53 packets out of the old capfile into the new tcpdump -r oldcapfile.cap -w newcapfile.cap port 53 Capture all pop3 traffic and all traffic from a particular host Captures all pop3 traffic and all traffic to or from the specified host on the first interface of a Mac OS X computer tcpdump -w foo.cap -i en0 ether host 00:03:9a:28:44:01 or port 110 Capture all traffic not a mac address Captures all traffic not from the host 00:1b:63:ce:83:2e, useful for filtering out your own traffic. tcpdump -i en1 not ether src 00:1b:63:ce:83:2e Capture LLDP traffic This matches 2 bytes starting at the 12th byte against 88cc tcpdump -v -s 1500 -c 1 '(ether[12:2]=0x88cc)' Capture SYN packets tcpdump -n 'tcp[13] & 2!=0' Capture SYN/ACK packets tcpdump -n 'tcp[13]=18' Or another way tcpdump 'tcp[tcpflags] && tcp-syn != 0' Or capture all SYN packets going only to two ethernet destinations: tcpdump 'tcp[13] & 2!=0 && (ether dst 00:22:64:f4:d0:70 or ether dst 00:22:64:f4:d0:6e)' Write capture to file and replay it at the same time sudo tcpdump -n 'host 216.200.102.84' -s 1500 -l -w - | tee logcopy.pcap | tcpdump -r - Write a circular buffer of traffic This will write 5 files 1 mb each and loop through them as the destination for writing traffic. That is, the filenames do not indicate chronology. The files will be named foo.cap[0-4] sudo tcpdump -C 1 -W 5 -w foo.cap Print out a list of observed src ip addresses every 5 seconds This is limited to 192.168.1 matches while true ; do date '+%F %T%z' sudo timeout 5 tcpdump -n 2>/dev/null | awk '$3 ~ /10.8/ { print gensub(/([0-9]*\\.[0-9]*\\.[0-9]*\\.[0-9]*)(\\.[0-9]*)?/, \"ip address: \\\\1\", \"g\", $3) ; }' | sort -t. -k4n | uniq -c done You can reassemble these files chronologically with mergecap -w merged.cap foo.cap* Show WPA 4-way handshakes tcpdump -n -i en0 \"ether proto 0x888e\" Links http://www.danielmiessler.com/study/tcpdump/ https://www.rationallyparanoid.com/articles/tcpdump.html","title":"tcpdump"},{"location":"tcpdump/#syntax-examples","text":"","title":"Syntax Examples"},{"location":"tcpdump/#capture-packets-to-and-from-an-ip-address","text":"Only captures data that includes 1.2.3.4 as source or destination address tcpdump host 1.2.3.4","title":"Capture packets to and from an IP address"},{"location":"tcpdump/#capture-traffic-that-contains-a-given-mac-address","text":"writes capfile.cap containing all traffic to or from the specified mac address on the network attached to eth1 tcpdump -w capfile.cap -i eth1 ether host 00:03:fa:46:2c:08","title":"Capture traffic that contains a given mac address"},{"location":"tcpdump/#filter-packets-from-an-existing-capture","text":"Filters port 53 packets out of the old capfile into the new tcpdump -r oldcapfile.cap -w newcapfile.cap port 53","title":"Filter packets from an existing capture"},{"location":"tcpdump/#capture-all-pop3-traffic-and-all-traffic-from-a-particular-host","text":"Captures all pop3 traffic and all traffic to or from the specified host on the first interface of a Mac OS X computer tcpdump -w foo.cap -i en0 ether host 00:03:9a:28:44:01 or port 110","title":"Capture all pop3 traffic and all traffic from a particular host"},{"location":"tcpdump/#capture-all-traffic-not-a-mac-address","text":"Captures all traffic not from the host 00:1b:63:ce:83:2e, useful for filtering out your own traffic. tcpdump -i en1 not ether src 00:1b:63:ce:83:2e","title":"Capture all traffic not a mac address"},{"location":"tcpdump/#capture-lldp-traffic","text":"This matches 2 bytes starting at the 12th byte against 88cc tcpdump -v -s 1500 -c 1 '(ether[12:2]=0x88cc)'","title":"Capture LLDP traffic"},{"location":"tcpdump/#capture-syn-packets","text":"tcpdump -n 'tcp[13] & 2!=0'","title":"Capture SYN packets"},{"location":"tcpdump/#capture-synack-packets","text":"tcpdump -n 'tcp[13]=18' Or another way tcpdump 'tcp[tcpflags] && tcp-syn != 0' Or capture all SYN packets going only to two ethernet destinations: tcpdump 'tcp[13] & 2!=0 && (ether dst 00:22:64:f4:d0:70 or ether dst 00:22:64:f4:d0:6e)'","title":"Capture SYN/ACK packets"},{"location":"tcpdump/#write-capture-to-file-and-replay-it-at-the-same-time","text":"sudo tcpdump -n 'host 216.200.102.84' -s 1500 -l -w - | tee logcopy.pcap | tcpdump -r -","title":"Write capture to file and replay it at the same time"},{"location":"tcpdump/#write-a-circular-buffer-of-traffic","text":"This will write 5 files 1 mb each and loop through them as the destination for writing traffic. That is, the filenames do not indicate chronology. The files will be named foo.cap[0-4] sudo tcpdump -C 1 -W 5 -w foo.cap","title":"Write a circular buffer of traffic"},{"location":"tcpdump/#print-out-a-list-of-observed-src-ip-addresses-every-5-seconds","text":"This is limited to 192.168.1 matches while true ; do date '+%F %T%z' sudo timeout 5 tcpdump -n 2>/dev/null | awk '$3 ~ /10.8/ { print gensub(/([0-9]*\\.[0-9]*\\.[0-9]*\\.[0-9]*)(\\.[0-9]*)?/, \"ip address: \\\\1\", \"g\", $3) ; }' | sort -t. -k4n | uniq -c done You can reassemble these files chronologically with mergecap -w merged.cap foo.cap*","title":"Print out a list of observed src ip addresses every 5 seconds"},{"location":"tcpdump/#show-wpa-4-way-handshakes","text":"tcpdump -n -i en0 \"ether proto 0x888e\"","title":"Show WPA 4-way handshakes"},{"location":"tcpdump/#links","text":"http://www.danielmiessler.com/study/tcpdump/ https://www.rationallyparanoid.com/articles/tcpdump.html","title":"Links"},{"location":"terraform/","text":"\"Write, Plan, and Create Infrastructure as Code\" - https://www.terraform.io/ Links https://www.terraform.io/docs/providers/github/index.html https://www.terraform.io/docs/providers/gitlab/index.html","title":"Terraform"},{"location":"terraform/#links","text":"https://www.terraform.io/docs/providers/github/index.html https://www.terraform.io/docs/providers/gitlab/index.html","title":"Links"},{"location":"time/","text":"Notes about time technologies. ISO 8601 ISO 8601 Data elements and interchange formats - Information interchange - Representation of dates and times is an international standard covering the exchange of date and time-related data. ISO 8601 format examples See the ISO 8601 wikipedia page for many examples. Much of the content in this section was taken from that article. One notable syntax is that the letter T should always precede times. This aids in parsing, and distinguishes between month and minute, which are both shortened to M. Another notable syntax is the use of Z to mean a timezone offset of 0 hours, or GMT. Single points in time $ for fmt in {date,hours,minutes,seconds,ns} ; do bash -x -c \" TZ=$( awk '$1 !~ /^#/ {print $3}' /usr/share/zoneinfo/zone.tab | sort -R | head -n 1 ) \\ date --iso-8601=${fmt} \" ; done ; + TZ=America/Paramaribo + date --iso-8601=date 2016-08-09 + TZ=Africa/Dakar + date --iso-8601=hours 2016-08-09T21+00:00 + TZ=Indian/Kerguelen + date --iso-8601=minutes 2016-08-10T02:58+05:00 + TZ=Pacific/Saipan + date --iso-8601=seconds 2016-08-10T07:58:48+10:00 + TZ=Pacific/Midway + date --iso-8601=ns 2016-08-09T10:58:48,503878101-11:00 Week: 2016-W32 Date with week number: 2016-W32-2 Month and day without year: -12-31 Durations, or ranges of time Durations are a component of time intervals and define the amount of intervening time in a time interval. Examples: P10Y - a duration of ten years. P5DT12H - a duration of five days and twelve hours. P3Y6M4DT12H30M5S - a duration of three years, six months, four days, twelve hours, thirty minutes, and five seconds. P1M - one month. PT1M - one minute. Time intervals A time interval is the intervening time between two time points. There are four ways to express a time interval: Start and end, such as 2007-03-01T13:00:00Z/2008-05-11T15:30:00Z Start and duration, such as 2007-03-01T13:00:00Z/P1Y2M10DT2H30M Duration and end, such as P1Y2M10DT2H30M/2008-05-11T15:30:00Z Duration only, such as P1Y2M10DT2H30M , with additional context information Repeating intervals Repeating intervals are formed by adding R[n]/ to the beginning of an interval expression. Such as R5/2007-03-01T13:00:00Z/2008-05-11T15:30:00Z . The n can be omitted if the interval should repeat forever. RFC 3339 RFC 3339 is considered a profile of ISO 8601. It defines a profile of ISO 8601 for use in Internet protocols and standards. It explicitly excludes durations and dates before the common era. The more complex formats such as week numbers and ordinal days are not permitted. https://tools.ietf.org/html/rfc3339 Leap Seconds \"A leap second is a one-second adjustment that is occasionally applied to Coordinated Universal Time (UTC) in order to keep its time of day close to the mean solar time, or UT1 .\" - https://en.wikipedia.org/wiki/Leap_second Leap seconds are scheduled by the international earth rotation and reference systems service (See also: https://en.wikipedia.org/wiki/International_Earth_Rotation_and_Reference_Systems_Service ) Leap seconds cause a variety of problems in computer systems, and complicate time tracking in general. Public time server handling of leap seconds Google time servers do leap second smearing - https://developers.google.com/time/ AWS time servers do leap second smearing - https://aws.amazon.com/about-aws/whats-new/2017/11/introducing-the-amazon-time-sync-service ntp.org servers do not leap smear: \"Leap Second Smearing MUST NOT be used for public servers, e.g. servers provided by metrology institutes, or servers participating in the NTP pool project.\" - https://docs.ntpsec.org/latest/leapsmear.html Leap Second Links When is the next leap second? Resolve Leap Second Issues in Red Hat Enterprise Linux Google Public NTP: Leap Smear Five different ways to handle leap seconds with NTP The Unix leap second mess ntp.org FAQ: What happens during a Leap Second? Code snips and examples Quick and dirty time sync in Linux for when NTP is blocked. date -s $(curl -s -D - google.com | sed '/Date:/s/.*Date: //p ; d') Links Reading ISO 8601 Date and Time on the Internet: Timestamps - RFC 3339 Examples of date (GNU) man date (linux) man date (freebsd) Falsehoods programmers believe about time More falsehoods programmers believe about time; \"wisdom of the crowd\" edition International Earth Rotation and Reference Systems Service Videos The Problem with Time & Timezones - Computerphile See Also ntp - Network Time Protocol ptp - Precision Time Protocol","title":"time"},{"location":"time/#iso-8601","text":"ISO 8601 Data elements and interchange formats - Information interchange - Representation of dates and times is an international standard covering the exchange of date and time-related data.","title":"ISO 8601"},{"location":"time/#iso-8601-format-examples","text":"See the ISO 8601 wikipedia page for many examples. Much of the content in this section was taken from that article. One notable syntax is that the letter T should always precede times. This aids in parsing, and distinguishes between month and minute, which are both shortened to M. Another notable syntax is the use of Z to mean a timezone offset of 0 hours, or GMT.","title":"ISO 8601 format examples"},{"location":"time/#single-points-in-time","text":"$ for fmt in {date,hours,minutes,seconds,ns} ; do bash -x -c \" TZ=$( awk '$1 !~ /^#/ {print $3}' /usr/share/zoneinfo/zone.tab | sort -R | head -n 1 ) \\ date --iso-8601=${fmt} \" ; done ; + TZ=America/Paramaribo + date --iso-8601=date 2016-08-09 + TZ=Africa/Dakar + date --iso-8601=hours 2016-08-09T21+00:00 + TZ=Indian/Kerguelen + date --iso-8601=minutes 2016-08-10T02:58+05:00 + TZ=Pacific/Saipan + date --iso-8601=seconds 2016-08-10T07:58:48+10:00 + TZ=Pacific/Midway + date --iso-8601=ns 2016-08-09T10:58:48,503878101-11:00 Week: 2016-W32 Date with week number: 2016-W32-2 Month and day without year: -12-31","title":"Single points in time"},{"location":"time/#durations-or-ranges-of-time","text":"Durations are a component of time intervals and define the amount of intervening time in a time interval.","title":"Durations, or ranges of time"},{"location":"time/#examples","text":"P10Y - a duration of ten years. P5DT12H - a duration of five days and twelve hours. P3Y6M4DT12H30M5S - a duration of three years, six months, four days, twelve hours, thirty minutes, and five seconds. P1M - one month. PT1M - one minute.","title":"Examples:"},{"location":"time/#time-intervals","text":"A time interval is the intervening time between two time points. There are four ways to express a time interval: Start and end, such as 2007-03-01T13:00:00Z/2008-05-11T15:30:00Z Start and duration, such as 2007-03-01T13:00:00Z/P1Y2M10DT2H30M Duration and end, such as P1Y2M10DT2H30M/2008-05-11T15:30:00Z Duration only, such as P1Y2M10DT2H30M , with additional context information","title":"Time intervals"},{"location":"time/#repeating-intervals","text":"Repeating intervals are formed by adding R[n]/ to the beginning of an interval expression. Such as R5/2007-03-01T13:00:00Z/2008-05-11T15:30:00Z . The n can be omitted if the interval should repeat forever.","title":"Repeating intervals"},{"location":"time/#rfc-3339","text":"RFC 3339 is considered a profile of ISO 8601. It defines a profile of ISO 8601 for use in Internet protocols and standards. It explicitly excludes durations and dates before the common era. The more complex formats such as week numbers and ordinal days are not permitted. https://tools.ietf.org/html/rfc3339","title":"RFC 3339"},{"location":"time/#leap-seconds","text":"\"A leap second is a one-second adjustment that is occasionally applied to Coordinated Universal Time (UTC) in order to keep its time of day close to the mean solar time, or UT1 .\" - https://en.wikipedia.org/wiki/Leap_second Leap seconds are scheduled by the international earth rotation and reference systems service (See also: https://en.wikipedia.org/wiki/International_Earth_Rotation_and_Reference_Systems_Service ) Leap seconds cause a variety of problems in computer systems, and complicate time tracking in general.","title":"Leap Seconds"},{"location":"time/#public-time-server-handling-of-leap-seconds","text":"Google time servers do leap second smearing - https://developers.google.com/time/ AWS time servers do leap second smearing - https://aws.amazon.com/about-aws/whats-new/2017/11/introducing-the-amazon-time-sync-service ntp.org servers do not leap smear: \"Leap Second Smearing MUST NOT be used for public servers, e.g. servers provided by metrology institutes, or servers participating in the NTP pool project.\" - https://docs.ntpsec.org/latest/leapsmear.html","title":"Public time server handling of leap seconds"},{"location":"time/#leap-second-links","text":"When is the next leap second? Resolve Leap Second Issues in Red Hat Enterprise Linux Google Public NTP: Leap Smear Five different ways to handle leap seconds with NTP The Unix leap second mess ntp.org FAQ: What happens during a Leap Second?","title":"Leap Second Links"},{"location":"time/#code-snips-and-examples","text":"","title":"Code snips and examples"},{"location":"time/#quick-and-dirty-time-sync-in-linux-for-when-ntp-is-blocked","text":"date -s $(curl -s -D - google.com | sed '/Date:/s/.*Date: //p ; d')","title":"Quick and dirty time sync in Linux for when NTP is blocked."},{"location":"time/#links","text":"","title":"Links"},{"location":"time/#reading","text":"ISO 8601 Date and Time on the Internet: Timestamps - RFC 3339 Examples of date (GNU) man date (linux) man date (freebsd) Falsehoods programmers believe about time More falsehoods programmers believe about time; \"wisdom of the crowd\" edition International Earth Rotation and Reference Systems Service","title":"Reading"},{"location":"time/#videos","text":"The Problem with Time & Timezones - Computerphile","title":"Videos"},{"location":"time/#see-also","text":"ntp - Network Time Protocol ptp - Precision Time Protocol","title":"See Also"},{"location":"tls/","text":"TLS is Transport Layer Security. It used to be called SSL: the Secure Sockets Layer. It has to do with encrypted IP traffic and stuff stuff. Apache SSL steps Generate a host key: openssl genrsa -out foo.com.key 2048 Generate a CSR from that key: openssl req -new -key foo.com.key -out foo.com.csr To set up VirtualHosts, follow this template: http://wiki.apache.org/httpd/NameBasedSSLVHosts Examples Show info about a pem file openssl x509 -noout -text -in foo.pem Show certificate options openssl x509 -text -in foo.crt Validate a keys / cert pair To validate that a particular key was used to generate a certificate, useful for testing https key/crt files, do the following and make sure the modulus sections match: openssl rsa -noout -text -in server.key | grep -i -A9 modulus openssl x509 -noout -text -in server.crt | grep -i -A9 modulus Or diff <(openssl rsa -in my.key -modulus | grep Modulus) <(openssl x509 -in my.crt -modulus | grep Modulus) Or as a function: function cert-key-compare { if [[ \"$1\" != *key* ]] || [[ \"$2\" != *cert* ]] ; then echo \"usage: cert-key-compare key certificate\" ; else diff \\ <(openssl rsa -in $1 -modulus 2>/dev/null | grep Modulus) \\ <(openssl x509 -in $2 -modulus 2>/dev/null | grep Modulus) \\ > /dev/null && echo \"key and crt match.\" || echo \"key and crt do not match\" fi } See some information about a server's certificate openssl s_client -connect linuxforums.org:443 Encrypt a file openssl enc -aes-256-cbc -salt -in yourfile -out yourfile.enc Decrypt a file openssl enc -aes-256-cbc -d -in encryptedfile.enc -out decryptedfile Encrypt / Decrypt bash functions function encrypt_file() { openssl enc -aes-256-cbc -salt -in \"${1}\" -out \"${1}.enc\" ; } function decrypt_file() { openssl enc -aes-256-cbc -d -in \"${1}\" -out \"${1}.dec\" ; } Links An overview of the SSL or TLS handshake The Transport Layer Security (TLS) Protocol Version 1.2 - https://tools.ietf.org/html/rfc5246","title":"tls"},{"location":"tls/#apache-ssl-steps","text":"Generate a host key: openssl genrsa -out foo.com.key 2048 Generate a CSR from that key: openssl req -new -key foo.com.key -out foo.com.csr To set up VirtualHosts, follow this template: http://wiki.apache.org/httpd/NameBasedSSLVHosts","title":"Apache SSL steps"},{"location":"tls/#examples","text":"","title":"Examples"},{"location":"tls/#show-info-about-a-pem-file","text":"openssl x509 -noout -text -in foo.pem","title":"Show info about a pem file"},{"location":"tls/#show-certificate-options","text":"openssl x509 -text -in foo.crt","title":"Show certificate options"},{"location":"tls/#validate-a-keys-cert-pair","text":"To validate that a particular key was used to generate a certificate, useful for testing https key/crt files, do the following and make sure the modulus sections match: openssl rsa -noout -text -in server.key | grep -i -A9 modulus openssl x509 -noout -text -in server.crt | grep -i -A9 modulus Or diff <(openssl rsa -in my.key -modulus | grep Modulus) <(openssl x509 -in my.crt -modulus | grep Modulus) Or as a function: function cert-key-compare { if [[ \"$1\" != *key* ]] || [[ \"$2\" != *cert* ]] ; then echo \"usage: cert-key-compare key certificate\" ; else diff \\ <(openssl rsa -in $1 -modulus 2>/dev/null | grep Modulus) \\ <(openssl x509 -in $2 -modulus 2>/dev/null | grep Modulus) \\ > /dev/null && echo \"key and crt match.\" || echo \"key and crt do not match\" fi }","title":"Validate a keys / cert pair"},{"location":"tls/#see-some-information-about-a-servers-certificate","text":"openssl s_client -connect linuxforums.org:443","title":"See some information about a server's certificate"},{"location":"tls/#encrypt-a-file","text":"openssl enc -aes-256-cbc -salt -in yourfile -out yourfile.enc","title":"Encrypt a file"},{"location":"tls/#decrypt-a-file","text":"openssl enc -aes-256-cbc -d -in encryptedfile.enc -out decryptedfile","title":"Decrypt a file"},{"location":"tls/#encrypt-decrypt-bash-functions","text":"function encrypt_file() { openssl enc -aes-256-cbc -salt -in \"${1}\" -out \"${1}.enc\" ; } function decrypt_file() { openssl enc -aes-256-cbc -d -in \"${1}\" -out \"${1}.dec\" ; }","title":"Encrypt / Decrypt bash functions"},{"location":"tls/#links","text":"An overview of the SSL or TLS handshake The Transport Layer Security (TLS) Protocol Version 1.2 - https://tools.ietf.org/html/rfc5246","title":"Links"},{"location":"top-variant-list/","text":"The top interface is a common pattern in the CLI tool world. Here are some top style tools. top style tools atop - Linux top tool that catches short-lived processes. glances - A better top which shows more aspects of the system. htop - Top, but different. Perhaps better. iftop - Top for network interfaces. innotop - Top for database connections. iotop - Top for IO. nethogs - Network top that shows usage by pid. ntop - Top for networking. powertop - Top for power usage. top - The original.","title":"top-variant-list"},{"location":"top-variant-list/#top-style-tools","text":"atop - Linux top tool that catches short-lived processes. glances - A better top which shows more aspects of the system. htop - Top, but different. Perhaps better. iftop - Top for network interfaces. innotop - Top for database connections. iotop - Top for IO. nethogs - Network top that shows usage by pid. ntop - Top for networking. powertop - Top for power usage. top - The original.","title":"top style tools"},{"location":"top/","text":"top is a CLI tool to show running processes. GNU top Usage Change the number of displayed processes with n Display all CPUs with 1 Kill a process with k Renice a process with r Save current display as default in ~/.toprc with W Show or hide idle processes with i Sort output with O BSD top Usage Start top sorted by cpu top -u See also Top variant list","title":"top"},{"location":"top/#gnu-top-usage","text":"Change the number of displayed processes with n Display all CPUs with 1 Kill a process with k Renice a process with r Save current display as default in ~/.toprc with W Show or hide idle processes with i Sort output with O","title":"GNU top Usage"},{"location":"top/#bsd-top-usage","text":"","title":"BSD top Usage"},{"location":"top/#start-top-sorted-by-cpu","text":"top -u","title":"Start top sorted by cpu"},{"location":"top/#see-also","text":"Top variant list","title":"See also"},{"location":"touch/","text":"touch is a command to modify the date of filesystem metadata for a given file. Examples Create an empty file touch somefile Randomize the mtime for a given file bash's random only goes up to 32767, which is about 9 hours. With RANDOM * 32767 + RANDOM we can get this up to just over 34 years. randomize-mtime() { seconds=\"$(( $(date +%s) - $(( RANDOM * 32767 )) - RANDOM))\" new_mtime=\"$(gdate -d @\"${seconds}\" \"+%Y%m%d%H%M.%S\")\" echo \"${new_mtime} $*\" 1>&2 touch -m -t \"${new_mtime}\" \"$@\" } # change mtime of all files to the same random mtime randomize-mtime test-foo{1..3} ; # change mtime of each file to a different random mtime for F in test-bar{1..3} ; do randomize-mtime \"$F\" done","title":"touch"},{"location":"touch/#examples","text":"","title":"Examples"},{"location":"touch/#create-an-empty-file","text":"touch somefile","title":"Create an empty file"},{"location":"touch/#randomize-the-mtime-for-a-given-file","text":"bash's random only goes up to 32767, which is about 9 hours. With RANDOM * 32767 + RANDOM we can get this up to just over 34 years. randomize-mtime() { seconds=\"$(( $(date +%s) - $(( RANDOM * 32767 )) - RANDOM))\" new_mtime=\"$(gdate -d @\"${seconds}\" \"+%Y%m%d%H%M.%S\")\" echo \"${new_mtime} $*\" 1>&2 touch -m -t \"${new_mtime}\" \"$@\" } # change mtime of all files to the same random mtime randomize-mtime test-foo{1..3} ; # change mtime of each file to a different random mtime for F in test-bar{1..3} ; do randomize-mtime \"$F\" done","title":"Randomize the mtime for a given file"},{"location":"ubuntu/","text":"\"Ubuntu is an open source software operating system that runs from the desktop, to the cloud, to all your internet connected things.\" - https://www.ubuntu.com/ Versions overview Codename Ubuntu Version EOL Init Kernel Ruby Python bash Bionic 18.04 LTS 2023-04 systemd 4.15 Artful 17.10 2018-07 systemd 4.13 Xenial 16.04 LTS 2021-04 systemd 4.4 2.7.12 / 3.5 4.3.46 Trusty 14.04 LTS 2019-04 Upstart 3.13 1.9.3 2.7.6 4.3.11 Precise 12.04 LTS 2017-04 Upstart 3.2 1.8.7 Lucid 10.04 LTS 2015-04 Upstart 2.6.32 Links Ubuntu Kernel Support and Schedules List of releases Table of Versions","title":"Ubuntu Linux"},{"location":"ubuntu/#versions-overview","text":"Codename Ubuntu Version EOL Init Kernel Ruby Python bash Bionic 18.04 LTS 2023-04 systemd 4.15 Artful 17.10 2018-07 systemd 4.13 Xenial 16.04 LTS 2021-04 systemd 4.4 2.7.12 / 3.5 4.3.46 Trusty 14.04 LTS 2019-04 Upstart 3.13 1.9.3 2.7.6 4.3.11 Precise 12.04 LTS 2017-04 Upstart 3.2 1.8.7 Lucid 10.04 LTS 2015-04 Upstart 2.6.32","title":"Versions overview"},{"location":"ubuntu/#links","text":"Ubuntu Kernel Support and Schedules List of releases Table of Versions","title":"Links"},{"location":"upstart/","text":"\"Upstart is an event-based replacement for the /sbin/init daemon which handles starting of tasks and services during boot, stopping them during shutdown and supervising them while the system is running.\" - http://upstart.ubuntu.com Examples Start multiple instances of the same services http://upstart.ubuntu.com/cookbook/#instance Master start on runlevel [2345] stop on runlevel [^2345] respawn env job_count=6 pre-start script for i in $(seq -w 1 ${job_count}) ; do start photoworker N=${i} done end script post-stop script for i in $(seq -w 1 ${job_count}) ; do stop photoworker N=${i} done end script Child respawn respawn limit 10 5 instance $N env logfile=\"/var/log/worker_photoworker.log\" chdir /srv/photoworkers/current/web/services/jobworkers pre-start exec bash -c \"echo $(date --rfc-3339=seconds) beginning worker run >> ${logfile}\" exec su -s /bin/sh -c 'exec \"$0\" \"$@\"' php-worker -- php photoworker.php >> ${logfile} 2>&1 post-stop exec bash -c \"echo $(date --rfc-3339=seconds) ended worker run >> ${logfile}\" Redirect all output of an upstart script to syslog Found at http://serverfault.com/questions/114052/logging-a-daemons-output-with-upstart script FIFO=fifo.temp mkfifo $FIFO ( logger -t myservice <$FIFO & ) exec > $FIFO rm $FIFO exec /usr/local/bin/myservice 2>&1 end script Links http://upstart.ubuntu.com/cookbook","title":"Upstart"},{"location":"upstart/#examples","text":"","title":"Examples"},{"location":"upstart/#start-multiple-instances-of-the-same-services","text":"http://upstart.ubuntu.com/cookbook/#instance","title":"Start multiple instances of the same services"},{"location":"upstart/#master","text":"start on runlevel [2345] stop on runlevel [^2345] respawn env job_count=6 pre-start script for i in $(seq -w 1 ${job_count}) ; do start photoworker N=${i} done end script post-stop script for i in $(seq -w 1 ${job_count}) ; do stop photoworker N=${i} done end script","title":"Master"},{"location":"upstart/#child","text":"respawn respawn limit 10 5 instance $N env logfile=\"/var/log/worker_photoworker.log\" chdir /srv/photoworkers/current/web/services/jobworkers pre-start exec bash -c \"echo $(date --rfc-3339=seconds) beginning worker run >> ${logfile}\" exec su -s /bin/sh -c 'exec \"$0\" \"$@\"' php-worker -- php photoworker.php >> ${logfile} 2>&1 post-stop exec bash -c \"echo $(date --rfc-3339=seconds) ended worker run >> ${logfile}\"","title":"Child"},{"location":"upstart/#redirect-all-output-of-an-upstart-script-to-syslog","text":"Found at http://serverfault.com/questions/114052/logging-a-daemons-output-with-upstart script FIFO=fifo.temp mkfifo $FIFO ( logger -t myservice <$FIFO & ) exec > $FIFO rm $FIFO exec /usr/local/bin/myservice 2>&1 end script","title":"Redirect all output of an upstart script to syslog"},{"location":"upstart/#links","text":"http://upstart.ubuntu.com/cookbook","title":"Links"},{"location":"vagrant/","text":"\"Vagrant is a tool for building and managing virtual machine environments in a single workflow.\" - https://www.vagrantup.com/intro/index.html Links https://www.vagrantup.com https://docs.vagrantup.com/v2 https://atlas.hashicorp.com Examples: https://github.com/patrickdlee/vagrant-examples OS X Shell tweaks brew tap homebrew/completions brew install vagrant-completion Then in .bash_profile: if [ -f $(brew --prefix)/etc/bash_completion ]; then . $(brew --prefix)/etc/bash_completion fi Plugins vagrant plugin install vagrant-vbguest vagrant plugin install vagrant-hosts Usage Examples List which boxes you have stored locally vagrant box List Remove an old version of a vagrant box vagrant box remove ubuntu/trusty64 --box-version 20151201.0.0 Script box updates This may fail in some circumstances, I haven't tested it exhaustively. vagrant box outdated --machine-readable --global | awk -F, '$4 == \"warn\" {print $5; exit 1}' | awk -F\"'\" '{print $2}' | xargs -n1 vagrant box update --box Show status of all running Vagrant boxes, not just the one in the CWD vagrant global-status","title":"vagrant"},{"location":"vagrant/#links","text":"https://www.vagrantup.com https://docs.vagrantup.com/v2 https://atlas.hashicorp.com Examples: https://github.com/patrickdlee/vagrant-examples","title":"Links"},{"location":"vagrant/#os-x-shell-tweaks","text":"brew tap homebrew/completions brew install vagrant-completion Then in .bash_profile: if [ -f $(brew --prefix)/etc/bash_completion ]; then . $(brew --prefix)/etc/bash_completion fi","title":"OS X Shell tweaks"},{"location":"vagrant/#plugins","text":"vagrant plugin install vagrant-vbguest vagrant plugin install vagrant-hosts","title":"Plugins"},{"location":"vagrant/#usage-examples","text":"","title":"Usage Examples"},{"location":"vagrant/#list-which-boxes-you-have-stored-locally","text":"vagrant box List","title":"List which boxes you have stored locally"},{"location":"vagrant/#remove-an-old-version-of-a-vagrant-box","text":"vagrant box remove ubuntu/trusty64 --box-version 20151201.0.0","title":"Remove an old version of a vagrant box"},{"location":"vagrant/#script-box-updates","text":"This may fail in some circumstances, I haven't tested it exhaustively. vagrant box outdated --machine-readable --global | awk -F, '$4 == \"warn\" {print $5; exit 1}' | awk -F\"'\" '{print $2}' | xargs -n1 vagrant box update --box","title":"Script box updates"},{"location":"vagrant/#show-status-of-all-running-vagrant-boxes-not-just-the-one-in-the-cwd","text":"vagrant global-status","title":"Show status of all running Vagrant boxes, not just the one in the CWD"},{"location":"videos/","text":"Corey Quinn Scale 14x - Docker Must Die Sensu @ Yelp part 1 Sensu @ Yelp part 2 Surge 2015 - Bryan Cantrill - Docker in Production: Tales From the Engine Room Terrible ideas in Git The Well Tempered API - Why can we play 400 year old music but our software only lasts a few months or years? Using Swagger to tame HTTP/JSON interfaces Working Theory of Monitoring Yelp's Theory of PaaSes talk from Box SRE Hour","title":"Tech videos"},{"location":"vim/","text":"\"Vim is a text editor that is upwards compatible to Vi.\" - man vim Modelines modelines are commented lines in files that set vim settings to use when editing that file. http://vim.wikia.com/wiki/Modeline_magic modeline example: # vim: set expandtab ts=2","title":"vim"},{"location":"vim/#modelines","text":"modelines are commented lines in files that set vim settings to use when editing that file. http://vim.wikia.com/wiki/Modeline_magic","title":"Modelines"},{"location":"vim/#modeline-example","text":"# vim: set expandtab ts=2","title":"modeline example:"},{"location":"wget/","text":"\"GNU Wget is a free software package for retrieving files using HTTP, HTTPS, FTP and FTPS the most widely-used Internet protocols. It is a non-interactive commandline tool, so it may easily be called from scripts, cron jobs, terminals without X-Windows support, etc.\" - https://www.gnu.org/software/wget Examples Mirror site for local viewing From the man page: to download a single page and all its requisites (even if they exist on separate websites), and make sure the lot displays properly locally, this author likes to use a few options in addition to -p: wget -E -H -k -K -p http://<site>/<document> Download all images from a site To politely download all images from within a current remote directory: wget --wait=2 --random-wait --tries=0 --waitretry=30 -np -N -r -A.jpg http://www.site.com/directory/ Simple use of cookies Some servers that need referrers and cookies can be accessed by doing: wget --save-cookies=\"cookies.txt\" \"foo.html wget --load-cookies=\"cookies.txt\" --referer=\"foo.html\" \"foo.mp3\" Set default behavior ~/.wgetrc sets default parameter values tries=0 continue=1 See Also aria2 curl httpstat - download and show a some useful connection information","title":"Wget"},{"location":"wget/#examples","text":"","title":"Examples"},{"location":"wget/#mirror-site-for-local-viewing","text":"From the man page: to download a single page and all its requisites (even if they exist on separate websites), and make sure the lot displays properly locally, this author likes to use a few options in addition to -p: wget -E -H -k -K -p http://<site>/<document>","title":"Mirror site for local viewing"},{"location":"wget/#download-all-images-from-a-site","text":"To politely download all images from within a current remote directory: wget --wait=2 --random-wait --tries=0 --waitretry=30 -np -N -r -A.jpg http://www.site.com/directory/","title":"Download all images from a site"},{"location":"wget/#simple-use-of-cookies","text":"Some servers that need referrers and cookies can be accessed by doing: wget --save-cookies=\"cookies.txt\" \"foo.html wget --load-cookies=\"cookies.txt\" --referer=\"foo.html\" \"foo.mp3\"","title":"Simple use of cookies"},{"location":"wget/#set-default-behavior","text":"~/.wgetrc sets default parameter values tries=0 continue=1","title":"Set default behavior"},{"location":"wget/#see-also","text":"aria2 curl httpstat - download and show a some useful connection information","title":"See Also"},{"location":"winbind/","text":"These examples may only work on Samba 3. See info about Winbindd here: https://wiki.samba.org/index.php/Configuring_Winbindd_on_a_Samba_AD_DC Examples Ping the winbind servers wbinfo -p list the domain users wbinfo -u try authenticating the user against winbind wbinfo -a dhoherd","title":"winbind"},{"location":"winbind/#examples","text":"","title":"Examples"},{"location":"winbind/#ping-the-winbind-servers","text":"wbinfo -p","title":"Ping the winbind servers"},{"location":"winbind/#list-the-domain-users","text":"wbinfo -u","title":"list the domain users"},{"location":"winbind/#try-authenticating-the-user-against-winbind","text":"wbinfo -a dhoherd","title":"try authenticating the user against winbind"},{"location":"wireshark/","text":"Wireshark) is the new name for what was Ethereal. It is a graphical packet sniffer that uses the same libraries as tcpdump. Filters Display filters have their own syntax, whereas capture filters use tcpdump syntax. Display Filters Filter by packet data content Display all packets that contain \"foo\" in the data section: data contains foo Display hosts within a given subnet ip.addr == 10.57.8.244/30 Display data within a port range To see all ceph-osd data tcp.port >= 6800 and tcp.port <= 7300 See also tcpdump","title":"Wireshark"},{"location":"wireshark/#filters","text":"Display filters have their own syntax, whereas capture filters use tcpdump syntax.","title":"Filters"},{"location":"wireshark/#display-filters","text":"","title":"Display Filters"},{"location":"wireshark/#filter-by-packet-data-content","text":"Display all packets that contain \"foo\" in the data section: data contains foo","title":"Filter by packet data content"},{"location":"wireshark/#display-hosts-within-a-given-subnet","text":"ip.addr == 10.57.8.244/30","title":"Display hosts within a given subnet"},{"location":"wireshark/#display-data-within-a-port-range","text":"To see all ceph-osd data tcp.port >= 6800 and tcp.port <= 7300","title":"Display data within a port range"},{"location":"wireshark/#see-also","text":"tcpdump","title":"See also"},{"location":"wonder-workshop/","text":"Wonder Workshop Dash Dash is a tripod robot with two wheels and a caster, with IR I/O, bluetooth, microphone, speaker, LEDs, and proximity sensors. Links https://github.com/playi/WonderPy - Python API for Wonder Dash, Dot and Cue. https://www.makewonder.com/robots/dash/","title":"wonder-workshop"},{"location":"wonder-workshop/#dash","text":"Dash is a tripod robot with two wheels and a caster, with IR I/O, bluetooth, microphone, speaker, LEDs, and proximity sensors.","title":"Dash"},{"location":"wonder-workshop/#links","text":"https://github.com/playi/WonderPy - Python API for Wonder Dash, Dot and Cue. https://www.makewonder.com/robots/dash/","title":"Links"},{"location":"wuzz/","text":"\"Interactive cli tool for HTTP inspection\" - https://github.com/asciimoo/wuzz","title":"wuzz"},{"location":"xargs/","text":"xargs issues commands with the STDIN as arguments, by default appended to the end of the command. Examples Get the type of each listing from ls ls | xargs file Handle spaces and use the arg as something other than the last token The -I argument takes a string to use as a delimiter for any input. ls | xargs -I {} mv \"{}\" ~/some/dir/ Pass 5 args to each echo command, and never do more than 3 echo processes at a time echo {1..100} | xargs -n5 -P3 echo use sed to change git files containing a certain string This uses GNU sed -i , on macOS you should use sed -i '' or gsed -i . git grep -l 192.168.5 | xargs sed -i 's/192.168.5/172.18.0/' Issue the same command several times in parallel This takes 1 directory as input and starts a sub-shell that cd's to the directory and runs a command. Up to 4 subhells are run in parallel. This is very similar to the GNU parallel command. find ~/code/ -mindepth 1 -maxdepth 1 -type d | xargs -I {} -n1 -P4 bash -c \"cd {} ; make install-hooks ;\"","title":"xargs"},{"location":"xargs/#examples","text":"","title":"Examples"},{"location":"xargs/#get-the-type-of-each-listing-from-ls","text":"ls | xargs file","title":"Get the type of each listing from ls"},{"location":"xargs/#handle-spaces-and-use-the-arg-as-something-other-than-the-last-token","text":"The -I argument takes a string to use as a delimiter for any input. ls | xargs -I {} mv \"{}\" ~/some/dir/","title":"Handle spaces and use the arg as something other than the last token"},{"location":"xargs/#pass-5-args-to-each-echo-command-and-never-do-more-than-3-echo-processes-at-a-time","text":"echo {1..100} | xargs -n5 -P3 echo","title":"Pass 5 args to each echo command, and never do more than 3 echo processes at a time"},{"location":"xargs/#use-sed-to-change-git-files-containing-a-certain-string","text":"This uses GNU sed -i , on macOS you should use sed -i '' or gsed -i . git grep -l 192.168.5 | xargs sed -i 's/192.168.5/172.18.0/'","title":"use sed to change git files containing a certain string"},{"location":"xargs/#issue-the-same-command-several-times-in-parallel","text":"This takes 1 directory as input and starts a sub-shell that cd's to the directory and runs a command. Up to 4 subhells are run in parallel. This is very similar to the GNU parallel command. find ~/code/ -mindepth 1 -maxdepth 1 -type d | xargs -I {} -n1 -P4 bash -c \"cd {} ; make install-hooks ;\"","title":"Issue the same command several times in parallel"},{"location":"yaml/","text":"\"YAML Ain't Markup Language\" - https://yaml.org/ Links http://www.yaml.org/ http://www.yaml.org/refcard.html https://en.wikipedia.org/wiki/YAML http://www.yamllint.com/","title":"yaml"},{"location":"yaml/#links","text":"http://www.yaml.org/ http://www.yaml.org/refcard.html https://en.wikipedia.org/wiki/YAML http://www.yamllint.com/","title":"Links"},{"location":"youtube-dl/","text":"\"Command-line program to download videos from YouTube.com and other video sites\" - https://github.com/rg3/youtube-dl/ Examples Show available media formats youtube-dl -F 'https://youtu.be/LdCq6y1Uu5Y' Download the best quality within resolution bounds youtube-dl -f 'bestvideo[height<=480]+bestaudio' 'https://youtu.be/-kgTCpv_W64' Download songs longer than 30 minutes The --match-filter only alters what gets downloaded, not what is displayed when using options like -j youtube-dl -f bestaudio --match-filter 'duration > 1800' 'https://www.youtube.com/user/boyceavenue' Reformat a youtube channel into lines as \"URL - Title\" youtube-dl -j --flat-playlist 'https://www.youtube.com/watch?v=CHMZW9kLpg0&list=PL1B627337ED6F55F0' | jq -S -r '\"https://youtu.be/\" + .id + \" - \" + .title'","title":"youtube-dl"},{"location":"youtube-dl/#examples","text":"","title":"Examples"},{"location":"youtube-dl/#show-available-media-formats","text":"youtube-dl -F 'https://youtu.be/LdCq6y1Uu5Y'","title":"Show available media formats"},{"location":"youtube-dl/#download-the-best-quality-within-resolution-bounds","text":"youtube-dl -f 'bestvideo[height<=480]+bestaudio' 'https://youtu.be/-kgTCpv_W64'","title":"Download the best quality within resolution bounds"},{"location":"youtube-dl/#download-songs-longer-than-30-minutes","text":"The --match-filter only alters what gets downloaded, not what is displayed when using options like -j youtube-dl -f bestaudio --match-filter 'duration > 1800' 'https://www.youtube.com/user/boyceavenue'","title":"Download songs longer than 30 minutes"},{"location":"youtube-dl/#reformat-a-youtube-channel-into-lines-as-url-title","text":"youtube-dl -j --flat-playlist 'https://www.youtube.com/watch?v=CHMZW9kLpg0&list=PL1B627337ED6F55F0' | jq -S -r '\"https://youtu.be/\" + .id + \" - \" + .title'","title":"Reformat a youtube channel into lines as \"URL - Title\""},{"location":"yum/","text":"\"yum - Yellowdog Updater Modified\" - man yum \"yum is an interactive, rpm based, package manager.\" - man yum Examples Show a list of enabled repositories yum repolist Show a list of available repositories yum repolist all Show all installed packages, their versions and their source repo yum list installed List available packages and the repo they come from yum list available | grep jre Show all duplicates in a search This is a good way to get a complete list of packages that are available that match a certain string yum --showduplicates search thrift Query available packages in a given repository yum --disablerepo=\"*\" --enablerepo=\"epel\" list available Upgrade and skip broken dependencies yum upgrade -y --skip-broken Upgrade and skip certain packages yum upgrade --exclude=*rabbitmq* Check for package conflicts install yum-utils, then run package-cleanup Find a package that includes a specific command or file yum whatprovides \"*/filename\" Check for groups of packages yum grouplist Enable optional installs in groups Add group_package_types=mandatory,default,optional in /etc/yum.conf Download but do not install packages for update yum upgrade --downloadonly --skip-broken Install a local file using yum yum localinstall whatever.rpm Auto-updates for Centos5 yum install yum-updatesd Auto-updates for Centos6 yum install yum-cron See Also rpm - interact with rpms directly https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Deployment_Guide/sec-Managing_Yum_Repositories.html","title":"yum"},{"location":"yum/#examples","text":"","title":"Examples"},{"location":"yum/#show-a-list-of-enabled-repositories","text":"yum repolist","title":"Show a list of enabled repositories"},{"location":"yum/#show-a-list-of-available-repositories","text":"yum repolist all","title":"Show a list of available repositories"},{"location":"yum/#show-all-installed-packages-their-versions-and-their-source-repo","text":"yum list installed","title":"Show all installed packages, their versions and their source repo"},{"location":"yum/#list-available-packages-and-the-repo-they-come-from","text":"yum list available | grep jre","title":"List available packages and the repo they come from"},{"location":"yum/#show-all-duplicates-in-a-search","text":"This is a good way to get a complete list of packages that are available that match a certain string yum --showduplicates search thrift","title":"Show all duplicates in a search"},{"location":"yum/#query-available-packages-in-a-given-repository","text":"yum --disablerepo=\"*\" --enablerepo=\"epel\" list available","title":"Query available packages in a given repository"},{"location":"yum/#upgrade-and-skip-broken-dependencies","text":"yum upgrade -y --skip-broken","title":"Upgrade and skip broken dependencies"},{"location":"yum/#upgrade-and-skip-certain-packages","text":"yum upgrade --exclude=*rabbitmq*","title":"Upgrade and skip certain packages"},{"location":"yum/#check-for-package-conflicts","text":"install yum-utils, then run package-cleanup","title":"Check for package conflicts"},{"location":"yum/#find-a-package-that-includes-a-specific-command-or-file","text":"yum whatprovides \"*/filename\"","title":"Find a package that includes a specific command or file"},{"location":"yum/#check-for-groups-of-packages","text":"yum grouplist","title":"Check for groups of packages"},{"location":"yum/#enable-optional-installs-in-groups","text":"Add group_package_types=mandatory,default,optional in /etc/yum.conf","title":"Enable optional installs in groups"},{"location":"yum/#download-but-do-not-install-packages-for-update","text":"yum upgrade --downloadonly --skip-broken","title":"Download but do not install packages for update"},{"location":"yum/#install-a-local-file-using-yum","text":"yum localinstall whatever.rpm","title":"Install a local file using yum"},{"location":"yum/#auto-updates-for-centos5","text":"yum install yum-updatesd","title":"Auto-updates for Centos5"},{"location":"yum/#auto-updates-for-centos6","text":"yum install yum-cron","title":"Auto-updates for Centos6"},{"location":"yum/#see-also","text":"rpm - interact with rpms directly https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Deployment_Guide/sec-Managing_Yum_Repositories.html","title":"See Also"},{"location":"zerotier/","text":"\"ZeroTier delivers the capabilities of VPNs, SDN, and SD-WAN with a single system. Manage all your connected resources across both local and wide area networks as if the whole world is a single data center.\" - https://www.zerotier.com/","title":"ZeroTier"},{"location":"zfs/","text":"ZFS is the Zetabyte File System. Links OpenZFS - http://open-zfs.org Tuning Guide - http://www.solarisinternals.com/wiki/index.php/ZFS_Evil_Tuning_Guide Hardware recommendations - http://blog.zorinaq.com/?e=10 Mac ZFS - http://code.google.com/p/maczfs/ Shadow migration feature - http://docs.oracle.com/cd/E23824_01/html/821-1448/gkkud.html Speed tuning - http://icesquare.com/wordpress/how-to-improve-zfs-performance/ ZFS RAID levels - http://www.zfsbuild.com/2010/05/26/zfs-raid-levels/ http://en.wikipedia.org/wiki/ZFS http://wiki.freebsd.org/ZFSQuickStartGuide http://www.solarisinternals.com/wiki/index.php/ZFS_Best_Practices_Guide http://zfsguru.com http://zfsonlinux.org/faq.html http://www.oracle.com/technetwork/articles/servers-storage-admin/o11-113-size-zfs-dedup-1354231.html http://wiki.freebsd.org/ZFSTuningGuide#Deduplication Corruption / failure to import - https://github.com/zfsonlinux/zfs/issues/2457 https://www.percona.com/blog/2018/05/15/about-zfs-performance/ https://wiki.freebsd.org/ZFSTuningGuide Tips Memory For normal operation, 1gb of memory per tb of disk space is suitable. For dedup operation, 5gb of memory per tb of addressable disk space is suitable. Log devices Use a log device if you have lots of writes. Mirror it, because if you lose it you lose the whole volume. Speed and latency are most important, not size. Log flushes every 5 seconds. Get SLC if possible, otherwise MLC l2arc Cache devices Use if you have lots of reads. Size does matter, with big devices more data can be cached for faster reads of more data. Speed and latency matter. Mirrororing l2arc does not matter because if it fails, reads come from the spinning disks. Too big of a device can suck up resources and cause poor performance. See https://wiki.freebsd.org/ZFSTuningGuide Good explanation: https://blogs.oracle.com/brendan/entry/test zdb Show the potential savings of turning on dedupe on zpool tank http://hub.opensolaris.org/bin/view/Community+Group+zfs/dedup zdb -S tank Show transactions and human readable dates in the zdb history Use zdb -e for pools that are not mounted. zdb -hh tank \\ | egrep 'txg|time' \\ | while read -r _ a b ; do if [ \"$a\" == \"time:\" ] ; then date -d @$b \"+$a %F %T\" ; else echo \"$a $b\" ; fi ; done zpool Create a zpool and its base filesystem zpool create -f -o cachefile=/tmp/zpool.cache zpoolname /dev/ada1 #create a zpool Add a cache device to a pool # add ada0p3 as a cache device to the tank zpool zpool add tank cache ada0p3 Show all configured zpool options for a given zpool zpool get all tank Show history of all operations on a given pool # show history of operations on the pool, eg: snapshots, attribute changes zpool history Show real time statistics on a given zpool # show per-device statistics every 1 second zpool iostat -v 1 Show basic information about all imported zpools # show zpool space info, deduplication ratio and health zpool list Show deduplication tables # show deduplication table entries. Take entries * size / 1024 / 1024 to calculate DDT consumption zpool status -D z2 Replace a disk in a zpool # Replace the first disk with the second in the tank pool zpool replace -f tank /dev/disk/by-id/ata-ST3000DM001-9YN166_W1F09CW9 /dev/disk/by-id/ata-ST3000DM001-9YN166_Z1F0N9S7 Real example $ zpool replace -f tank /dev/disk/by-id/ata-HGST_HDN724040ALE640_PK1334PCJY9ASS /dev/disk/by-id/ata-HGST_HUH728080ALE600_VKHA6YDX $ zpool status pool: home state: ONLINE scan: scrub repaired 0 in 0h0m with 0 errors on Sun Dec 10 00:24:07 2017 config: NAME STATE READ WRITE CKSUM home ONLINE 0 0 0 ata-M4-CT064M4SSD2_0000000012170908F759-part4 ONLINE 0 0 0 errors: No known data errors pool: tank state: DEGRADED status: One or more devices is currently being resilvered. The pool will continue to function, possibly in a degraded state. action: Wait for the resilver to complete. scan: resilver in progress since Mon Jan 8 19:57:45 2018 47.1M scanned out of 13.7T at 6.72M/s, 592h39m to go 11.5M resilvered, 0.00% done config: NAME STATE READ WRITE CKSUM tank DEGRADED 0 0 0 raidz1-0 DEGRADED 0 0 0 replacing-0 UNAVAIL 0 0 0 ata-HGST_HDN724040ALE640_PK1334PCJY9ASS UNAVAIL 0 1 0 corrupted data ata-HGST_HUH728080ALE600_VKHA6YDX ONLINE 0 0 0 (resilvering) ata-HGST_HDN724040ALE640_PK2334PEHG8LAT ONLINE 0 0 0 ata-HGST_HDN724040ALE640_PK2334PEHGD37T ONLINE 0 0 0 ata-HGST_HDN724040ALE640_PK2338P4H3TJPC ONLINE 0 0 0 errors: No known data errors zfs show differences between current filesystem state and snapshot state zfs diff tank tank@snap Show configured properties for a filesystem zfs get all Show custom filesystem attributes # show custom attributes that override inherited attributes zfs get all -s local tank Show an overview of all mounted zfs filesystems # show disk space including free physical disk space and mount info zfs list Show specified fields of each filesystem # show the listed fields of all filesystems zfs list -t all -o name,referenced,used,written,creation,userused@root Show only snapshots zfs list -t snapshot Show space consumed by file owner zfs userspace tank Disable atime updates for a filesystem zfs set atime=off tank Set compression to lz4 for a filesystem zfs set compression=lz4 tank Set deduplication to enabled for a filesystem zfs set dedup=on tank Set a filesystem to readonly zfs set readonly=on zpoolname/dataset Set a filesystem to allow NFS sharing zfs set sharenfs=on tank Create a dataset # create a dataset 'sole' on zpool 'tank' zfs create tank/sole Destroy multiple snapshots zfs destroy tank@20130413-weekly,20130420-weekly,20130428-weekly,20130505-weekly zfs send / receive Replicate a zpool (use the latest snapshot name as the source) to a blank zpool: zfs send -v -D -R tank@20120907-oldest | zfs receive -F -v z2 -D enables a deduplicated stream. -R enables a recursive send of all snapshots and filesystems up to that point. -F enables deletion of any snapshots on the target that don't exist on the sender -v enables verbose mode recursively zfs send a filesystem to a remote host and recieve it as a new dataset zfs send -v -D -R z1@20120907-oldest | ssh otherhost zfs receive -v z2/z1 Show summary of what would be sent This shows an entire dataset up to the given snapshot zfs send -n -v -D -R tank@20140531-monthly Show the space differences between two snapshots zfs send -n -v -D -i tank@20140531-monthly tank@20141031-monthly Show the amount of new space consumed by each monthly zfs list -o name | grep 'tank@.*monthly' | while read -r X ; do [[ ! $a =~ .*monthly ]] && a=$X || zfs send -n -v -D -i $a $X && a=$X ; done 2>&1 | grep send Complex examples Create a raidz called tank Create a raidz pool from 4 disks and set some properties: pool=tank zpool create -f \"${pool}\" raidz /dev/disk/by-id/scsi-SATA_HGST_HDN724040A_PK2338P4H*-part1 -o ashift=12 zfs set dedup=on \"${pool}\" zpool set listsnapshots=on \"${pool}\" zfs set atime=off \"${pool}\" zfs set compression=lz4 \"${pool}\" Create a case insensitive raidz3 out of 50 files pool=tank for X in {1..50} ; do mkfile -n 2g ${pool}.$X ; done ; sudo zpool create -O casesensitivity=insensitive ${pool} raidz3 /Users/danielh/Desktop/Stuff/${pool}/${pool}.{1..50} Troubleshooting Mount a pool that is giving you Trouble zpool import -o failmode=continue -o readonly=on zpool_name This helped me get read access to a pool that was kernel panicking with the following error when I tried to import it normally: Dec 7 14:48:40 localhost kernel: PANIC: blkptr at ffff8803fddb4200 DVA 0 has invalid OFFSET 294940902907904 ZFS on Mac OS X http://openzfsonosx.org Create a ZFS partition on /dev/disk3 # Must eject device in Disk Utility first diskutil partitiondisk /dev/disk3 GPTFormat ZFS %noformat% 100% # strange syntax, but works zpool create backups1 /dev/disk3s2 # create the zpool mdutil -i off /Volumes/backups1 # required on MacZFS since spotlight does not function ZFS on Linux If you get module errors: modprobe zfs ; ldconfig If you get permission denied, check selinux settings CentOS 6 Repository sudo yum install -y epel-release # assumes later CentOS 6 where epel is provided upstream sudo yum localinstall --nogpgcheck http://archive.zfsonlinux.org/epel/zfs-release.el6.noarch.rpm sudo yum install zfs -y Reinstalling when things fail #!/bin/bash -x yum install -y kernel-devel-$(uname -r) zfs_version=0.6.5.4 dkms remove -m zfs -v \"${zfs_version}\" --all dkms remove -m spl -v \"${zfs_version}\" --all dkms add -m spl -v \"${zfs_version}\" --force dkms add -m zfs -v \"${zfs_version}\" --force dkms install -m spl -v \"${zfs_version}\" --force dkms install -m zfs -v \"${zfs_version}\" --force Inspect the rpm for what scripts it runs This is useful for debugging failures after kernel upgrade. rpm -q --scripts zfs-dkms Building on CentOS 6 yum groupinstall \"Development tools\" && yum install -y libuuid-devel zlib-devel bc lsscsi mdadm parted kernel-debug # For spl, then again for zfs: ./configure && make && make rpm && rpm -i *64.rpm","title":"ZFS"},{"location":"zfs/#links","text":"OpenZFS - http://open-zfs.org Tuning Guide - http://www.solarisinternals.com/wiki/index.php/ZFS_Evil_Tuning_Guide Hardware recommendations - http://blog.zorinaq.com/?e=10 Mac ZFS - http://code.google.com/p/maczfs/ Shadow migration feature - http://docs.oracle.com/cd/E23824_01/html/821-1448/gkkud.html Speed tuning - http://icesquare.com/wordpress/how-to-improve-zfs-performance/ ZFS RAID levels - http://www.zfsbuild.com/2010/05/26/zfs-raid-levels/ http://en.wikipedia.org/wiki/ZFS http://wiki.freebsd.org/ZFSQuickStartGuide http://www.solarisinternals.com/wiki/index.php/ZFS_Best_Practices_Guide http://zfsguru.com http://zfsonlinux.org/faq.html http://www.oracle.com/technetwork/articles/servers-storage-admin/o11-113-size-zfs-dedup-1354231.html http://wiki.freebsd.org/ZFSTuningGuide#Deduplication Corruption / failure to import - https://github.com/zfsonlinux/zfs/issues/2457 https://www.percona.com/blog/2018/05/15/about-zfs-performance/ https://wiki.freebsd.org/ZFSTuningGuide","title":"Links"},{"location":"zfs/#tips","text":"","title":"Tips"},{"location":"zfs/#memory","text":"For normal operation, 1gb of memory per tb of disk space is suitable. For dedup operation, 5gb of memory per tb of addressable disk space is suitable.","title":"Memory"},{"location":"zfs/#log-devices","text":"Use a log device if you have lots of writes. Mirror it, because if you lose it you lose the whole volume. Speed and latency are most important, not size. Log flushes every 5 seconds. Get SLC if possible, otherwise MLC","title":"Log devices"},{"location":"zfs/#l2arc-cache-devices","text":"Use if you have lots of reads. Size does matter, with big devices more data can be cached for faster reads of more data. Speed and latency matter. Mirrororing l2arc does not matter because if it fails, reads come from the spinning disks. Too big of a device can suck up resources and cause poor performance. See https://wiki.freebsd.org/ZFSTuningGuide Good explanation: https://blogs.oracle.com/brendan/entry/test","title":"l2arc Cache devices"},{"location":"zfs/#zdb","text":"","title":"zdb"},{"location":"zfs/#show-the-potential-savings-of-turning-on-dedupe-on-zpool-tank","text":"http://hub.opensolaris.org/bin/view/Community+Group+zfs/dedup zdb -S tank","title":"Show the potential savings of turning on dedupe on zpool tank"},{"location":"zfs/#show-transactions-and-human-readable-dates-in-the-zdb-history","text":"Use zdb -e for pools that are not mounted. zdb -hh tank \\ | egrep 'txg|time' \\ | while read -r _ a b ; do if [ \"$a\" == \"time:\" ] ; then date -d @$b \"+$a %F %T\" ; else echo \"$a $b\" ; fi ; done","title":"Show transactions and human readable dates in the zdb history"},{"location":"zfs/#zpool","text":"","title":"zpool"},{"location":"zfs/#create-a-zpool-and-its-base-filesystem","text":"zpool create -f -o cachefile=/tmp/zpool.cache zpoolname /dev/ada1 #create a zpool","title":"Create a zpool and its base filesystem"},{"location":"zfs/#add-a-cache-device-to-a-pool","text":"# add ada0p3 as a cache device to the tank zpool zpool add tank cache ada0p3","title":"Add a cache device to a pool"},{"location":"zfs/#show-all-configured-zpool-options-for-a-given-zpool","text":"zpool get all tank","title":"Show all configured zpool options for a given zpool"},{"location":"zfs/#show-history-of-all-operations-on-a-given-pool","text":"# show history of operations on the pool, eg: snapshots, attribute changes zpool history","title":"Show history of all operations on a given pool"},{"location":"zfs/#show-real-time-statistics-on-a-given-zpool","text":"# show per-device statistics every 1 second zpool iostat -v 1","title":"Show real time statistics on a given zpool"},{"location":"zfs/#show-basic-information-about-all-imported-zpools","text":"# show zpool space info, deduplication ratio and health zpool list","title":"Show basic information about all imported zpools"},{"location":"zfs/#show-deduplication-tables","text":"# show deduplication table entries. Take entries * size / 1024 / 1024 to calculate DDT consumption zpool status -D z2","title":"Show deduplication tables"},{"location":"zfs/#replace-a-disk-in-a-zpool","text":"# Replace the first disk with the second in the tank pool zpool replace -f tank /dev/disk/by-id/ata-ST3000DM001-9YN166_W1F09CW9 /dev/disk/by-id/ata-ST3000DM001-9YN166_Z1F0N9S7","title":"Replace a disk in a zpool"},{"location":"zfs/#real-example","text":"$ zpool replace -f tank /dev/disk/by-id/ata-HGST_HDN724040ALE640_PK1334PCJY9ASS /dev/disk/by-id/ata-HGST_HUH728080ALE600_VKHA6YDX $ zpool status pool: home state: ONLINE scan: scrub repaired 0 in 0h0m with 0 errors on Sun Dec 10 00:24:07 2017 config: NAME STATE READ WRITE CKSUM home ONLINE 0 0 0 ata-M4-CT064M4SSD2_0000000012170908F759-part4 ONLINE 0 0 0 errors: No known data errors pool: tank state: DEGRADED status: One or more devices is currently being resilvered. The pool will continue to function, possibly in a degraded state. action: Wait for the resilver to complete. scan: resilver in progress since Mon Jan 8 19:57:45 2018 47.1M scanned out of 13.7T at 6.72M/s, 592h39m to go 11.5M resilvered, 0.00% done config: NAME STATE READ WRITE CKSUM tank DEGRADED 0 0 0 raidz1-0 DEGRADED 0 0 0 replacing-0 UNAVAIL 0 0 0 ata-HGST_HDN724040ALE640_PK1334PCJY9ASS UNAVAIL 0 1 0 corrupted data ata-HGST_HUH728080ALE600_VKHA6YDX ONLINE 0 0 0 (resilvering) ata-HGST_HDN724040ALE640_PK2334PEHG8LAT ONLINE 0 0 0 ata-HGST_HDN724040ALE640_PK2334PEHGD37T ONLINE 0 0 0 ata-HGST_HDN724040ALE640_PK2338P4H3TJPC ONLINE 0 0 0 errors: No known data errors","title":"Real example"},{"location":"zfs/#zfs","text":"","title":"zfs"},{"location":"zfs/#show-differences-between-current-filesystem-state-and-snapshot-state","text":"zfs diff tank tank@snap","title":"show differences between current filesystem state and snapshot state"},{"location":"zfs/#show-configured-properties-for-a-filesystem","text":"zfs get all","title":"Show configured properties for a filesystem"},{"location":"zfs/#show-custom-filesystem-attributes","text":"# show custom attributes that override inherited attributes zfs get all -s local tank","title":"Show custom filesystem attributes"},{"location":"zfs/#show-an-overview-of-all-mounted-zfs-filesystems","text":"# show disk space including free physical disk space and mount info zfs list","title":"Show an overview of all mounted zfs filesystems"},{"location":"zfs/#show-specified-fields-of-each-filesystem","text":"# show the listed fields of all filesystems zfs list -t all -o name,referenced,used,written,creation,userused@root","title":"Show specified fields of each filesystem"},{"location":"zfs/#show-only-snapshots","text":"zfs list -t snapshot","title":"Show only snapshots"},{"location":"zfs/#show-space-consumed-by-file-owner","text":"zfs userspace tank","title":"Show space consumed by file owner"},{"location":"zfs/#disable-atime-updates-for-a-filesystem","text":"zfs set atime=off tank","title":"Disable atime updates for a filesystem"},{"location":"zfs/#set-compression-to-lz4-for-a-filesystem","text":"zfs set compression=lz4 tank","title":"Set compression to lz4 for a filesystem"},{"location":"zfs/#set-deduplication-to-enabled-for-a-filesystem","text":"zfs set dedup=on tank","title":"Set deduplication to enabled for a filesystem"},{"location":"zfs/#set-a-filesystem-to-readonly","text":"zfs set readonly=on zpoolname/dataset","title":"Set a filesystem to readonly"},{"location":"zfs/#set-a-filesystem-to-allow-nfs-sharing","text":"zfs set sharenfs=on tank","title":"Set a filesystem to allow NFS sharing"},{"location":"zfs/#create-a-dataset","text":"# create a dataset 'sole' on zpool 'tank' zfs create tank/sole","title":"Create a dataset"},{"location":"zfs/#destroy-multiple-snapshots","text":"zfs destroy tank@20130413-weekly,20130420-weekly,20130428-weekly,20130505-weekly","title":"Destroy multiple snapshots"},{"location":"zfs/#zfs-send-receive","text":"Replicate a zpool (use the latest snapshot name as the source) to a blank zpool: zfs send -v -D -R tank@20120907-oldest | zfs receive -F -v z2 -D enables a deduplicated stream. -R enables a recursive send of all snapshots and filesystems up to that point. -F enables deletion of any snapshots on the target that don't exist on the sender -v enables verbose mode","title":"zfs send / receive"},{"location":"zfs/#recursively-zfs-send-a-filesystem-to-a-remote-host-and-recieve-it-as-a-new-dataset","text":"zfs send -v -D -R z1@20120907-oldest | ssh otherhost zfs receive -v z2/z1","title":"recursively zfs send a filesystem to a remote host and recieve it as a new dataset"},{"location":"zfs/#show-summary-of-what-would-be-sent","text":"This shows an entire dataset up to the given snapshot zfs send -n -v -D -R tank@20140531-monthly","title":"Show summary of what would be sent"},{"location":"zfs/#show-the-space-differences-between-two-snapshots","text":"zfs send -n -v -D -i tank@20140531-monthly tank@20141031-monthly","title":"Show the space differences between two snapshots"},{"location":"zfs/#show-the-amount-of-new-space-consumed-by-each-monthly","text":"zfs list -o name | grep 'tank@.*monthly' | while read -r X ; do [[ ! $a =~ .*monthly ]] && a=$X || zfs send -n -v -D -i $a $X && a=$X ; done 2>&1 | grep send","title":"Show the amount of new space consumed by each monthly"},{"location":"zfs/#complex-examples","text":"","title":"Complex examples"},{"location":"zfs/#create-a-raidz-called-tank","text":"Create a raidz pool from 4 disks and set some properties: pool=tank zpool create -f \"${pool}\" raidz /dev/disk/by-id/scsi-SATA_HGST_HDN724040A_PK2338P4H*-part1 -o ashift=12 zfs set dedup=on \"${pool}\" zpool set listsnapshots=on \"${pool}\" zfs set atime=off \"${pool}\" zfs set compression=lz4 \"${pool}\"","title":"Create a raidz called tank"},{"location":"zfs/#create-a-case-insensitive-raidz3-out-of-50-files","text":"pool=tank for X in {1..50} ; do mkfile -n 2g ${pool}.$X ; done ; sudo zpool create -O casesensitivity=insensitive ${pool} raidz3 /Users/danielh/Desktop/Stuff/${pool}/${pool}.{1..50}","title":"Create a case insensitive raidz3 out of 50 files"},{"location":"zfs/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"zfs/#mount-a-pool-that-is-giving-you-trouble","text":"zpool import -o failmode=continue -o readonly=on zpool_name This helped me get read access to a pool that was kernel panicking with the following error when I tried to import it normally: Dec 7 14:48:40 localhost kernel: PANIC: blkptr at ffff8803fddb4200 DVA 0 has invalid OFFSET 294940902907904","title":"Mount a pool that is giving you Trouble"},{"location":"zfs/#zfs-on-mac-os-x","text":"http://openzfsonosx.org","title":"ZFS on Mac OS X"},{"location":"zfs/#create-a-zfs-partition-on-devdisk3","text":"# Must eject device in Disk Utility first diskutil partitiondisk /dev/disk3 GPTFormat ZFS %noformat% 100% # strange syntax, but works zpool create backups1 /dev/disk3s2 # create the zpool mdutil -i off /Volumes/backups1 # required on MacZFS since spotlight does not function","title":"Create a ZFS partition on /dev/disk3"},{"location":"zfs/#zfs-on-linux","text":"If you get module errors: modprobe zfs ; ldconfig If you get permission denied, check selinux settings","title":"ZFS on Linux"},{"location":"zfs/#centos-6-repository","text":"sudo yum install -y epel-release # assumes later CentOS 6 where epel is provided upstream sudo yum localinstall --nogpgcheck http://archive.zfsonlinux.org/epel/zfs-release.el6.noarch.rpm sudo yum install zfs -y","title":"CentOS 6 Repository"},{"location":"zfs/#reinstalling-when-things-fail","text":"#!/bin/bash -x yum install -y kernel-devel-$(uname -r) zfs_version=0.6.5.4 dkms remove -m zfs -v \"${zfs_version}\" --all dkms remove -m spl -v \"${zfs_version}\" --all dkms add -m spl -v \"${zfs_version}\" --force dkms add -m zfs -v \"${zfs_version}\" --force dkms install -m spl -v \"${zfs_version}\" --force dkms install -m zfs -v \"${zfs_version}\" --force","title":"Reinstalling when things fail"},{"location":"zfs/#inspect-the-rpm-for-what-scripts-it-runs","text":"This is useful for debugging failures after kernel upgrade. rpm -q --scripts zfs-dkms","title":"Inspect the rpm for what scripts it runs"},{"location":"zfs/#building-on-centos-6","text":"yum groupinstall \"Development tools\" && yum install -y libuuid-devel zlib-devel bc lsscsi mdadm parted kernel-debug # For spl, then again for zfs: ./configure && make && make rpm && rpm -i *64.rpm","title":"Building on CentOS 6"},{"location":"zookeeper/","text":"ZooKeeper is a high-performance coordination service for distributed applications. - https://zookeeper.apache.org/doc/trunk/ Examples The four letter words \"ZooKeeper responds to a small set of commands. Each command is composed of four letters. You issue the commands to ZooKeeper via telnet or nc, at the client port.\" - https://zookeeper.apache.org/doc/r3.4.8/zookeeperAdmin.html#sc_zkCommands Lists brief details for the server and connected clients echo 'stat' | nc localhost 2181 View a list of variables that could be used for monitoring the health of the cluster echo 'mntr' | nc localhost 2181 List full details for the server echo 'srvr' | nc localhost 2181 Links https://zookeeper.apache.org/doc/r3.4.8/zookeeperAdmin.html","title":"zookeeper"},{"location":"zookeeper/#examples","text":"","title":"Examples"},{"location":"zookeeper/#the-four-letter-words","text":"\"ZooKeeper responds to a small set of commands. Each command is composed of four letters. You issue the commands to ZooKeeper via telnet or nc, at the client port.\" - https://zookeeper.apache.org/doc/r3.4.8/zookeeperAdmin.html#sc_zkCommands","title":"The four letter words"},{"location":"zookeeper/#lists-brief-details-for-the-server-and-connected-clients","text":"echo 'stat' | nc localhost 2181","title":"Lists brief details for the server and connected clients"},{"location":"zookeeper/#view-a-list-of-variables-that-could-be-used-for-monitoring-the-health-of-the-cluster","text":"echo 'mntr' | nc localhost 2181","title":"View a list of variables that could be used for monitoring the health of the cluster"},{"location":"zookeeper/#list-full-details-for-the-server","text":"echo 'srvr' | nc localhost 2181","title":"List full details for the server"},{"location":"zookeeper/#links","text":"https://zookeeper.apache.org/doc/r3.4.8/zookeeperAdmin.html","title":"Links"}]}